{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2017soft/AIME/blob/start-with-brac/DreamToExplore_Agent_Debug.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swo1QNejwSmS",
        "outputId": "6c0581b9-0ed0-4627-b08c-de0355a62ad5"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.argv = sys.argv[:1]\n",
        "%set_env CUDA_VISIBLE_DEVICES=0\n",
        "from google.colab import drive\n",
        "drive.flush_and_unmount()\n",
        "drive.mount('/content/gdrive')\n",
        "os.chdir('/content/gdrive/My Drive')\n",
        "!cp /content/gdrive/My\\ Drive/EIM.py /content\n",
        "!cp /content/gdrive/My\\ Drive/planner_regularizer_EIM.py /content\n",
        "!cp -r /content/gdrive/My\\ Drive/torchsample /content/\n",
        "!cp /content/gdrive/My\\ Drive/ObstacleData.py /content/\n",
        "!python /content/gdrive/My\\ Drive/setup.py build_ext --swig-opts=\"-modern -I../include\"\n",
        "!pip install tensorboardX\n",
        "import torchsample\n",
        "sys.path.append('/content/gdrive/My\\ Drive')\n",
        "import utils_planner as utils\n",
        "!pip install expecttest\n",
        "from tensorboardX import SummaryWriter\n",
        "import torch\n",
        "from EIM import ConditionalMixtureEIM, RecorderKeys, TrainIterationRecMod, ConfigInitialRecMod, DRERecMod, ComponentUpdateRecMod, WeightUpdateRecMod, Recorder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "e4Fu5syCNlJf",
        "outputId": "a337bc72-057c-4eec-d54a-7ecf6ea1f6c1"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "if subprocess.run('nvidia-smi').returncode:\n",
        "  raise RuntimeError(\n",
        "      'Cannot communicate with GPU. '\n",
        "      'Make sure you are using a GPU Colab runtime. '\n",
        "      'Go to the Runtime menu and select Choose runtime type.')\n",
        "\n",
        "print('Installing dm_control...')\n",
        "!pip install -q dm_control>=1.0.5\n",
        "\n",
        "# Configure dm_control to use the EGL rendering backend (requires GPU)\n",
        "%env MUJOCO_GL=egl\n",
        "\n",
        "print('Checking that the dm_control installation succeeded...')\n",
        "try:\n",
        "  from dm_control import suite\n",
        "  env = suite.load('cartpole', 'swingup')\n",
        "  pixels = env.physics.render()\n",
        "except Exception as e:\n",
        "  raise e from RuntimeError(\n",
        "      'Something went wrong during installation. Check the shell output above '\n",
        "      'for more information.\\n'\n",
        "      'If using a hosted Colab runtime, make sure you enable GPU acceleration '\n",
        "      'by going to the Runtime menu and selecting \"Choose runtime type\".')\n",
        "else:\n",
        "  del pixels, suite\n",
        "\n",
        "!echo Installed dm_control $(pip show dm_control | grep -Po \"(?<=Version: ).+\")\n",
        "#plotting EIM against Mojuco \n",
        "# The basic mujoco wrapper.\n",
        "from dm_control import mujoco\n",
        "# Access to enums and MuJoCo library functions.\n",
        "from dm_control.mujoco.wrapper.mjbindings import enums\n",
        "from dm_control.mujoco.wrapper.mjbindings import mjlib\n",
        "###\n",
        "import torch\n",
        "import scipy.interpolate as inter\n",
        "# PyMJCF\n",
        "from dm_control import mjcf\n",
        "\n",
        "# Composer high level imports\n",
        "from dm_control import composer\n",
        "from dm_control.composer.observation import observable\n",
        "from dm_control.composer import variation\n",
        "\n",
        "# Imports for Composer tutorial example\n",
        "from dm_control.composer.variation import distributions\n",
        "from dm_control.composer.variation import noises\n",
        "from dm_control.locomotion.arenas import floors\n",
        "\n",
        "# Control Suite\n",
        "from dm_control import suite\n",
        "\n",
        "# Run through corridor example\n",
        "from dm_control.locomotion.walkers import cmu_humanoid\n",
        "from dm_control.locomotion.arenas import corridors as corridor_arenas\n",
        "from dm_control.locomotion.tasks import corridors as corridor_tasks\n",
        "\n",
        "# Soccer\n",
        "from dm_control.locomotion import soccer\n",
        "\n",
        "# Manipulation\n",
        "from dm_control import manipulation\n",
        "\n",
        "# General\n",
        "import copy\n",
        "import os\n",
        "import itertools\n",
        "import numpy as np\n",
        "import glob\n",
        "# Graphics-related\n",
        "import matplotlib\n",
        "import matplotlib.animation as animation\n",
        "import matplotlib.pyplot as plt\n",
        "from EIM import Colors, ModelRecModWithModelVis\n",
        "import PIL.Image\n",
        "from dm_control import mujoco\n",
        "from dm_control.rl import control\n",
        "from dm_control.suite import base\n",
        "from dm_control.suite import common\n",
        "from dm_control.utils import containers\n",
        "from dm_control.utils import rewards\n",
        "from dm_control.utils import io as resources\n",
        "from IPython.display import HTML\n",
        "import torch.utils.data as data\n",
        "import re\n",
        "def substringFinder(words):\n",
        "    words.sort(key=lambda x:len(x))\n",
        "    search = words.pop(0)\n",
        "    s_len = len(search)\n",
        "    for ln in range(s_len, 0, -1):\n",
        "        for start in range(0, s_len-ln+1):\n",
        "            cand = search[start:start+ln]\n",
        "            for word in words:\n",
        "                if cand not in word:\n",
        "                    break\n",
        "            else:\n",
        "                return cand\n",
        "    return False\n",
        "\n",
        "\n",
        "\n",
        "# Use svg backend for figure rendering\n",
        "fig, ax = plt.subplots()\n",
        "image_format = 'svg' # e.g .png, .svg, etc.\n",
        "image_name = 'myimage.svg'\n",
        "\n",
        "fig.savefig(image_name, format=image_format, dpi=1200)\n",
        "\n",
        "\n",
        "# Font sizes\n",
        "SMALL_SIZE = 8\n",
        "MEDIUM_SIZE = 10\n",
        "BIGGER_SIZE = 12\n",
        "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
        "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
        "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
        "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
        "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
        "\n",
        "def display_video(frames, framerate=30, return_anim=False, filename=None):\n",
        "    #https://github.com/raymond-van/planet/blob/ebe8632967ac9f638b78d6e4fc822e0740b96c86/utils.py\n",
        "    height, width, _ = frames[0].shape\n",
        "    dpi = 70\n",
        "    orig_backend = matplotlib.get_backend()\n",
        "    matplotlib.use('Agg')  # Switch to headless 'Agg' to inhibit figure rendering.\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(width / dpi, height / dpi), dpi=dpi)\n",
        "    matplotlib.use(orig_backend)  # Switch back to the original backend.\n",
        "    ax.set_axis_off()\n",
        "    ax.set_aspect('equal')\n",
        "    ax.set_position([0, 0, 1, 1])\n",
        "    im = ax.imshow(frames[0])\n",
        "    def update(frame):\n",
        "      im.set_data(frame)\n",
        "      return [im]\n",
        "    interval = 1000/framerate\n",
        "    anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,\n",
        "                                   interval=interval, blit=True, repeat=False)\n",
        "    if is_notebook():\n",
        "        # print(\"displaying in notebook\")\n",
        "        if return_anim:\n",
        "            return anim\n",
        "        else: \n",
        "            return HTML(anim.to_jshtml())\n",
        "    else:\n",
        "        if filename is not None:\n",
        "           writervideo = animation.FFMpegWriter(fps=60) \n",
        "           anim.save(filename+\".mp4\", writer=writervideo)\n",
        "        else:\n",
        "           print(\"displaying in script\")\n",
        "           return anim\n",
        "\n",
        "# Check if running from notebook or python script\n",
        "def is_notebook():\n",
        "    try:\n",
        "        if os.environ.get('COLAB_NOTEBOOK_TEST', True):\n",
        "            return True   # notebook\n",
        "        else:\n",
        "            return False # script\n",
        "    except NameError:\n",
        "        return False\n",
        "\n",
        "def display_img(img):\n",
        "    if type(img) == torch.Tensor:\n",
        "        fig = plt.figure()\n",
        "        plt.imshow(img.permute(1,2,0))\n",
        "    else:\n",
        "        return PIL.Image.fromarray(img)\n",
        "    \n",
        "# Load the environment\n",
        "\n",
        "#cheetah, hopper, walker, swimmer, humaoid\n",
        "def xyz2pixels(xyz, camera_matrix):\n",
        "    \"\"\" Project 3D locations to pixel locations using the camera matrix \"\"\" \n",
        "    #https://github.com/rinuboney/FPAC/blob/b918755d9137fcfb77f2a855db1d3c661444bdf0/utils.py\n",
        "    xyzs = np.ones((xyz.shape[0], xyz.shape[1]+1))\n",
        "    xyzs[:, :xyz.shape[1]] = xyz\n",
        "    xs, ys, s = camera_matrix.dot(xyzs.T)\n",
        "    x, y = xs/s, ys/s\n",
        "    return x, y\n",
        "\n",
        "def get_positions(physics,duration):\n",
        "    framerate = 60\n",
        "    timevals = []\n",
        "    velocity = []\n",
        "    video_frames = []\n",
        "    # Simulate and save data\n",
        "    index_pos= physics.named.data.geom_xpos.axes.row.names\n",
        "    col_names=['x', 'y', 'z']\n",
        "    positions=[]\n",
        "    frames = []\n",
        "    physics.reset()\n",
        "    while physics.data.time < duration:\n",
        "      physics.step()\n",
        "      timevals.append(physics.data.time)\n",
        "      velocity.append(physics.data.qvel.copy())\n",
        "      positions.append(physics.named.data.qpos.copy())\n",
        "      data_pos=np.zeros((len(index_pos),len(col_names)))\n",
        "      for index, row_name in enumerate(index_pos):\n",
        "          for column, column_name in enumerate(col_names):\n",
        "              data_pos[index,column]=physics.named.data.geom_xpos[row_name, column_name]\n",
        "      frames.append(data_pos)\n",
        "      if len(video_frames) < (physics.data.time) * framerate:\n",
        "         pixels = physics.render(camera_id=1)\n",
        "         video_frames.append(pixels)\n",
        "\n",
        "    A = np.stack(frames, axis=2)\n",
        "    \"\"\"\n",
        "    dpi = 100\n",
        "    width = 480*2\n",
        "    height = 640\n",
        "    figsize = (width / dpi, height / dpi)\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "    _, ax = plt.subplots(2, 3, figsize=figsize, dpi=dpi, sharex=True)\n",
        "    ax[0,0].plot(timevals, angular_velocity)\n",
        "    ax[0,0].set_title('velocity')\n",
        "    ax[0,0].set_ylabel('meter / second')\n",
        "    for i in range(len(index_pos)):\n",
        "        ax[1,1].plot(timevals, A[i,0,:])\n",
        "    ax[1,1].set_xlabel('time (seconds)')\n",
        "    ax[1,1].set_ylabel('meters')\n",
        "    _ = ax[1,1].set_title('X')\n",
        "    for i in range(len(index_pos)):\n",
        "        ax[0,1].plot(timevals, A[i,1,:])\n",
        "    ax[0,1].set_xlabel('time (seconds)')\n",
        "    ax[0,1].set_ylabel('meters')\n",
        "    _ = ax[0,1].set_title('Y')\n",
        "    for i in range(len(index_pos)):\n",
        "        ax[1,0].plot(timevals, A[i,2,:])\n",
        "    ax[1,0].set_xlabel('time (seconds)')\n",
        "    ax[1,0].set_ylabel('meters')\n",
        "    _ = ax[1,0].set_title('height')\n",
        "    ax[1,2].plot(timevals, stem_height)\n",
        "    ax[1,2].set_xlabel('time (seconds)')\n",
        "    ax[1,2].set_ylabel('meters')\n",
        "    _ = ax[1,2].set_title('position')\n",
        "    fig.patch.set_visible(False)\n",
        "    ax[0,2].axis('off')\n",
        "    plt.tight_layout() \n",
        "    plt.show()   \n",
        "    \"\"\"\n",
        "    return positions, velocity, A , display_video(video_frames, framerate)\n",
        "\n",
        "class MujocoDataset(data.Dataset):\n",
        "\n",
        "    def __init__(self, observation, next_observation):\n",
        "        super(MujocoDataset, self).__init__()\n",
        "        self.dataset1 = observation\n",
        "        self.dataset2 =next_observation\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset1)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        obs1=self.dataset1[index]\n",
        "        obs2=self.dataset2[index]\n",
        "        return obs1, obs2\n",
        "    \n",
        "class MujocoData:\n",
        "\n",
        "    def __init__(self, observation, next_observation, num_obstacles=2, samples_per_context=None, seed=0):\n",
        "        self.data =(observation, next_observation)\n",
        "        self._num_obstacles = num_obstacles\n",
        "        self._rng = np.random.RandomState(seed)\n",
        "        self._num_obstacles = num_obstacles\n",
        "        self._context_dim = 2 * num_obstacles\n",
        "        self._sample_dim = 2 * num_obstacles #+ 2\n",
        "        if samples_per_context is not None:\n",
        "           self._samples_per_context = samples_per_context\n",
        "        else:\n",
        "          self._samples_per_context = observation.shape[0]//2\n",
        "        mujoco_dataset=MujocoDataset(observation, next_observation)\n",
        "        self.train_samples =data.DataLoader(mujoco_dataset, batch_size=observation.shape[0], shuffle=True)\n",
        "        self.test_samples = data.DataLoader(mujoco_dataset, batch_size=observation.shape[0], shuffle=True)\n",
        "    def get_spline(self, x):\n",
        "        x_ext = np.zeros(2 * self._num_obstacles + 4, dtype=x.dtype)\n",
        "        x_ext[0] = 0.0\n",
        "        x_ext[1] = 0.5\n",
        "        x_ext[2:-2] = (x + 1) / 2\n",
        "        x_ext[-2] = 1.0\n",
        "        x_ext[-1] = 0.5 #'(x[-1] + 1) / 2\n",
        "        k = \"quadratic\" if self._num_obstacles == 1 else \"cubic\"\n",
        "        return inter.interp1d(x_ext[::2], x_ext[1::2], kind=k)\n",
        "\n",
        "class MujocoModelRecMod(ModelRecModWithModelVis):\n",
        "      def __init__(self, mujoco_data, env_name, train_samples, test_samples, true_log_density=None, eval_fn=None,\n",
        "                 test_log_iters=50, save_log_iters=50):\n",
        "        super().__init__( train_samples, test_samples, true_log_density, eval_fn, test_log_iters, save_log_iters)\n",
        "        self._data = mujoco_data\n",
        "        self._env_name= env_name\n",
        "        \n",
        "      def get_model_and_assets(self):\n",
        "          \"\"\"Returns a tuple containing the model XML string and a dict of assets.\"\"\"\n",
        "          root_dir = '/home/memole/dm_control/lib/python3.8/site-packages/dm_control/suite/'\n",
        "          self.environment=dict()\n",
        "          for files in glob.glob(os.path.join(root_dir, '*.xml')):\n",
        "              head, tail = os.path.split(files)\n",
        "              x=substringFinder([self._env_name.lower(), tail])\n",
        "              if not isinstance(x, bool):\n",
        "                 if len(x)>=3:\n",
        "                    self.environment[x]=files\n",
        "          if len(self.environment)>0:\n",
        "              d = max(self.environment, key=len)\n",
        "              xml = resources.GetResource(self.environment[d])\n",
        "              return xml, common.ASSETS\n",
        "          else: \n",
        "             raise ValueError('There is no environment here.')\n",
        "         \n",
        "      def gt_keypoints(self, h, w):\n",
        "          \"\"\" Extract 2D pixel locations of objects in the environment \"\"\"\n",
        "          camera_matrix = mujoco.Camera(self.physics, height=h, width=w, camera_id=1).matrix\n",
        "          xyz = self.physics.named.data.geom_xpos.copy()\n",
        "          \"\"\"\n",
        "          head_pos = self.physics.named.data.geom_xpos['head']\n",
        "          head_mat = self.physics.named.data.geom_xmat['head'].reshape(3, 3)\n",
        "          head_size = self.physics.named.model.geom_size['head']\n",
        "          offsets = np.array([-1, 1]) * head_size[:, None]\n",
        "          xyz_local = np.stack(itertools.product(*offsets)).T\n",
        "          xyz_global = head_pos[:, None] + head_mat @ xyz_local   \n",
        "          # # Camera matrices multiply homogenous [x, y, z, 1] vectors.\n",
        "          corners_homogeneous = np.ones((4, xyz_global.shape[1]), dtype=float)\n",
        "          corners_homogeneous[:3, :] = xyz_global\n",
        "          uh, vh, zh = camera_matrix @ corners_homogeneous\n",
        "          # x and y are in the pixel coordinate system of the head.\n",
        "          xh = uh / zh\n",
        "          yh = vh / zh\n",
        "          max_width = physics.model.vis.global_.offwidth\n",
        "          max_height = physics.model.vis.global_.offheight\n",
        "          mujoco.MovableCamera(physics, height=max_height, width=max_width)\n",
        "          \"\"\"\n",
        "          return xyz2pixels(xyz, camera_matrix)\n",
        "\n",
        "      def _plot_model(self, model, title):\n",
        "            x_plt = np.arange(0, 1, 1e-2)\n",
        "            color = Colors()\n",
        "            self.physics = mujoco.Physics.from_xml_string(*self.get_model_and_assets())\n",
        "            contexts =[]\n",
        "            pixels   =[]\n",
        "            # Visualize the joint axis.\n",
        "            scene_option = mujoco.wrapper.core.MjvOption()\n",
        "            scene_option.flags[enums.mjtVisFlag.mjVIS_JOINT] = True\n",
        "            observation, next_observation=self._data\n",
        "            with self.physics.reset_context():\n",
        "               \"\"\"Returns a copy of the generalized positions (system configuration).\"\"\"\n",
        "               #position=physics.data.qpos[:]\n",
        "               pos_size = self.physics.data.qpos[:].shape[0]\n",
        "               \"\"\"Returns a copy of the generalized velocities.\"\"\"\n",
        "               #velocity=physics.data.qvel[:]\n",
        "               #observation = np.concatenate([position, velocity]).ravel()\n",
        "               self.physics.data.qpos[:], self.physics.data.qvel[:] = observation[:pos_size, ...], observation[pos_size:, ...]\n",
        "               pixels.append(self.physics.render(scene_option=scene_option, camera_id=1, segmentation=True))\n",
        "               height, width, _ = pixels[0].shape\n",
        "               \"\"\"Observations consist of an OrderedDict containing one or more NumPy arrays\"\"\"\n",
        "               contexts.append( self.gt_keypoints(height, width))\n",
        "               self.physics.data.qpos[:], self.physics.data.qvel[:] = next_observation[:pos_size, ...], next_observation[pos_size:, ...]\n",
        "               pixels.append(self.physics.render(scene_option=scene_option, camera_id=1, segmentation=True))\n",
        "               contexts.append( self.gt_keypoints(height, width))\n",
        "            fig, ax = plt.subplots(1, 1)\n",
        "            PIL.Image.fromarray(pixels[0][:,:,0])\n",
        "            for i in range(len(contexts)):\n",
        "                context= contexts[i,i+1]\n",
        "                lines=[]\n",
        "                for k, c in enumerate(model.components):\n",
        "                    m = (c.mean(context)[0] + 1) / 2\n",
        "                    cov = c.covar(context)[0]\n",
        "                    mx, my = m[::2], m[1::2]\n",
        "                    plt.scatter(200 * mx, 100 * my, c=color(k))\n",
        "                    for j in range(mx.shape[0]):\n",
        "                       mean = np.array([mx[j], my[j]])\n",
        "                       cov_j = cov[2 * j: 2 * (j + 1), 2 * j: 2 * (j + 1)]\n",
        "                       plt_cx, plt_cy = self._draw_2d_covariance(mean, cov_j, 1, return_raw=True)\n",
        "                       plt.plot(200 * plt_cx, 100 * plt_cy, c=color(k), linestyle=\"dotted\", linewidth=2)\n",
        "                    for j in range(2):\n",
        "                        s = np.array(c.sample(contexts[i:i + 1]))\n",
        "                        spline = self._data.get_spline(s[0])\n",
        "                        l, = plt.plot(200 * x_plt, 100 * spline(x_plt), c=color(k), linewidth=1)\n",
        "     \n",
        "                lines.append(l)\n",
        "                weights = model.gating_distribution.probabilities(context)[0]\n",
        "                strs = [\"{:.3f}\".format(weights[j]) for j in range(model.num_components)]\n",
        "                plt.legend(lines, strs, loc=1)\n",
        "                plt.gca().set_axis_off()\n",
        "                plt.gca().set_xlim(0, 200)\n",
        "                plt.gca().set_ylim(0, 100)\n",
        "            plt.savefig(\"EIM_out_imgs/\" + re.sub(r\"\\s+\", '-', title) + \".png\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoUIxTZjaXut",
        "outputId": "414a3826-4f0f-4665-f523-d6b62a715206"
      },
      "outputs": [],
      "source": [
        "from math import inf\n",
        "import torch\n",
        "from torch import jit\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.distributions.normal import Normal\n",
        "from torch.distributions import transforms as tT\n",
        "from torch.distributions.transformed_distribution import TransformedDistribution\n",
        "from torch.testing._internal.common_utils import TestCase\n",
        "from torch.autograd import Variable\n",
        "from collections import Counter\n",
        "import collections\n",
        "from absl import logging\n",
        "import numpy as np\n",
        "import os\n",
        "import gin\n",
        "  \n",
        "local_device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "LOG_STD_MIN = torch.tensor(-5, dtype=torch.float64, device=local_device,requires_grad=False)\n",
        "\n",
        "LOG_STD_MAX = torch.tensor(2 , dtype=torch.float64, device=local_device,requires_grad=False)\n",
        "\n",
        "def get_spec_means_mags(spec):\n",
        "  means = (spec.maximum + spec.minimum) / 2.0\n",
        "  mags = (spec.maximum - spec.minimum) / 2.0\n",
        "  means = Variable(torch.tensor(means).type(torch.FloatTensor), requires_grad=False)\n",
        "  mags  = Variable(torch.tensor(mags).type(torch.FloatTensor), requires_grad=False)\n",
        "  return means, mags\n",
        "\n",
        "class Split(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    models a split in the network. works with convolutional models (not FC).\n",
        "    specify out channels for the model to divide by n_parts.\n",
        "    \"\"\"\n",
        "    def __init__(self, module, n_parts: int, dim=1):\n",
        "        super().__init__()\n",
        "        self._n_parts = n_parts\n",
        "        self._dim = dim\n",
        "        self._module = module\n",
        "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "        self.to(device=self.device)\n",
        "    def forward(self, inputs):\n",
        "        output = self._module(inputs)\n",
        "        if output.ndim==1:\n",
        "           result=torch.hsplit(output, self._n_parts )\n",
        "        else:\n",
        "           chunk_size = output.shape[self._dim] // self._n_parts\n",
        "           result =torch.split(output, chunk_size, dim=self._dim)\n",
        "\n",
        "        return result\n",
        "      \n",
        "###############################################\n",
        "##################  Networks  #################\n",
        "###############################################\n",
        "class ActorNetwork(nn.Module):\n",
        "  \"\"\"Actor network.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      latent_spec,\n",
        "      action_spec,\n",
        "      fc_layer_params=(),\n",
        "      ):\n",
        "    super(ActorNetwork, self).__init__()\n",
        "    self._action_spec = action_spec\n",
        "    self._layers = nn.ModuleList()\n",
        "    for hidden_size in fc_layer_params:\n",
        "        if len(self._layers)==0:\n",
        "           self._layers.append(nn.Linear(latent_spec.shape[0], hidden_size))\n",
        "        else:\n",
        "           self._layers.append(nn.Linear(hidden_size, hidden_size))\n",
        "        self._layers.append(nn.ReLU())\n",
        "    output_layer = nn.Linear(hidden_size,self._action_spec.shape[0] * 2)\n",
        "    #output_layer = nn.LazyLinear(self._action_spec.shape[0] * 2)\n",
        "    self._layers.append(output_layer)\n",
        "    \n",
        "    self._action_means, self._action_mags = get_spec_means_mags(\n",
        "        self._action_spec)\n",
        "    self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    self.to(device=self.device)\n",
        "\n",
        "  def to(self, *args, **kwargs):\n",
        "      super().to(*args, **kwargs)\n",
        "\n",
        "  @property\n",
        "  def action_spec(self):\n",
        "      return self._action_spec\n",
        "\n",
        "  def _get_outputs(self, state):\n",
        "      h = state\n",
        "      \n",
        "      for l in nn.Sequential(*(list(self._layers.children())[:-1])):\n",
        "          h = l(h)\n",
        "\n",
        "      self._mean_logvar_layers = Split(\n",
        "         self._layers[-1],\n",
        "         n_parts=2,\n",
        "      )\n",
        "      mean, log_std = self._mean_logvar_layers(h)\n",
        "      \n",
        "      a_tanh_mode = torch.tanh(mean) * self._action_mags + self._action_means\n",
        "      log_std = torch.tanh(log_std)\n",
        "      log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)\n",
        "      std = torch.exp(log_std)\n",
        "      #base_distribution = torch.normal(0.0, 1.0)\n",
        "      #transforms = torch.distributions.transforms.ComposeTransform([torch.distributions.transforms.AffineTransform(loc=self._action_means, scale=self._action_mag, event_dim=mean.shape[-1]), torch.nn.Tanh(),torch.distributions.transforms.AffineTransform(loc=mean, scale=std, event_dim=mean.shape[-1])])\n",
        "      #a_distribution = torch.distributions.transformed_distribution.TransformedDistribution(base_distribution, transforms)\n",
        "      a_distribution = TransformedDistribution(\n",
        "                        base_distribution=Normal(loc=torch.full_like(mean, 0).to(device=self.device), \n",
        "                                                 scale=torch.full_like(mean, 1).to(device=self.device)), \n",
        "                        transforms=tT.ComposeTransform([\n",
        "                                   tT.AffineTransform(loc=self._action_means.to(device=self.device), scale=self._action_mags.to(device=self.device), event_dim=mean.shape[-1]), \n",
        "                                   tT.TanhTransform(cache_size=1),\n",
        "                                   tT.AffineTransform(loc=mean, scale=std, event_dim=mean.shape[-1])]))\n",
        "      #https://www.ccoderun.ca/programming/doxygen/pytorch/classtorch_1_1distributions_1_1transformed__distribution_1_1TransformedDistribution.html\n",
        "      return a_distribution, a_tanh_mode\n",
        "\n",
        "  def get_log_density(self, state, action):\n",
        "    a_dist, _ = self._get_outputs(state.to(device=self.device))\n",
        "    log_density = a_dist.log_prob(action.to(device=self.device))\n",
        "    return log_density\n",
        "\n",
        "  @property\n",
        "  def weights(self):\n",
        "    w_list = []\n",
        "    for l in self._layers:\n",
        "      w_list.append(l.weight[0])\n",
        "    return w_list\n",
        "\n",
        "  def __call__(self, state):\n",
        "    a_dist, a_tanh_mode = self._get_outputs(state.to(device=self.device))\n",
        "    a_sample = a_dist.sample()\n",
        "    log_pi_a = a_dist.log_prob(a_sample)\n",
        "    return a_tanh_mode, a_sample, log_pi_a\n",
        "\n",
        "  def sample_n(self, state, n=1):\n",
        "    a_dist, a_tanh_mode = self._get_outputs(state)\n",
        "    a_sample = a_dist.sample([n])\n",
        "    log_pi_a = a_dist.log_prob(a_sample)\n",
        "    return a_tanh_mode, a_sample, log_pi_a\n",
        "\n",
        "  def sample(self, state):\n",
        "    return self.sample_n(state, n=1)[1][0]\n",
        "\n",
        "class CriticNetwork(nn.Module):\n",
        "    \"\"\"Critic Network.\"\"\"\n",
        "    def __init__(\n",
        "      self,\n",
        "      latent_spec,\n",
        "      action_spec,\n",
        "      fc_layer_params=(),\n",
        "      ):\n",
        "      super(CriticNetwork, self).__init__()\n",
        "      self._action_spec = action_spec\n",
        "      self._layers = nn.ModuleList()\n",
        "      for hidden_size in fc_layer_params:\n",
        "          if len(self._layers)==0:\n",
        "              \n",
        "              self._layers.append(nn.Linear(latent_spec.shape[0]+action_spec.shape[0], hidden_size))\n",
        "          else:\n",
        "              self._layers.append(nn.Linear(hidden_size, hidden_size))\n",
        "          self._layers.append(nn.ReLU())\n",
        "      output_layer = nn.Linear(hidden_size,1)\n",
        "      self._layers.append(output_layer)\n",
        "      self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "      self.to(device=self.device)\n",
        "\n",
        "    def forward(self, state,action):\n",
        "        hidden = torch.cat([state.to(self.device),action.to(self.device)],dim=-1)\n",
        "        for l in self._layers:\n",
        "            hidden = l(hidden)\n",
        "        return hidden\n",
        "\n",
        "class ValueNetwork(nn.Module):\n",
        "    \"\"\"Value Network.\"\"\"\n",
        "    def __init__(\n",
        "      self,\n",
        "      latent_spec,\n",
        "      fc_layer_params=(),\n",
        "      ):\n",
        "      super(ValueNetwork, self).__init__()\n",
        "      self._latent_spec = latent_spec\n",
        "      self._layers = nn.ModuleList()\n",
        "      for hidden_size in fc_layer_params:\n",
        "          if len(self._layers)==0:\n",
        "              \n",
        "              self._layers.append(nn.Linear(latent_spec.shape[0], hidden_size))\n",
        "          else:\n",
        "              self._layers.append(nn.Linear(hidden_size, hidden_size))\n",
        "          self._layers.append(nn.ReLU())\n",
        "      output_layer = nn.Linear(hidden_size,1)\n",
        "      self._layers.append(output_layer)\n",
        "      self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "      self.to(device=self.device)\n",
        "\n",
        "    def forward(self, state):\n",
        "        hidden = state.to(device=self.device)\n",
        "        for l in self._layers:\n",
        "            hidden = l(hidden)\n",
        "        return hidden\n",
        "\n",
        "class Flags(object):\n",
        "\n",
        "  def __init__(self, **kwargs):\n",
        "    for key, val in kwargs.items():\n",
        "      if kwargs.get(key) is not None:\n",
        "         setattr(self, key, val)\n",
        "      else:\n",
        "         pass\n",
        "\n",
        "def get_modules(model_params, observation_spec, action_spec):\n",
        "  \"\"\"Gets pytorch modules for Q-function, policy, and discriminator.\"\"\"\n",
        "  model_params, n_q_fns = model_params\n",
        "  \n",
        "  if len(model_params) == 1:\n",
        "    model_params = tuple([model_params[0]] * 4)\n",
        "  elif len(model_params) < 4:\n",
        "    raise ValueError('Bad model parameters %s.' % model_params)\n",
        "  def q_net_factory():\n",
        "    return CriticNetwork(\n",
        "        observation_spec, \n",
        "        action_spec,\n",
        "        fc_layer_params=model_params[0])\n",
        "  def p_net_factory():\n",
        "    return ActorNetwork(\n",
        "        observation_spec,\n",
        "        action_spec,\n",
        "        fc_layer_params=model_params[1])\n",
        "  def c_net_factory():\n",
        "    return CriticNetwork(\n",
        "        observation_spec, \n",
        "        action_spec,\n",
        "        fc_layer_params=model_params[2])\n",
        "  def v_net_factory():\n",
        "    return ValueNetwork(\n",
        "        observation_spec, \n",
        "        fc_layer_params=model_params[3])\n",
        "  modules_list = utils.Flags(\n",
        "      q_net_factory=q_net_factory,\n",
        "      p_net_factory=p_net_factory,\n",
        "      c_net_factory=c_net_factory,\n",
        "      v_net_factory=v_net_factory,\n",
        "      n_q_fns=n_q_fns,\n",
        "      )\n",
        "  return modules_list\n",
        "#######################################\n",
        "################ AGENT ################\n",
        "#######################################\n",
        "ALPHA_MAX = 500.0\n",
        "class GeneralAgent(nn.Module):\n",
        "  \"\"\"Tensorflow module for agent.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      modules_list=None,\n",
        "      ):\n",
        "    super(GeneralAgent, self).__init__()\n",
        "    self._modules_list = modules_list\n",
        "    self._build_modules()\n",
        "    self.device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    self.to(self.device)\n",
        "\n",
        "  def _build_modules(self):\n",
        "    pass\n",
        "\n",
        "\n",
        "\n",
        "class AgentModule(GeneralAgent):\n",
        "  \"\"\"Pytorch module for BRAC dual agent.\"\"\"\n",
        "  def __init__(self,modules_list):\n",
        "    # invoking the __init__ of the parent class\n",
        "    super().__init__(modules_list)\n",
        "\n",
        "  def _build_modules(self):\n",
        "    self._q_nets = list()  \n",
        "    n_q_fns = self._modules_list.n_q_fns\n",
        "    for _ in range(n_q_fns):\n",
        "      self._q_nets.append(\n",
        "          [self._modules_list.q_net_factory(),  # Learned Q-value.\n",
        "           self._modules_list.q_net_factory(),]  # Target Q-value.\n",
        "          )\n",
        "    self._p_net = self._modules_list.p_net_factory()\n",
        "    self._c_net = self._modules_list.c_net_factory()\n",
        "    self._v_net = self._modules_list.v_net_factory() #value network\n",
        "    self._alpha_var = torch.tensor(1.0, requires_grad=True)\n",
        "    self._alpha_entropy_var = torch.tensor(1.0, requires_grad=True)\n",
        "\n",
        "  def get_alpha(self, alpha_max=ALPHA_MAX):\n",
        "    return utils.clip_v2(\n",
        "        self._alpha_var, 0.0, alpha_max)\n",
        "\n",
        "  def get_alpha_entropy(self):\n",
        "    return utils.relu_v2(self._alpha_entropy_var)\n",
        "\n",
        "  def assign_alpha(self, alpha):\n",
        "    self._alpha_var=torch.tensor(alpha, requires_grad=True)\n",
        "\n",
        "  def assign_alpha_entropy(self, alpha):\n",
        "    self._alpha_entropy_var=torch.tensor(alpha, requires_grad=True)\n",
        "\n",
        "  @property\n",
        "  def a_variables(self):\n",
        "    return [self._alpha_var]\n",
        "\n",
        "  @property\n",
        "  def ae_variables(self):\n",
        "    return [self._alpha_entropy_var]\n",
        "\n",
        "  @property\n",
        "  def q_nets(self):\n",
        "    return self._q_nets\n",
        "\n",
        "  @property\n",
        "  def q_source_weights(self):\n",
        "    q_weights = []\n",
        "    for q_net, _ in self._q_nets:\n",
        "      for name, param in q_net._layers.named_parameters():\n",
        "          if 'weight' in name:\n",
        "             q_weights += list(param)\n",
        "    return q_weights\n",
        "\n",
        "  @property\n",
        "  def q_target_weights(self):\n",
        "    q_weights = []\n",
        "    for _, q_net in self._q_nets:\n",
        "      for name, param in q_net._layers.named_parameters():\n",
        "          if 'weight' in name:\n",
        "             q_weights += list(param)\n",
        "    return q_weights\n",
        "\n",
        "  @property\n",
        "  def q_source_variables(self):\n",
        "    vars_=[]\n",
        "    for q_net, _ in self._q_nets:\n",
        "        for param in q_net._layers:\n",
        "            if not isinstance(param, nn.ReLU):\n",
        "               vars_+=list(param.parameters())\n",
        "    return vars_\n",
        " \n",
        "  @property\n",
        "  def q_target_variables(self):\n",
        "    vars_ = []\n",
        "    for _, q_net in self._q_nets:\n",
        "        for param in q_net._layers:\n",
        "            if not isinstance(param, nn.ReLU):\n",
        "               vars_+=list(param.parameters())\n",
        "    return vars_\n",
        "\n",
        "  @property\n",
        "  def p_net(self):\n",
        "    return self._p_net\n",
        "\n",
        "  def p_fn(self, s):\n",
        "    return self._p_net(s)\n",
        "\n",
        "  @property\n",
        "  def v_net(self):\n",
        "    return self._v_net\n",
        "\n",
        "  @property\n",
        "  def v_variables(self):\n",
        "    vars_ = []\n",
        "    for param in self._v_net._layers:\n",
        "        if not isinstance(param, nn.ReLU):\n",
        "               vars_+=list(param.parameters())\n",
        "    return vars_\n",
        "\n",
        "  @property\n",
        "  def v_weights(self):\n",
        "    v_weights = []\n",
        "    for name, param in self._v_net._layers.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            v_weights += list(param)\n",
        "    return v_weights\n",
        "\n",
        "  @property\n",
        "  def p_weights(self):\n",
        "    p_weights = []\n",
        "    for name, param in self._p_net._layers.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            p_weights += list(param)\n",
        "    return p_weights\n",
        "\n",
        "\n",
        "  @property\n",
        "  def p_variables(self):\n",
        "    vars_=[]\n",
        "    for param in self._p_net._layers:\n",
        "        if not isinstance(param, nn.ReLU):\n",
        "           vars_+=list(param.parameters())\n",
        "    return vars_\n",
        "\n",
        "  @property\n",
        "  def c_net(self):\n",
        "    return self._c_net\n",
        "\n",
        "  @property\n",
        "  def c_weights(self):\n",
        "    c_weights = []\n",
        "    for name, param in self._c_net._layers.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            c_weights += list(param)\n",
        "    return c_weights\n",
        "\n",
        "  @property\n",
        "  def c_variables(self):\n",
        "    vars_=[]\n",
        "    for param in self._c_net._layers:\n",
        "        if not isinstance(param, nn.ReLU):\n",
        "           vars_+=list(param.parameters())\n",
        "    return vars_\n",
        "\n",
        "class Agent(object):\n",
        "  \"\"\"Class for learning policy and interacting with environment.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      observation_spec=None,\n",
        "      action_spec=None,\n",
        "      time_step_spec=None,\n",
        "      modules=None,\n",
        "      optimizers= ((0.001, 0.5, 0.99),),\n",
        "      batch_size=64,\n",
        "      weight_decays=(0.5,),\n",
        "      update_freq=1,\n",
        "      update_rate=0.005,\n",
        "      discount=0.99,\n",
        "      env_name='HalfCheetah-v2',\n",
        "      train_data=None,\n",
        "      resume=False, \n",
        "      device=None\n",
        "      ):\n",
        "    self._latent_spec = observation_spec\n",
        "    self._action_spec = action_spec\n",
        "    self._time_step_spec = time_step_spec\n",
        "    self._modules_list = modules\n",
        "    self._optimizers = optimizers\n",
        "    self._batch_size = batch_size\n",
        "    self._weight_decays = weight_decays\n",
        "    self._train_data = train_data\n",
        "    self._update_freq = update_freq\n",
        "    self._update_rate = update_rate\n",
        "    self._discount = discount\n",
        "    self._env_name = env_name\n",
        "    self._resume = resume \n",
        "    if device is None:\n",
        "       self.device = torch.device(\n",
        "                    'cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    else:\n",
        "       self.device=device\n",
        "    \n",
        "    directory = os.getcwd()\n",
        "    checkpoint_dir=directory+\"/run\"\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "      os.makedirs(checkpoint_dir)\n",
        "    self.checkpoint_path = os.path.join(checkpoint_dir, \"checkpoint.pth\")\n",
        "    self._build_agent()\n",
        "\n",
        "  def _build_agent(self):\n",
        "    \"\"\"Builds agent components.\"\"\"\n",
        "    if len(self._weight_decays) == 1:\n",
        "       self._weight_decays = tuple([self._weight_decays[0]] * 4)\n",
        "    self._build_fns()\n",
        "    train_batch = self._get_train_batch()\n",
        "    self._global_step = torch.tensor(0.0, requires_grad=False)\n",
        "    self._init_vars(train_batch)\n",
        "    self._build_optimizers()\n",
        "    self._train_info = collections.OrderedDict()\n",
        "    self._checkpointer = self._build_checkpointer()\n",
        "    self._test_policies = collections.OrderedDict()\n",
        "    self._build_test_policies()\n",
        "    self._online_policy = self._build_online_policy()\n",
        "    \n",
        "\n",
        "  def _build_fns(self):\n",
        "    self._agent_module = AgentModule(modules_list=self._modules_list)\n",
        "\n",
        "  def _get_vars(self):\n",
        "    return []\n",
        "\n",
        "  def _build_optimizers(self):\n",
        "     opt = self._optimizers[0]\n",
        "     self._optimizer = torch.optim.Adam(\n",
        "            self._agent_module.parameters(),\n",
        "            lr= opt[0],\n",
        "            betas=(opt[1], opt[2]),\n",
        "        )\n",
        "    \n",
        "\n",
        "  def _build_loss(self, batch):\n",
        "    raise NotImplementedError\n",
        "\n",
        "  def _update_target_vars(self):\n",
        "      #?\n",
        "      # requires self._vars_learning and self._vars_target as state_dict`s\n",
        "      for var_name, var_t in self._vars_target.items():\n",
        "          updated_val = (self._update_rate\n",
        "                    * self._vars_learning[var_name].data\n",
        "                    + (1.0 - self._update_rate) * var_t.data)\n",
        "          var_t.data.copy_(updated_val)\n",
        "\n",
        "  def _build_test_policies(self):\n",
        "      raise NotImplementedError\n",
        "\n",
        "  def _build_online_policy(self):\n",
        "      return None\n",
        "  \n",
        "  #def _random_policy_fn(self, state):\n",
        "  #    return self._action_spec.sample(), None\n",
        "\n",
        "\n",
        "  @property\n",
        "  def test_policies(self):\n",
        "    return self._test_policies\n",
        "\n",
        "  @property\n",
        "  def online_policy(self):\n",
        "    return self._online_policy\n",
        "\n",
        "  def _get_train_batch(self):\n",
        "    \"\"\"Samples and constructs batch of transitions.\"\"\"\n",
        "    batch_indices = np.random.choice(self._train_data.size, self._batch_size)\n",
        "    batch_ = self._train_data.get_batch(batch_indices)\n",
        "    transition_batch = batch_\n",
        "    batch = dict(\n",
        "        s1=transition_batch.s1,\n",
        "        s2=transition_batch.s2,\n",
        "        r=transition_batch.reward,\n",
        "        dsc=transition_batch.discount,\n",
        "        a1=transition_batch.a1,\n",
        "        a2=transition_batch.a2,\n",
        "        )\n",
        "    return batch\n",
        "\n",
        "            \n",
        "  def _train_step(self):\n",
        "      train_batch = self._get_train_batch()\n",
        "      loss = self._build_loss(train_batch)\n",
        "      self._optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      self._optimizer.step()\n",
        "      self._global_step += 1\n",
        "      if self._global_step % self._update_freq == 0:\n",
        "          self._update_target_vars()\n",
        "    \n",
        "\n",
        "  def _init_vars(self, batch):\n",
        "      pass\n",
        "\n",
        "  def _get_source_target_vars(self):\n",
        "      return [], []\n",
        "\n",
        "  def _update_target_fns(self, source_vars, target_vars):\n",
        "    utils.soft_variables_update(\n",
        "        source_vars,\n",
        "        target_vars,\n",
        "        tau=self._update_rate)\n",
        "\n",
        "  def print_train_info(self):\n",
        "      summary_str = utils.get_summary_str(\n",
        "                step=self._global_step, info=self._train_info)\n",
        "      logging.info(summary_str)\n",
        "    \n",
        "\n",
        "  def write_train_summary(self, summary_writer):\n",
        "    info = self._train_info\n",
        "    step = self._global_step.numpy()\n",
        "    utils.write_summary(summary_writer, info,step)\n",
        "\n",
        "  def _build_checkpointer(self):\n",
        "      #?save\n",
        "      pass\n",
        "\n",
        "  def _load_checkpoint(self):\n",
        "      #?restore\n",
        "      pass\n",
        "\n",
        "  @property\n",
        "  def global_step(self):\n",
        "    return self._global_step.numpy()\n",
        "  \n",
        "################# Policies #################\n",
        "class GaussianRandomSoftPolicy(nn.Module):\n",
        "  \"\"\"Adds Gaussian noise to actor's action.\"\"\"\n",
        "\n",
        "  def __init__(self, a_network, std=0.1, clip_eps=1e-3):\n",
        "    super(GaussianRandomSoftPolicy, self).__init__()\n",
        "    self._a_network = a_network\n",
        "    self._std = std\n",
        "    self._clip_eps = clip_eps\n",
        "    self.device = torch.device(\n",
        "                    'cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    self.to(device=self.device)\n",
        "  def __call__(self, observation, state=()):\n",
        "    action = self._a_network(observation.to(device=self.device))[1]\n",
        "    noise = torch.normal(mean=torch.zeros(action.shape), std=self._std).to(device=self.device)\n",
        "    action = action + noise\n",
        "    spec = self._a_network.action_spec\n",
        "    action = torch.clamp(action, spec.minimum + self._clip_eps,\n",
        "                              spec.maximum - self._clip_eps)\n",
        "    return action, state\n",
        "  \n",
        "class DeterministicSoftPolicy(nn.Module):\n",
        "  \"\"\"Returns mode of policy distribution.\"\"\"\n",
        "\n",
        "  def __init__(self, a_network):\n",
        "    super(DeterministicSoftPolicy, self).__init__()\n",
        "    self._a_network = a_network\n",
        "    self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    self.to(device=self.device)\n",
        "\n",
        "\n",
        "  def __call__(self, latent_states):\n",
        "    action = self._a_network(latent_states)[0]\n",
        "    return action\n",
        "\n",
        "class RandomSoftPolicy(nn.Module):\n",
        "  \"\"\"Returns sample from policy distribution.\"\"\"\n",
        "\n",
        "  def __init__(self, a_network):\n",
        "    super(RandomSoftPolicy, self).__init__()\n",
        "    self._a_network = a_network\n",
        "    self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    self.to(device=self.device)\n",
        "\n",
        "\n",
        "  def __call__(self, latent_states):\n",
        "    action = self._a_network(latent_states)[1]\n",
        "    return action\n",
        "\n",
        "class MaxQSoftPolicy(nn.Module):\n",
        "  \"\"\"Samples a few actions from policy, returns the one with highest Q-value.\"\"\"\n",
        "\n",
        "  def __init__(self, a_network, q_network, n=10):\n",
        "    super(MaxQSoftPolicy, self).__init__()\n",
        "    self._a_network = a_network\n",
        "    self._q_network = q_network\n",
        "    self._n = n\n",
        "    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    self.to(self.device)\n",
        "  def __call__(self, latent_state):\n",
        "    \n",
        "    \n",
        "    actions = self._a_network.sample_n(latent_state.to(device=self.device), self._n)[1]\n",
        "    batch_size = actions.shape[-1]\n",
        "    actions_ = torch.reshape(actions, [self._n * batch_size, -1])\n",
        "    states_ = torch.tile(latent_state[None].to(self.device), (self._n, 1, 1))\n",
        "    states_ = torch.reshape(states_, [self._n * batch_size, -1])\n",
        "    qvals = self._q_network(states_, actions_)\n",
        "    qvals = torch.reshape(qvals, [self._n, batch_size]).to(device=self.device)\n",
        "    a_indices = torch.argmax(qvals, dim=0).to(device=self.device)\n",
        "    gather_indices = torch.stack(\n",
        "        [a_indices, torch.arange(batch_size, dtype=torch.int64).to(device=self.device)], dim=-1)\n",
        "    action = utils.gather_nd(actions, gather_indices)\n",
        "    return action\n",
        "######################################################\n",
        "###################### D2E Agent ##################### \n",
        "######################################################\n",
        "\n",
        "\n",
        "\n",
        "######################Define Agent#################### \n",
        "gin.clear_config()\n",
        "gin.config._REGISTRY.clear()\n",
        "@gin.configurable\n",
        "class D2EAgent(Agent):\n",
        "  \"\"\"dual agent class.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      alpha=1.0,\n",
        "      alpha_max=ALPHA_MAX,\n",
        "      train_alpha=False,\n",
        "      value_penalty=True,\n",
        "      target_divergence=0.0,\n",
        "      alpha_entropy=0.0,\n",
        "      train_alpha_entropy=False,\n",
        "      target_entropy=None,\n",
        "      EIM_config = ConditionalMixtureEIM.get_default_config(),\n",
        "      divergence_name='kl',\n",
        "      warm_start=2000,\n",
        "      c_iter=3,\n",
        "      ensemble_q_lambda=1.0,\n",
        "      **kwargs):\n",
        "    self._alpha = alpha\n",
        "    self._alpha_max = alpha_max\n",
        "    self._train_alpha = train_alpha\n",
        "    self._value_penalty = value_penalty\n",
        "    self._target_divergence = target_divergence\n",
        "    self._divergence_name = divergence_name\n",
        "    self._train_alpha_entropy = train_alpha_entropy\n",
        "    self._alpha_entropy = alpha_entropy\n",
        "    self._target_entropy = target_entropy\n",
        "    self._EIM_config = EIM_config\n",
        "    self._warm_start = warm_start\n",
        "    self._c_iter = c_iter\n",
        "    self._ensemble_q_lambda = ensemble_q_lambda\n",
        "    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    super(D2EAgent, self).__init__(**kwargs)\n",
        "  def _build_fns(self):\n",
        "    self._agent_module = AgentModule(modules_list=self._modules_list)\n",
        "    self._q_fns = self._agent_module.q_nets\n",
        "    self._p_fn = self._agent_module.p_fn\n",
        "    self._c_fn = self._agent_module.c_net\n",
        "    self._v_fn = self._agent_module.v_net #adding value network\n",
        "    self._divergence = utils.get_divergence(\n",
        "        name=self._divergence_name, \n",
        "        c=self._c_fn,\n",
        "        device=self.device)\n",
        "    #################################################################################################################################\n",
        "    ############ Setting Up Configuration parameters of the EIM model to compute desity ratio of the transition function ############\n",
        "    #################################################################################################################################\n",
        "    self._EIM_config.train_epochs = 20\n",
        "    print(self._latent_spec.shape)\n",
        "    self._EIM_config.num_components = self._latent_spec.shape[0]//2 #number of dimension of latent space\n",
        "\n",
        "    self._EIM_config.components_net_hidden_layers = [30, 30]\n",
        "    self._EIM_config.components_batch_size = self._latent_spec.shape[0] #is it a correct batch size???\n",
        "    self._EIM_config.components_num_epochs = 10\n",
        "    self._EIM_config.components_net_reg_loss_fact = 0.0\n",
        "    self._EIM_config.components_net_drop_prob = 0.0\n",
        "\n",
        "    self._EIM_config.gating_net_hidden_layers = [30, 30]\n",
        "    self._EIM_config.gating_batch_size = self._latent_spec.shape[0] #???\n",
        "    self._EIM_config.gating_num_epochs = 10\n",
        "    self._EIM_config.gating_net_reg_loss_fact = 0.0\n",
        "    self._EIM_config.gating_net_drop_prob = 0.0\n",
        "\n",
        "    self._EIM_config.dre_reg_loss_fact = 0.0005\n",
        "    self._EIM_config.dre_early_stopping = True\n",
        "    self._EIM_config.dre_drop_prob = 0.0\n",
        "    self._EIM_config.dre_num_iters =  10\n",
        "    self._EIM_config.dre_batch_size = self._latent_spec.shape[0] ###???\n",
        "    self._EIM_config.dre_hidden_layers = [30, 30]\n",
        "    ########################################################################################################################\n",
        "    ##########################################   END of EIM cofiguration SETUP    ##########################################\n",
        "    ########################################################################################################################\n",
        "    self._agent_module.assign_alpha(self._alpha)\n",
        "    if self._target_entropy is None:\n",
        "      self._target_entropy = - self._action_spec.shape[0]\n",
        "    self._get_alpha_entropy = self._agent_module.get_alpha_entropy\n",
        "    self._agent_module.assign_alpha_entropy(self._alpha_entropy)\n",
        "\n",
        "  def _get_alpha(self):\n",
        "    return self._agent_module.get_alpha(\n",
        "        alpha_max=self._alpha_max)\n",
        "\n",
        "  def _get_q_vars(self):\n",
        "    return self._agent_module.q_source_variables\n",
        "\n",
        "  def _get_p_vars(self):\n",
        "    return self._agent_module.p_variables\n",
        "\n",
        "  def _get_c_vars(self):\n",
        "    return self._agent_module.c_variables\n",
        "\n",
        "  def _get_v_vars(self):\n",
        "    return self._agent_module.v_variables\n",
        "  \n",
        "  def _get_q_weight_norm(self):\n",
        "    weights = self._agent_module.q_source_weights\n",
        "    norms = []\n",
        "    for w in weights:\n",
        "      norm = torch.sum(torch.square(w))\n",
        "      norms.append(norm)\n",
        "    return torch.stack(norms).sum(dim=0)\n",
        "\n",
        "  def _get_p_weight_norm(self):\n",
        "    weights = self._agent_module.p_weights\n",
        "    norms = []\n",
        "    for w in weights:\n",
        "      norm = torch.sum(torch.square(w))\n",
        "      norms.append(norm)\n",
        "    return torch.stack(norms).sum(dim=0)\n",
        "\n",
        "  def _get_c_weight_norm(self):\n",
        "    weights = self._agent_module.c_weights\n",
        "    norms = []\n",
        "    for w in weights:\n",
        "      norm = torch.sum(torch.square(w))\n",
        "      norms.append(norm)\n",
        "    return torch.stack(norms).sum(dim=0)\n",
        "\n",
        "  def _get_v_weight_norm(self):\n",
        "    weights = self._agent_module.v_weights\n",
        "    norms = []\n",
        "    for w in weights:\n",
        "      norm = torch.sum(torch.square(w))\n",
        "      norms.append(norm)\n",
        "    return torch.stack(norms).sum(dim=0)\n",
        "\n",
        "\n",
        "  def ensemble_q(self, qs):\n",
        "    lambda_ = self._ensemble_q_lambda\n",
        "    return (lambda_ *torch.min(qs, dim=-1).values\n",
        "            + (1 - lambda_) * torch.max(qs, dim=-1).values)\n",
        "\n",
        "  def _ensemble_q2_target(self, q2_targets):\n",
        "    return self.ensemble_q(q2_targets)\n",
        "\n",
        "  def _ensemble_q1(self, q1s):\n",
        "    return self.ensemble_q(q1s)\n",
        "\n",
        "  def _build_q_loss(self, batch):\n",
        "    s1 = batch['s1']\n",
        "    s2 = batch['s2']\n",
        "    a1 = batch['a1']\n",
        "    a2_b = batch['a2']\n",
        "    r = batch['r']\n",
        "    dsc = batch['dsc']\n",
        "    _, a2_p, log_pi_a2_p = self._p_fn(s2.to(device=self.device))\n",
        "    q2_targets = []\n",
        "    q1_preds = []\n",
        "    for q_fn, q_fn_target in self._q_fns:\n",
        "      q2_target_ = q_fn_target(s2.to(device=self.device), a2_p)\n",
        "      q1_pred = q_fn(s1.to(device=self.device), a1.to(device=self.device))\n",
        "      q1_preds.append(q1_pred)\n",
        "      q2_targets.append(q2_target_)\n",
        "    q2_targets = torch.stack(q2_targets, dim=-1)\n",
        "    q2_target = self._ensemble_q2_target(q2_targets)\n",
        "    div_estimate = self._divergence.dual_estimate(\n",
        "        s2.to(device=self.device), a2_p, a2_b.to(device=self.device))\n",
        "    \n",
        "    v2_target = q2_target - self._v_fn(s2.to(device=self.device))\n",
        "    if self._value_penalty:\n",
        "       v2_target = v2_target - self._get_alpha()[0] * div_estimate\n",
        "    with torch.no_grad():\n",
        "         q1_target = r.to(device=self.device) + dsc.to(device=self.device) * self._discount * v2_target\n",
        "    q_losses = []\n",
        "    for q1_pred in q1_preds:\n",
        "      q_loss_ = torch.mean(torch.square(q1_pred - q1_target))\n",
        "      q_losses.append(q_loss_)\n",
        "    q_loss = torch.sum(torch.FloatTensor(q_losses))\n",
        "    q_w_norm = self._get_q_weight_norm()\n",
        "    norm_loss = self._weight_decays[0] * q_w_norm\n",
        "    loss = q_loss + norm_loss\n",
        "\n",
        "    info = collections.OrderedDict()\n",
        "    info['q_loss'] = q_loss\n",
        "    info['q_norm'] = q_w_norm\n",
        "    info['r_mean'] = torch.mean(r)\n",
        "    info['dsc_mean'] = torch.mean(dsc)\n",
        "    info['q2_target_mean'] = torch.mean(q2_target)\n",
        "    info['q1_target_mean'] = torch.mean(q1_target)\n",
        "\n",
        "    return loss, info\n",
        "\n",
        "  def _build_p_loss(self, batch):\n",
        "    s = batch['s1']\n",
        "    a_b = batch['a1']\n",
        "    _, a_p, log_pi_a_p = self._p_fn(s)\n",
        "    v1  = self._v_fn(s.to(device=self.device))\n",
        "    q1s = []\n",
        "    for q_fn, _ in self._q_fns:\n",
        "      q1_ = q_fn(s, a_p)\n",
        "      q1s.append(q1_)\n",
        "    q1s = torch.stack(q1s, dim=-1)\n",
        "    q1 = self._ensemble_q1(q1s)\n",
        "    div_estimate = self._divergence.dual_estimate(\n",
        "        s, a_p, a_b)\n",
        "    q_start = torch.gt(self._global_step, self._warm_start).type(torch.float32)\n",
        "    p_loss = torch.mean(\n",
        "        self._get_alpha_entropy()[0] * log_pi_a_p\n",
        "        + self._get_alpha()[0] * div_estimate + v1\n",
        "        - q1 * q_start)\n",
        "    p_w_norm = self._get_p_weight_norm()\n",
        "    norm_loss = self._weight_decays[1] * p_w_norm\n",
        "    loss = p_loss + norm_loss\n",
        "\n",
        "    info = collections.OrderedDict()\n",
        "    info['p_loss'] = p_loss\n",
        "    info['p_norm'] = p_w_norm\n",
        "\n",
        "    return loss, info\n",
        "  def _recorder_EIM(self, batch):\n",
        "      s1  = batch['s1']\n",
        "      s2  = batch['s2']\n",
        "      \"\"\"configure experiment\"\"\"\n",
        "      plot_realtime = True\n",
        "      plot_save = False\n",
        "      data=MujocoData(s1,s2)\n",
        "      \"\"\"Recording\"\"\"\n",
        "      recorder_dict = {\n",
        "         RecorderKeys.TRAIN_ITER: TrainIterationRecMod(),\n",
        "         RecorderKeys.INITIAL: ConfigInitialRecMod(),\n",
        "         RecorderKeys.MODEL: MujocoModelRecMod(data,\n",
        "                                               self._env_name,\n",
        "                                               train_samples=data.train_samples,\n",
        "                                               test_samples=data.test_samples),\n",
        "         RecorderKeys.DRE: DRERecMod(torch.from_numpy(np.asarray(data.train_samples))),\n",
        "         RecorderKeys.COMPONENT_UPDATE: ComponentUpdateRecMod(plot=True, summarize=False)}\n",
        "      if self._EIM_config.num_components > 1:\n",
        "         recorder_dict[RecorderKeys.WEIGHTS_UPDATE] = WeightUpdateRecMod(plot=True)\n",
        "\n",
        "\n",
        "      return Recorder(recorder_dict, plot_realtime=plot_realtime, save=plot_save, save_path=\"rec\")\n",
        "\n",
        "\n",
        "  def _build_v_loss(self, batch):\n",
        "    s1  = batch['s1']\n",
        "    a_b = batch['a1']\n",
        "    s2  = batch['s2']\n",
        "    _, a_p, _ = self._p_fn(s1.to(device=self.device))\n",
        "    v_target = self._v_fn(s2.to(device=self.device))\n",
        "    q1_target = []\n",
        "    for q_fn, q_fn_target in self._q_fns:\n",
        "      q1_ = q_fn_target(s1.to(device=self.device), a_b.to(device=self.device))\n",
        "      q1_target.append(q1_)\n",
        "    q1_target = torch.stack(q1_target, dim=-1)\n",
        "    q1_target_ensemble = self.ensemble_q(q1_target)\n",
        "    div_estimate = self._divergence.dual_estimate(\n",
        "        s1.to(device=self.device), a_p, a_b.to(device=self.device))\n",
        "    #########################\n",
        "    #input_data next_state (s2) and state (s1) \n",
        "    self._EIM_model = ConditionalMixtureEIM(self._EIM_config, train_samples=(s1, s2), seed=torch.initial_seed(), recorder=self._recorder_EIM(batch))\n",
        "    EIM_gate_res=0\n",
        "    if self._EIM_model._model.num_components > 1:\n",
        "       EIM_gate_res =self._EIM_model.update_gating()\n",
        "    \n",
        "    EIM_res = self._EIM_model.update_components()\n",
        "    #Equation 20 in Dream to Explore paper\n",
        "    v_loss = torch.mean(q1_target_ensemble - v_target\n",
        "        - self._get_alpha()[0] * (div_estimate \n",
        "        + EIM_res + EIM_gate_res))\n",
        "    v_w_norm = self._get_v_weight_norm()\n",
        "    norm_loss = self._weight_decays[4] * v_w_norm\n",
        "    loss = v_loss + norm_loss\n",
        "\n",
        "    info = collections.OrderedDict()\n",
        "    info['value_loss'] = v_loss\n",
        "    info['vale_norm'] = v_w_norm\n",
        "    return loss, info\n",
        "\n",
        "  def _build_c_loss(self, batch):\n",
        "    s = batch['s1']\n",
        "    a_b = batch['a1']\n",
        "    _, a_p, _ = self._p_fn(s)\n",
        "    c_loss = self._divergence.dual_critic_loss(\n",
        "        s, a_p, a_b)\n",
        "    c_w_norm = self._get_c_weight_norm()\n",
        "    norm_loss = self._weight_decays[2] * c_w_norm\n",
        "    loss = c_loss + norm_loss\n",
        "\n",
        "    info = collections.OrderedDict()\n",
        "    info['c_loss'] = c_loss\n",
        "    info['c_norm'] = c_w_norm\n",
        "\n",
        "    return loss, info\n",
        "\n",
        "  def _build_a_loss(self, batch):\n",
        "    s = batch['s1']\n",
        "    a_b = batch['a1']\n",
        "    _, a_p, _ = self._p_fn(s)\n",
        "    alpha = self._get_alpha()\n",
        "    div_estimate = self._divergence.dual_estimate(\n",
        "        s, a_p, a_b)\n",
        "    a_loss = - torch.mean(alpha * (div_estimate - self._target_divergence))\n",
        "\n",
        "    info = collections.OrderedDict()\n",
        "    info['a_loss'] = a_loss\n",
        "    info['alpha'] = alpha\n",
        "    info['div_mean'] = torch.mean(div_estimate)\n",
        "    info['div_std'] = torch.std(div_estimate)\n",
        "\n",
        "    return a_loss, info\n",
        "\n",
        "  def _build_ae_loss(self, batch):\n",
        "    s = batch['s1']\n",
        "    _, _, log_pi_a = self._p_fn(s)\n",
        "    alpha = self._get_alpha_entropy()\n",
        "    ae_loss = torch.mean(alpha * (- log_pi_a - self._target_entropy))\n",
        "\n",
        "    info = collections.OrderedDict()\n",
        "    info['ae_loss'] = ae_loss\n",
        "    info['alpha_entropy'] = alpha\n",
        "\n",
        "    return ae_loss, info\n",
        "\n",
        "  def _get_source_target_vars(self):\n",
        "    return (self._agent_module.q_source_variables,\n",
        "            self._agent_module.q_target_variables)\n",
        "\n",
        "  def _build_optimizers(self):\n",
        "    opts = self._optimizers\n",
        "    \n",
        "    if len(opts) == 1:\n",
        "      opts = tuple([opts[0]] * 4)\n",
        "    elif len(opts) < 4:\n",
        "      raise ValueError('Bad optimizers %s.' % opts)\n",
        "      \n",
        "    self._q_optimizer = torch.optim.Adam(self._get_q_vars(),lr=opts[0][0], betas=(opts[0][1],opts[0][2]), weight_decay=self._weight_decays[0])\n",
        "    self._p_optimizer = torch.optim.Adam(self._get_p_vars(),lr=opts[1][0], betas=(opts[1][1],opts[1][2]), weight_decay=self._weight_decays[1])\n",
        "    self._c_optimizer = torch.optim.Adam(self._get_c_vars(),lr=opts[2][0], betas=(opts[2][1],opts[2][2]), weight_decay=self._weight_decays[2])\n",
        "    self._v_optimizer = torch.optim.Adam(self._get_v_vars(),lr=opts[3][0], betas=(opts[3][1],opts[3][2]), weight_decay=self._weight_decays[3])\n",
        "    self._a_optimizer = torch.optim.Adam(self._a_vars, lr=opts[4][0], betas=(opts[4][1],opts[4][2]))\n",
        "    self._ae_optimizer = torch.optim.Adam(self._ae_vars, lr=opts[4][0], betas=(opts[4][1],opts[4][2]))\n",
        "\n",
        "  def _optimize_step(self, batch):\n",
        "    info = collections.OrderedDict()\n",
        "    if torch.equal(self._global_step % torch.tensor(self._update_freq), torch.tensor(0,dtype=torch.float32)):\n",
        "      source_vars, target_vars = self._get_source_target_vars()\n",
        "      self._update_target_fns(source_vars, target_vars)\n",
        "    # Update policy network parameter\n",
        "    #https://bit.ly/3Bno0GC\n",
        "    # policy network's update should be done before updating q network, or there will make some errors\n",
        "    self._agent_module.p_net.train()\n",
        "    self._p_optimizer.zero_grad()\n",
        "    policy_loss,_=self._build_p_loss(batch)\n",
        "    policy_loss.backward(retain_graph=True)\n",
        "    self._p_optimizer.step()\n",
        "    # Update value network\n",
        "    self._agent_module.v_net.train()\n",
        "    self._v_optimizer.zero_grad()\n",
        "    value_loss,_=self._build_v_loss(batch)\n",
        "    value_loss.backward(retain_graph=True)\n",
        "    self._v_optimizer.step()\n",
        "    # Update q networks parameter\n",
        "    for q_fn, q_fn_target in self._q_fns:\n",
        "        q_fn.train()\n",
        "        q_fn_target.train()\n",
        "    self._q_optimizer.zero_grad()\n",
        "    q_losses,q_info= self._build_q_loss(batch)\n",
        "    q_losses.backward()\n",
        "    self._q_optimizer.step()\n",
        "    #Update critic network parameter\n",
        "    self._agent_module.c_net.train()\n",
        "    self._c_optimizer.zero_grad()\n",
        "    critic_loss,_= self._build_c_loss( batch)\n",
        "    critic_loss.backward()\n",
        "    self._c_optimizer.step()\n",
        "    #Train EIM to compute transition ratio\n",
        "    self._EIM_model.train_emm()\n",
        "    \n",
        "    if self._train_alpha:\n",
        "       self._a_optimizer.zero_grad()\n",
        "       a_loss,a_info = self._build_a_loss( batch)\n",
        "       a_loss.backward()\n",
        "       self._a_optimizer.step()\n",
        "    if self._train_alpha_entropy:\n",
        "       self._ae_optimizer.zero_grad()\n",
        "       ae_loss,ae_info = self._build_ae_loss( batch)\n",
        "       ae_loss.backward()\n",
        "       self._ae_optimizer.step()\n",
        "    #1)policy loss\n",
        "    info[\"policy_loss\"]=policy_loss.cpu().item()\n",
        "    #2)Q loss\n",
        "    info[\"Q_loss\"]=q_losses.cpu().item()\n",
        "    info[\"reward_mean\"]= q_info[\"r_mean\"].cpu().item()\n",
        "    info[\"dsc_mean\"]= q_info[\"dsc_mean\"].cpu().item()\n",
        "    info[\"q1_target_mean\"]=q_info[\"q1_target_mean\"].cpu().item()\n",
        "    info[\"q2_target_mean\"]=q_info[\"q2_target_mean\"].cpu().item()\n",
        "        #) value loss\n",
        "    info[\"value_loss\"]= value_loss.cpu().item()\n",
        "    #7) critic loss\n",
        "    info[\"critic_loss\"]=critic_loss.cpu().item()\n",
        "    #8)alpha loss\n",
        "    if self._train_alpha:\n",
        "       info[\"alpha_loss\"]=a_loss.cpu().item()\n",
        "       self._agent_module.assign_alpha(a_info[\"alpha\"])\n",
        "    #10)alpha entropy loss\n",
        "    if self._train_alpha_entropy:\n",
        "       info[\"alpha_entropy_loss\"]=ae_loss.cpu().item()\n",
        "       self._agent_module.assign_alpha_entropy(ae_info[\"alpha_entropy\"])\n",
        "    return info\n",
        "  \n",
        "  def _extra_c_step(self, batch):\n",
        "      self._c_optimizer.zero_grad()\n",
        "      critic_loss,_ = self._build_c_loss( batch)\n",
        "      critic_loss.backward()\n",
        "      self._c_optimizer.step()\n",
        "\n",
        "  def train_step(self):\n",
        "    train_batch = self._get_train_batch()\n",
        "    info = self._optimize_step(train_batch)\n",
        "    for _ in range(self._c_iter - 1):\n",
        "      train_batch = self._get_train_batch()\n",
        "      self._extra_c_step(train_batch)\n",
        "    for key, val in info.items():\n",
        "      self._train_info[key] = val\n",
        "    self._global_step.add(1)\n",
        "\n",
        "\n",
        "  def _build_test_policies(self):\n",
        "    policy = DeterministicSoftPolicy(\n",
        "        a_network=self._agent_module.p_net)\n",
        "    self._test_policies['main'] = policy\n",
        "    policy = MaxQSoftPolicy(\n",
        "        a_network=self._agent_module.p_net,\n",
        "        q_network=self._agent_module.q_nets[0][0],\n",
        "        )\n",
        "    self._test_policies['max_q'] = policy\n",
        "\n",
        "  def _build_online_policy(self):\n",
        "    return RandomSoftPolicy(\n",
        "        a_network=self._agent_module.p_net,\n",
        "        )\n",
        "\n",
        "  def _init_vars(self, batch):\n",
        "    self._build_q_loss(batch)\n",
        "    self._build_p_loss(batch)\n",
        "    self._build_v_loss(batch)\n",
        "    self._build_c_loss(batch)\n",
        "    self._q_vars = self._get_q_vars()\n",
        "    self._p_vars = self._get_p_vars()\n",
        "    self._c_vars = self._get_c_vars()\n",
        "    self._v_vars = self._get_v_vars()\n",
        "    self._a_vars = self._agent_module.a_variables\n",
        "    self._ae_vars = self._agent_module.ae_variables\n",
        "\n",
        "  def _build_checkpointer(self):\n",
        "      checkpoint = {\n",
        "            \"policy_net\": self._agent_module.p_net.state_dict(),\n",
        "            \"critic_net\": self._agent_module.c_net.state_dict(),\n",
        "            \"value_net\" : self._agent_module.v_net.state_dict(),\n",
        "            \"q_optimizer\": self._q_optimizer.state_dict(),\n",
        "            \"critic_optimizer\": self._c_optimizer.state_dict(),\n",
        "            \"policy_optimizer\": self._p_optimizer.state_dict(),\n",
        "            \"value_optimizer\": self._v_optimizer.state_dict(),\n",
        "            \"train_step\": self._global_step\n",
        "      }\n",
        "      for q_fn, q_fn_target in self._q_fns:\n",
        "          checkpoint[\"q_net\"]        = q_fn.state_dict()\n",
        "          checkpoint[\"q_net_target\"] = q_fn_target.state_dict()\n",
        "      if self._train_alpha:\n",
        "          checkpoint[\"alpha\"] = self._alpha_var\n",
        "          checkpoint[\"alpha_optimizer\"] = self._a_optimizer.state_dict()\n",
        "      if self._train_alpha_entropy:\n",
        "          checkpoint[\"alpha_entropy\"] = self._alpha_entropy_var\n",
        "          checkpoint[\"alpha_entropy_optimizer\"] = self._ae_optimizer.state_dict()   \n",
        "      torch.save(checkpoint, self.checkpoint_path)\n",
        "    \n",
        "  def _load_checkpoint(self):\n",
        "        checkpoint = torch.load(self.checkpoint_path, map_location=self.device)  # can load gpu's data on cpu machine\n",
        "        for q_fn, q_fn_target in self._q_fns:\n",
        "            q_fn.load_state_dict(checkpoint[\"q_net\"])\n",
        "            q_fn_target.load_state_dict(checkpoint[\"q_net_target\"])\n",
        "        self._agent_module.p_net.load_state_dict(checkpoint[\"policy_net\"])\n",
        "        self._agent_module.c_net.load_state_dict(checkpoint[\"critic_net\"])\n",
        "        self._agent_module.v_net.load_state_dict(checkpoint[\"value_net\"])\n",
        "        self._p_optimizer.load_state_dict(checkpoint[\"policy_optimizer1\"])\n",
        "        self._q_optimizer.load_state_dict(checkpoint[\"q_optimizer\"])\n",
        "        self._c_optimizer.load_state_dict(checkpoint[\"critic_optimizer\"])\n",
        "        self._v_optimizer.load_state_dict(checkpoint[\"value_optimizer\"])\n",
        "        self._global_step = checkpoint[\"train_step\"]\n",
        "        if self._train_alpha:\n",
        "            self._alpha_var = checkpoint[\"alpha\"]\n",
        "            self._a_optimizer.load_state_dict(checkpoint[\"alpha_optimizer\"])\n",
        "        if self._train_alpha_entropy:\n",
        "            self._alpha_entropy_var=checkpoint[\"alpha_entropy\"]\n",
        "            self._ae_optimizer.load_state_dict(checkpoint[\"alpha_entropy_optimizer\"])\n",
        "        print(\"load checkpoint from \\\"\" + self.checkpoint_path +\n",
        "              \"\\\" at \" + str(self._global_step) + \" time step\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8RdRdJBuz1_",
        "outputId": "bc775a61-fe31-4d82-d696-7af5e2db511d"
      },
      "outputs": [],
      "source": [
        "#########################\n",
        "#shorturl.at/oprZ1\n",
        "#from train_eval_utils\n",
        "from typing import Callable\n",
        "\n",
        "# TODO(wuyifan): external version for loading environments\n",
        "import gym\n",
        "import os\n",
        "\n",
        "# Get the prereqs\n",
        "!apt-get -qq update\n",
        "!apt-get -qq install -y libosmesa6-dev libgl1-mesa-glx libglfw3 libgl1-mesa-dev libglew-dev patchelf\n",
        "# Get Mujoco\n",
        "!mkdir ~/.mujoco\n",
        "!wget -q https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz -O mujoco.tar.gz\n",
        "!tar -zxf mujoco.tar.gz -C \"$HOME/.mujoco\"\n",
        "!ls $HOME/.mujoco\n",
        "!rm mujoco.tar.gz\n",
        "# Add it to the actively loaded path and the bashrc path (these only do so much)\n",
        "!echo 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HOME/.mujoco/mujoco210/bin:/usr/lib/nvidia' >> \"/root/.bashrc\" \n",
        "!echo 'export LD_PRELOAD=$LD_PRELOAD:/usr/lib/x86_64-linux-gnu/libGLEW.so' >> \"/root/.bashrc\" \n",
        "# THE ANNOYING ONE, FORCE IT INTO LDCONFIG SO WE ACTUALLY GET ACCESS TO IT THIS SESSION\n",
        "!echo \"/root/.mujoco/mujoco210/bin\" > /etc/ld.so.conf.d/mujoco_ld_lib_path.conf\n",
        "!ldconfig\n",
        "# Install Mujoco-py\n",
        "!pip3 install -U 'mujoco-py<2.2,>=2.1'\n",
        "# run once\n",
        "!touch .mujoco_setup_complete\n",
        "!pip install llvmlite\n",
        "_mujoco_run_once = False\n",
        "if not _mujoco_run_once:\n",
        "  # Add it to the actively loaded path and the bashrc path (these only do so much)\n",
        "  try:\n",
        "    os.environ['LD_LIBRARY_PATH']=os.environ['LD_LIBRARY_PATH'] + ':/root/.mujoco/mujoco210/bin:/usr/lib/nvidia:/usr/local/nvidia/lib:/usr/local/nvidia/lib64'\n",
        "  except KeyError:\n",
        "    os.environ['LD_LIBRARY_PATH']='/root/.mujoco/mujoco210/bin'\n",
        "  try:\n",
        "    os.environ['LD_PRELOAD']=os.environ['LD_PRELOAD'] + ':/usr/lib/x86_64-linux-gnu/libGLEW.so'\n",
        "  except KeyError:\n",
        "    os.environ['LD_PRELOAD']='/usr/lib/x86_64-linux-gnu/libGLEW.so'\n",
        "  # presetup so we don't see output on first env initialization\n",
        "  os.environ['MUJOCO_PY_MUJOCO_PATH'] = '/root/.mujoco/mujoco210'\n",
        "  os.environ['MUJOCO_PY_MJKEY_PATH'] = '/root/.mujoco/mjkey.txt'\n",
        "  !echo $PYTHONPATH\n",
        "  print(sys.path)\n",
        "  sys.path.insert(1, \"/root/.mujoco/\")\n",
        "  print(sys.path)\n",
        "  import mujoco_py\n",
        "  _mujoco_run_once = True\n",
        "###############\n",
        "MJC_PATH = '/root/.mujoco'\n",
        "%cd $MJC_PATH\n",
        "if not os.path.exists('mujoco-py'):\n",
        "  !git clone https://github.com/openai/mujoco-py.git\n",
        "%cd mujoco-py\n",
        "%pip install -e .\n",
        "\n",
        "## cythonize at the first import\n",
        "import mujoco_py\n",
        "!pip install sk-video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5OabnefsUIX",
        "outputId": "3aa010cb-9bd3-4e6b-edb5-84bd5979eaae"
      },
      "outputs": [],
      "source": [
        "from absl import app\n",
        "from absl import flags\n",
        "from absl import logging\n",
        "\n",
        "import gin\n",
        "import time\n",
        "\n",
        "!pip install cnest\n",
        "!pip install einops\n",
        "!pip install sacred\n",
        "#https://github.com/Haichao-Zhang/alf_randperm_reproduce/blob/d34cb1287a110135566afaa353ceb73faa2f3f91/alf/environments\n",
        "drive.mount('/content/gdrive/')\n",
        "\n",
        "sys.path.append('/content/gdrive/My Drive')\n",
        "sys.path.insert(1, r'/content/gdrive/My Drive/data_structures')\n",
        "sys.path.insert(0,'/content/drive/My Drive/ColabNotebooks')\n",
        "!ls -ltr '/content/gdrive/My Drive/alf_gym_wrapper.py'\n",
        "!cp /content/gdrive/My\\ Drive/alf_gym_wrapper.py /content\n",
        "!cp /content/gdrive/My\\ Drive/alf_environment.py /content\n",
        "!cp /content/gdrive/My\\ Drive/data_structures.py /content\n",
        "!cp /content/gdrive/My\\ Drive/nest.py /content\n",
        "!cp /content/gdrive/My\\ Drive/tensor_specs.py /content\n",
        "import alf_gym_wrapper\n",
        "import importlib\n",
        "\n",
        "def dir_path(path):\n",
        "    if os.path.isdir(path):\n",
        "        return path\n",
        "    else:\n",
        "        raise argparse.ArgumentTypeError(f\"readable_dir:{path} is not a valid path\")\n",
        "\n",
        "!ls /content/gdrive/My\\ Drive/offlinerl\n",
        "\n",
        "import argparse\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxFp3PVUwQiR",
        "outputId": "04ba1b62-63b0-40ea-9ec7-062d67fcf58b"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import numpy as np\n",
        "import os\n",
        "import gin\n",
        "import gym\n",
        "import mujoco_py\n",
        "from absl import app\n",
        "from absl import flags\n",
        "from absl import logging\n",
        "import tensor_specs\n",
        "import data_structures as ds\n",
        "import time\n",
        "import alf_gym_wrapper\n",
        "import nest\n",
        "import importlib  \n",
        "import imageio\n",
        "import mujoco_py\n",
        "from alf_environment import TimeLimit\n",
        "import torch.nn as nn\n",
        "import pickle\n",
        "class ContinuousRandomPolicy(nn.Module):\n",
        "  \"\"\"Samples actions uniformly at random.\"\"\"\n",
        "\n",
        "  def __init__(self, action_spec):\n",
        "    super(ContinuousRandomPolicy, self).__init__()\n",
        "    self._action_spec = action_spec\n",
        "\n",
        "  def __call__(self, observation, state=()):\n",
        "    action = tensor_specs.sample_bounded_spec(\n",
        "        self._action_spec, \n",
        "        #outer_dims=[observation.shape[0]]\n",
        "        )\n",
        "    return action, state\n",
        "\n",
        "PolicyConfig = collections.namedtuple(\n",
        "    'PolicyConfig', 'ptype, ckpt, wrapper, model_params')\n",
        "\n",
        "\n",
        "PTYPES = [\n",
        "   'randwalk',\n",
        "    'randinit',\n",
        "    'load',\n",
        "]\n",
        "\n",
        "WRAPPER_TYPES = [\n",
        "    'none',\n",
        "    'gaussian',\n",
        "]\n",
        "\n",
        "# params: (wrapper_type, *wrapper_params)\n",
        "# wrapper_type: none, eps, gaussian, gaussianeps\n",
        "#!rm -r /content/gdrive/My\\ Drive/testdata/Pendulum-v0/agent_partial_target\n",
        "\n",
        "def wrap_policy(a_net, wrapper):\n",
        "  \"\"\"Wraps actor network with desired randomization.\"\"\"\n",
        "  if wrapper[0] == 'none':\n",
        "    policy = RandomSoftPolicy(a_net)\n",
        "  elif wrapper[0] == 'gaussian':\n",
        "    policy = GaussianRandomSoftPolicy(\n",
        "        a_net, std=wrapper[1])\n",
        "  return policy\n",
        "\n",
        "\n",
        "def load_policy(policy_cfg, action_spec, observation_spec):\n",
        "  \"\"\"Loads policy based on config.\"\"\"\n",
        "  if policy_cfg.ptype not in PTYPES:\n",
        "    raise ValueError('Unknown policy type %s.' % policy_cfg.ptype)\n",
        "  if policy_cfg.ptype == 'randwalk':\n",
        "    policy = ContinuousRandomPolicy(action_spec)\n",
        "  elif policy_cfg.ptype in ['randinit', 'load']:\n",
        "    a_net = ActorNetwork(\n",
        "        observation_spec,\n",
        "        action_spec,\n",
        "        fc_layer_params=policy_cfg.model_params)\n",
        "    if policy_cfg.ptype == 'load' and os.path.exists(policy_cfg.ckpt):\n",
        "      logging.info('Loading policy from %s...', policy_cfg.ckpt)\n",
        "      a_net = torch.load(policy_cfg.ckpt)\n",
        "    policy = wrap_policy(a_net, policy_cfg.wrapper)\n",
        "  return policy\n",
        "\n",
        "\n",
        "def parse_policy_cfg(policy_cfg):\n",
        "  return PolicyConfig(*policy_cfg)\n",
        "\n",
        "def maybe_makedirs(log_dir):\n",
        "  import os.path\n",
        "  if not os.path.exists(log_dir):\n",
        "     os.mkdir(log_dir)\n",
        "#####################################\n",
        "#from train_eval_utils\n",
        "from typing import Callable\n",
        "Transition = collections.namedtuple(\n",
        "    'Transition', 's1, s2, a1, a2, discount, reward')\n",
        "\n",
        "import matplotlib.animation as animation\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "#animation \n",
        "def update_scene(num, frames, patch):\n",
        "    patch.set_data(frames[num])\n",
        "    return patch,\n",
        "\n",
        "def plot_animation(frames, repeat=False, interval=40):\n",
        "    plt.close()  # or else nbagg sometimes plots in the previous cell\n",
        "    fig = plt.figure()\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "    return animation.FuncAnimation(fig, update_scene, \n",
        "                                   fargs=(frames, patch),\n",
        "                                   frames=len(frames), \n",
        "                                   repeat=repeat, interval=interval)\n",
        "    \n",
        "def eval_policy_episodes(env, policy, n_episodes):\n",
        "  \"\"\"Evaluates policy performance.\"\"\"\n",
        "  results = []\n",
        "  \n",
        "  discount=[]\n",
        "  # For video / GIF.\n",
        "  for ep_idx in range(n_episodes):\n",
        "    time_step = env.reset()\n",
        "    total_rewards = 0.0\n",
        "    #env.render()\n",
        "    while not time_step.is_last():\n",
        "\n",
        "       action = policy(torch.from_numpy(time_step.observation))[0]  \n",
        "       if action.ndim<1:\n",
        "          time_step= env.step(action.unsqueeze(0).detach().cpu()) \n",
        "       else:  \n",
        "          time_step= env.step(action.detach().cpu())\n",
        "       total_rewards += time_step.reward or 0.\n",
        "       #img = env.render(mode = 'rgb_array')\n",
        "       #img = Image.fromarray(img,'RGB')\n",
        "       #img.save('/content/gdrive/My Drive/offlinerl/data/HalfCheetah_{}.jpg'.format(ep_idx))\n",
        "       results.append(total_rewards)\n",
        "       discount.append(time_step.discount)\n",
        "  results = torch.tensor(results)\n",
        "  return torch.mean(results).to(dtype=torch.float32), torch.std(results).to(dtype=torch.float32),torch.tensor(discount).to(torch.float32)\n",
        "\n",
        "\n",
        "def eval_policies(env, policies, n_episodes):\n",
        "  results_episode_return = []\n",
        "  infos = collections.OrderedDict()\n",
        "  for name, policy in policies.items():\n",
        "    mean, _ ,dsc = eval_policy_episodes(env, policy, n_episodes)\n",
        "    results_episode_return.append(mean)\n",
        "    infos[name] = collections.OrderedDict()\n",
        "    infos[name]['episode_mean'] = mean\n",
        "    infos[name]['discount'] = dsc\n",
        "  results = results_episode_return\n",
        "  return results, infos\n",
        "\n",
        "def map_structure(func: Callable, structure):\n",
        "    \n",
        "    if not callable(func):\n",
        "        raise TypeError(\"func must be callable, got: %s\" % func)\n",
        "\n",
        "    if isinstance(structure, list):\n",
        "        return [map_structure(func, item) for item in structure]\n",
        "\n",
        "    if isinstance(structure, dict):\n",
        "        return {key: map_structure(func, structure[key]) for key in structure}\n",
        "\n",
        "    return func(structure)\n",
        "\n",
        "\n",
        "###############\n",
        "\n",
        "MUJOCO_ENVS = [\n",
        "    \"Ant-v2\",\n",
        "    \"HalfCheetah-v2\",\n",
        "    \"Hopper-v2\",\n",
        "    \"Humanoid-v2\",\n",
        "    \"InvertedPendulum-v2\",\n",
        "    \"InvertedDoublePendulum-v2\",\n",
        "    \"Reacher-v2\",\n",
        "    \"Swimmer-v2\",\n",
        "    \"Walker2d-v2\"\n",
        "]\n",
        "MUJOCO_ENVS_LENNGTH={\"Ant-v2\":1000,\n",
        "    \"HalfCheetah-v2\":1000,\n",
        "    \"Hopper-v2\":1000,\n",
        "    \"Humanoid-v2\":1000,\n",
        "    \"InvertedPendulum-v2\":1000,\n",
        "    \"InvertedDoublePendulum-v2\":1000,\n",
        "    \"Reacher-v2\":50,\n",
        "    \"Swimmer-v2\":1000,\n",
        "    \"Walker2d-v2\":1000,\n",
        "    \"Pendulum-v1\":200\n",
        "    }\n",
        "\n",
        "\n",
        "def get_transition(time_step, next_time_step, action, next_action):\n",
        "  return Transition(\n",
        "      s1=time_step.observation,\n",
        "      s2=next_time_step.observation,\n",
        "      a1=action,\n",
        "      a2=next_action,\n",
        "      reward=next_time_step.reward,\n",
        "      discount=next_time_step.discount)\n",
        "  \n",
        "class DataCollector(object):\n",
        "  \"\"\"Class for collecting sequence of environment experience.\"\"\"\n",
        "\n",
        "  def __init__(self, env, policy, data):\n",
        "    self._env = env\n",
        "    self._policy = policy\n",
        "    self._data = data\n",
        "    self._saved_action = None\n",
        "\n",
        "  def collect_transition(self):\n",
        "    \"\"\"Collect single transition from environment.\"\"\"\n",
        "    time_step = self._env.current_time_step()\n",
        "    if self._saved_action is None:\n",
        "      self._saved_action = self._policy(torch.from_numpy(time_step.observation))[0]\n",
        "    action = self._saved_action\n",
        "    if action.ndim<1:\n",
        "          next_time_step = self._env.step(action.unsqueeze(0).detach().cpu())\n",
        "    else:\n",
        "          next_time_step = self._env.step(action.detach().cpu())\n",
        "    next_action = self._policy(torch.from_numpy(next_time_step.observation))[0]\n",
        "    self._saved_action = next_action\n",
        "    if not time_step.is_last():\n",
        "      transition = get_transition(time_step, next_time_step,\n",
        "                                  action, next_action)\n",
        "      \n",
        "      #time_step=time_step._replace(observation=np.expand_dims(time_step.observation, axis=0))\n",
        "\n",
        "      self._data.add_transitions(transition)\n",
        "      return 1\n",
        "    else:\n",
        "      return 0\n",
        "  \n",
        "#######################\n",
        "def gather(params, indices, axis = None):\n",
        "    if axis is None:\n",
        "        axis = 0\n",
        "    if axis < 0:\n",
        "        axis = len(params.shape) + axis\n",
        "    if axis == 0:\n",
        "        return params[indices]\n",
        "    elif axis == 1:\n",
        "        return params[:, indices]\n",
        "    elif axis == 2:\n",
        "        return params[:, :, indices]\n",
        "    elif axis == 3:\n",
        "        return params[:,:,:, indices]\n",
        "\n",
        "def scatter_update(tensor, indices, updates):\n",
        "    tensor = torch.tensor(tensor)\n",
        "    indices = torch.tensor(indices, dtype=torch.long)\n",
        "    updates = torch.tensor(updates)\n",
        "    tensor[indices] = updates\n",
        "    return tensor\n",
        "  \n",
        "class DatasetView(object):\n",
        "  \"\"\"Interface for reading from dataset.\"\"\"\n",
        "\n",
        "  def __init__(self, dataset, indices):\n",
        "    self._dataset = dataset\n",
        "    self._indices = indices\n",
        "\n",
        "  def get_batch(self, indices):\n",
        "    real_indices = self._indices[indices]\n",
        "    return self._dataset.get_batch(real_indices)\n",
        "\n",
        "  @property\n",
        "  def size(self):\n",
        "    return self._indices.shape[0]\n",
        "\n",
        "def save_copy(data, ckpt_name):\n",
        "  \"\"\"Creates a copy of the current data and save as a checkpoint.\"\"\"\n",
        "  new_data = Dataset(\n",
        "      observation_spec=data.config['observation_spec'],\n",
        "      action_spec=data.config['action_spec'],\n",
        "      size=data.size,\n",
        "      circular=False)\n",
        "  full_batch = data.get_batch(np.arange(data.size))\n",
        "  new_data.add_transitions(full_batch)\n",
        "  torch.save(new_data, ckpt_name+\".pt\")\n",
        "  \n",
        "class Dataset(nn.Module):\n",
        "  \"\"\"Tensorflow module of dataset of transitions.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      observation_spec,\n",
        "      action_spec,\n",
        "      size,\n",
        "      circular=True,\n",
        "      ):\n",
        "    super(Dataset, self).__init__()\n",
        "    self._size = size\n",
        "    self._circular = circular\n",
        "    obs_shape = list(observation_spec.shape)\n",
        "    obs_type = observation_spec.dtype\n",
        "    action_shape = list(action_spec.shape)\n",
        "    action_type = action_spec.dtype\n",
        "    self._s1 = self._zeros([size] + obs_shape, obs_type)\n",
        "    self._s2 = self._zeros([size] + obs_shape, obs_type)\n",
        "    self._a1 = self._zeros([size] + action_shape, action_type)\n",
        "    self._a2 = self._zeros([size] + action_shape, action_type)\n",
        "    self._discount = self._zeros([size], torch.float32)\n",
        "    self._reward = self._zeros([size], torch.float32)\n",
        "    self._data = Transition(\n",
        "        s1=self._s1, s2=self._s2, a1=self._a1, a2=self._a2,\n",
        "        discount=self._discount, reward=self._reward)\n",
        "    self._current_size = torch.tensor(0, dtype=torch.int32,requires_grad=False)\n",
        "    self._current_idx = torch.tensor(0, dtype=torch.int32, requires_grad=False)\n",
        "    self._capacity = torch.autograd.Variable(torch.tensor(self._size))\n",
        "    self._config = collections.OrderedDict(\n",
        "        observation_spec=observation_spec,\n",
        "        action_spec=action_spec,\n",
        "        size=size,\n",
        "        circular=circular)\n",
        "\n",
        "  @property\n",
        "  def config(self):\n",
        "    return self._config\n",
        "\n",
        "  def create_view(self, indices):\n",
        "    return DatasetView(self, indices)\n",
        "\n",
        "  def get_batch(self, indices):\n",
        "    indices = torch.LongTensor(indices)\n",
        "    def get_batch_(data_):\n",
        "      return gather(data_, indices)\n",
        "    transition_batch = nest.map_structure(get_batch_, self._data)\n",
        "    return transition_batch\n",
        "\n",
        "  @property\n",
        "  def data(self):\n",
        "    return self._data\n",
        "\n",
        "  @property\n",
        "  def capacity(self):\n",
        "    return self._size\n",
        "\n",
        "  @property\n",
        "  def size(self):\n",
        "    return self._current_size.numpy()\n",
        "\n",
        "  def _zeros(self, shape, dtype):\n",
        "    \"\"\"Create a variable initialized with zeros.\"\"\"\n",
        "    return torch.autograd.Variable(torch.zeros(shape, dtype = dtype))\n",
        "\n",
        "  def add_transitions(self, transitions,):\n",
        "    assert isinstance(transitions, Transition)\n",
        "    for i in transitions._fields:\n",
        "        attr=getattr(transitions,i)\n",
        "        if torch.is_tensor(attr):\n",
        "           attr=attr.detach().cpu()\n",
        "        #print(torch.is_tensor(attr))\n",
        "        transitions=transitions._replace(**{i:np.expand_dims(attr,axis=0)})\n",
        "    #transitions=transitions._replace(s1=np.expand_dims(transitions.s1,axis=0))\n",
        "    #print(f\"new transition:{transitions}\")\n",
        "    batch_size = transitions.s1.shape[0]\n",
        "    effective_batch_size = torch.minimum( torch.tensor(batch_size), torch.tensor(self._size - self._current_idx))\n",
        "    indices = self._current_idx + torch.arange(effective_batch_size)\n",
        "    for key in transitions._asdict().keys():\n",
        "        data = getattr(self._data, key)\n",
        "        batch = getattr(transitions, key)\n",
        "        data[indices]= torch.tensor(batch[:effective_batch_size])\n",
        "        #scatter_update(data, indices, batch[:effective_batch_size])\n",
        "    # Update size and index.\n",
        "    if torch.less(self._current_size, self._size):\n",
        "      self._current_size+=effective_batch_size\n",
        "    self._current_idx+=effective_batch_size\n",
        "    if self._circular:\n",
        "      if torch.greater_equal(self._current_idx, self._size):\n",
        "        self._current_idx=0\n",
        "#########################\n",
        "#utils.py\n",
        "def shuffle_indices_with_steps(n, steps=1, rand=None):\n",
        "  \"\"\"Randomly shuffling indices while keeping segments.\"\"\"\n",
        "  if steps == 0:\n",
        "    return np.arange(n)\n",
        "  if rand is None:\n",
        "    rand = np.random\n",
        "  n_segments = int(n // steps)\n",
        "  n_effective = n_segments * steps\n",
        "  batch_indices = rand.permutation(n_segments)\n",
        "  batches = np.arange(n_effective).reshape([n_segments, steps])\n",
        "  shuffled_batches = batches[batch_indices]\n",
        "  shuffled_indices = np.arange(n)\n",
        "  shuffled_indices[:n_effective] = shuffled_batches.reshape([-1])\n",
        "  return shuffled_indices\n",
        "\n",
        "\n",
        "#########################\n",
        "#collect_data.py\n",
        "\n",
        "def dir_path(path):\n",
        "    if os.path.isdir(path):\n",
        "        return path\n",
        "    else:\n",
        "        raise argparse.ArgumentTypeError(f\"readable_dir:{path} is not a valid path\")\n",
        "import shutil\n",
        "if not os.path.exists(\"/content/configs\"):\n",
        "   shutil.copytree('/content/gdrive/My Drive/configs', \"/content/configs\")\n",
        "#shutil.copy('/content/gdrive/My Drive/configs/D2E_example.py', \"/content\")\n",
        "\n",
        "if not os.path.exists('/content/gdrive/My Drive/offlinerl/data'):\n",
        "  os.makedirs('/content/gdrive/My Drive/offlinerl/data')\n",
        "else:\n",
        "  pass\n",
        "\n",
        "if not os.path.exists('/content/gdrive/My Drive/testdata/Pendulum-v0'):\n",
        "     os.makedirs('/content/gdrive/My Drive/testdata/Pendulum-v0/')\n",
        "else:\n",
        "  pass\n",
        "\n",
        "import argparse\n",
        "parser = argparse.ArgumentParser(description='BRAC')\n",
        "parser.add_argument('--root_offlinerl_dir', type=dir_path, default='/content/gdrive/My Drive/offlinerl/data', help='Root directory for saving data')\n",
        "parser.add_argument('--sub_offlinerl_dir', type=str, default=None, help='sub directory for saving data.')\n",
        "parser.add_argument('--test_srcdir', type=str, default=None, help='directory for saving test data.')\n",
        "parser.add_argument('--env_name', type=str, default='HalfCheetah-v2',help = 'env name.')\n",
        "parser.add_argument('--data_name', type=str, default='random', help = 'data name.')\n",
        "parser.add_argument('--env_loader', type=str, default='mujoco', help = 'env loader, suite/gym.')\n",
        "parser.add_argument('--config_dir', type=str, default='configs', help = 'config file dir.')\n",
        "parser.add_argument('--config_file', type=str, default='d2e_pure', help = 'config file name.')\n",
        "parser.add_argument('--policy_root_dir', type=str, default=None,help = 'Directory in which to find the behavior policy.')\n",
        "parser.add_argument('--n_samples', type=int, default=int(1e2), help = 'number of transitions to collect.')\n",
        "parser.add_argument('--n_eval_episodes', type=int, default=20, help = 'number episodes to eval each policy.')\n",
        "parser.add_argument(\"--gin_file\", type=str, default=[], nargs='*', help = 'Paths to the gin-config files.')\n",
        "\n",
        "parser.add_argument('--gin_bindings', type=str, default=[], nargs='*', help = 'Gin binding parameters.')\n",
        "args = parser.parse_args()\n",
        "if not os.path.exists(\"/content/gdrive/My Drive/offlinerl/data/HalfCheetah-v2/example/0\"):\n",
        "   os.makedirs(\"/content/gdrive/My Drive/offlinerl/data/HalfCheetah-v2/example/0\")\n",
        "if not os.path.exists(\"/content/gdrive/My Drive/offlinerl/data/Pendulum-v1/example/0\"):\n",
        "   os.makedirs(\"/content/gdrive/My Drive/offlinerl/data/Pendulum-v1/example/0\")\n",
        "def get_sample_counts(n, distr):\n",
        "  \"\"\"Provides size of each sub-dataset based on desired distribution.\"\"\"\n",
        "  distr = torch.tensor(distr)\n",
        "  distr = distr / torch.sum(distr)\n",
        "  counts = []\n",
        "  remainder = n\n",
        "  for i in range(distr.shape[0] - 1):\n",
        "    count = int(n * distr[i])\n",
        "    remainder -= count\n",
        "    counts.append(count)\n",
        "  counts.append(remainder)\n",
        "  return counts\n",
        "\n",
        "\n",
        "def collect_n_transitions(tf_env, policy, data, n, log_freq=1000):\n",
        "  \"\"\"Adds desired number of transitions to dataset.\"\"\"\n",
        "  collector = DataCollector(tf_env, policy, data)\n",
        "  time_st = time.time()\n",
        "  timed_at_step = 0\n",
        "  steps_collected = 0\n",
        "  while steps_collected < n:\n",
        "    count = collector.collect_transition()\n",
        "    steps_collected += count\n",
        "    if (steps_collected % log_freq == 0\n",
        "        or steps_collected == n) and count > 0:\n",
        "      steps_per_sec = ((steps_collected - timed_at_step)\n",
        "                       / (time.time() - time_st))\n",
        "      timed_at_step = steps_collected\n",
        "      time_st = time.time()\n",
        "      logging.info('(%d/%d) steps collected at %.4g steps/s.', steps_collected,\n",
        "                   n, steps_per_sec)\n",
        "\n",
        "\n",
        "\n",
        "def collect_data(\n",
        "    log_dir,\n",
        "    data_config,\n",
        "    n_samples=int(1e3),\n",
        "    env_name='HalfCheetah-v2',\n",
        "    log_freq=int(1e2),\n",
        "    n_eval_episodes=20,\n",
        "    ):\n",
        "  \"\"\"\n",
        "               **** Main function ****\n",
        "  Creates dataset of transitions based on desired config.\n",
        "  \"\"\"\n",
        "  seed=0\n",
        "  torch.manual_seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  dm_env = gym.spec(env_name).make()\n",
        "  env = alf_gym_wrapper.AlfGymWrapper(dm_env, discount=0.99)\n",
        "  env = TimeLimit(env, MUJOCO_ENVS_LENNGTH[env_name])\n",
        "  observation_spec = env.observation_spec()\n",
        "  action_spec = env.action_spec()\n",
        "  \n",
        "\n",
        "  # Initialize dataset.\n",
        "  sample_sizes = list([cfg[-1] for cfg in data_config])\n",
        "  \n",
        "  sample_sizes = get_sample_counts(n_samples, sample_sizes)\n",
        "  logging.info(\", \".join([\"%s\" % s for s in sample_sizes]))\n",
        "  data = Dataset(\n",
        "        observation_spec,\n",
        "        action_spec,\n",
        "        n_samples,\n",
        "        circular=False)\n",
        "  \n",
        "  # Collect data for each policy in data_config.\n",
        "  time_st = time.time()\n",
        "  test_results = collections.OrderedDict()\n",
        "  for (policy_name, policy_cfg, _), n_transitions in zip(\n",
        "      data_config, sample_sizes):\n",
        "    policy_cfg = parse_policy_cfg(policy_cfg)\n",
        "    policy = load_policy(policy_cfg, action_spec, observation_spec)\n",
        "    logging.info('Testing policy %s...', policy_name)\n",
        "    eval_mean, eval_std,_ = eval_policy_episodes(\n",
        "        env, policy, n_eval_episodes)\n",
        "    test_results[policy_name] = [eval_mean, eval_std]\n",
        "    logging.info('Return mean %.4g, std %.4g.', eval_mean, eval_std)\n",
        "    logging.info('Collecting data from policy %s...', policy_name)\n",
        "    collect_n_transitions(env, policy, data, n_transitions, log_freq)\n",
        "  # Save final dataset.\n",
        "  assert data.size == data.capacity\n",
        "  data_ckpt_name = os.path.join(log_dir, 'data_{}.pt'.format(env_name))\n",
        "  torch.save([data.capacity, data.state_dict()], data_ckpt_name)\n",
        "  whole_data_ckpt_name = os.path.join(log_dir, 'data_{}.pth'.format(env_name))\n",
        "  with open( whole_data_ckpt_name, 'wb') as filehandler: \n",
        "    pickle.dump(data, filehandler)\n",
        "\n",
        "  time_cost = time.time() - time_st\n",
        "  logging.info('Finished: %d transitions collected, '\n",
        "               'saved at %s, '\n",
        "               'time cost %.4gs.', n_samples, data_ckpt_name, time_cost)\n",
        "\n",
        "\n",
        "def main(args):\n",
        "  logging.set_verbosity(logging.INFO)\n",
        "  sub_dir = args.sub_offlinerl_dir\n",
        "  log_dir = os.path.join(\n",
        "      args.root_offlinerl_dir,\n",
        "      args.env_name,\n",
        "      args.data_name,\n",
        "      sub_dir,\n",
        "      )\n",
        "  maybe_makedirs(log_dir)\n",
        "  #print(args.config_dir)\n",
        "  #print(args.n_samples)\n",
        "  config_module = importlib.import_module(\n",
        "      '{}.{}'.format(args.config_dir, args.config_file))\n",
        "  collect_data(\n",
        "      log_dir=log_dir,\n",
        "      data_config=config_module.get_data_config(args.env_name,\n",
        "                                                args.policy_root_dir),\n",
        "      n_samples = args.n_samples,\n",
        "      env_name  = args.env_name,\n",
        "      n_eval_episodes=args.n_eval_episodes)\n",
        "\n",
        "def collect_gym_data(args):\n",
        "    args.sub_offlinerl_dir = '0'\n",
        "    args.env_name = 'Pendulum-v1'\n",
        "    args.data_name = 'example'\n",
        "    args.config_file = 'D2E_example'\n",
        "    data_dir = 'testdata'\n",
        "    args.test_srcdir = os.getcwd()\n",
        "    args.policy_root_dir = os.path.join(args.test_srcdir,\n",
        "                                               data_dir)\n",
        "    args.n_samples = 50000  # Short collection.\n",
        "    args.n_eval_episodes = 1\n",
        "    main(args)\n",
        "if __name__ == \"__main__\":\n",
        "  args = parser.parse_args(sys.argv[1:])\n",
        "  collect_gym_data(args)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXZey9iovFOT"
      },
      "outputs": [],
      "source": [
        "class AgentConfig(object):\n",
        "  \"\"\"Class for handling agent parameters.\"\"\"\n",
        "\n",
        "  def __init__(self, agent_flags):\n",
        "    self._agent_flags = agent_flags\n",
        "    self._agent_args = self._get_agent_args()\n",
        "\n",
        "  def _get_agent_args(self):\n",
        "    \"\"\"Gets agent parameters associated with config.\"\"\"\n",
        "    agent_flags = self._agent_flags\n",
        "    agent_args = Flags(\n",
        "        observation_spec=agent_flags.observation_spec,\n",
        "        action_spec=agent_flags.action_spec,\n",
        "        optimizers=agent_flags.optimizers,\n",
        "        batch_size=agent_flags.batch_size,\n",
        "        weight_decays=agent_flags.weight_decays,\n",
        "        update_rate=agent_flags.update_rate,\n",
        "        update_freq=agent_flags.update_freq,\n",
        "        discount=agent_flags.discount,\n",
        "        train_data=agent_flags.train_data,\n",
        "        )\n",
        "    agent_args.modules = self._get_modules()\n",
        "    return agent_args\n",
        "\n",
        "  def _get_modules(self):\n",
        "    raise NotImplementedError\n",
        "\n",
        "  @property\n",
        "  def agent_args(self):\n",
        "    return self._agent_args\n",
        "  \n",
        "class Config(AgentConfig):\n",
        "\n",
        "  def _get_modules(self):\n",
        "    return get_modules(\n",
        "        self._agent_flags.model_params,\n",
        "        self._agent_flags.observation_spec,\n",
        "        self._agent_flags.action_spec)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlyO0vJiI5pP"
      },
      "source": [
        "install deepmind control suite  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8ZX4jbziwBg9",
        "outputId": "46369c24-918a-44f5-873b-1fc6a14219e4"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import datetime\n",
        "import re\n",
        "import gin\n",
        "import dill\n",
        "\n",
        "# When using Gin interactively, reregistering a function is not an error.\n",
        "gin.enter_interactive_mode()\n",
        "gin.clear_config()\n",
        "def get_datetime():\n",
        "  now = datetime.datetime.now().isoformat()\n",
        "  now = re.sub(r'\\D', '', now)[:-6]\n",
        "  return now\n",
        "\n",
        "\n",
        "@gin.configurable(\"train_eval_offline\")\n",
        "def train_eval_offline(\n",
        "    # Basic args.\n",
        "    log_dir,\n",
        "    data_file,\n",
        "    env_name='HalfCheetah-v2',\n",
        "    n_train=int(1e6),\n",
        "    shuffle_steps=0,\n",
        "    seed=0,\n",
        "    use_seed_for_data=False,\n",
        "    # Train and eval args.\n",
        "    total_train_steps=int(1e6),\n",
        "    summary_freq=100,\n",
        "    print_freq=1000,\n",
        "    save_freq=int(2e4),\n",
        "    eval_freq=5000,\n",
        "    n_eval_episodes=20,\n",
        "    # Agent args.\n",
        "    model_params=(((200, 200),), 2),\n",
        "    optimizers=(( 0.0001, 0.5, 0.99),),\n",
        "    batch_size=256,\n",
        "    weight_decays=(0.0,),\n",
        "    update_freq=1,\n",
        "    update_rate=0.005,\n",
        "    discount=0.99,\n",
        "    ):\n",
        "  ###Training a policy with a fixed dataset.###\n",
        "  # Create tf_env to get specs.\n",
        "  dm_env = gym.spec(env_name).make()\n",
        "  env = alf_gym_wrapper.AlfGymWrapper(dm_env,discount=discount)\n",
        "  env = TimeLimit(env, MUJOCO_ENVS_LENNGTH[env_name])\n",
        "  observation_spec = env.observation_spec()\n",
        "  action_spec = env.action_spec()\n",
        "  \n",
        "  # Prepare data.\n",
        "  logging.info('Loading data from %s ...', data_file)\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  \n",
        "  \n",
        "  data_ckpt_name = os.path.join(data_file, 'data_{}.pt'.format(env_name))\n",
        "  whole_data_ckpt_name = os.path.join(data_file, 'data_{}.pth'.format(env_name))\n",
        "  \n",
        "  data_size, state = torch.load(data_ckpt_name, map_location=device)\n",
        "  \n",
        "  #full_data = Dataset(observation_spec, action_spec, data_size)\n",
        "  scores = {} # scores is an empty dict already\n",
        "\n",
        "  if os.path.getsize(whole_data_ckpt_name) > 0:       \n",
        "    with open(whole_data_ckpt_name, \"rb\") as f:\n",
        "        unpickler = pickle.Unpickler(f)\n",
        "        # if file is not empty scores will be equal\n",
        "        # to the value unpickled\n",
        "        scores = unpickler.load()\n",
        "  \n",
        "  full_data=scores\n",
        "  for k, v in full_data._config.items():\n",
        "        if k =='observation_spec':\n",
        "          full_data._config['observation_spec']=observation_spec\n",
        "        elif k=='action_spec':\n",
        "          full_data._config['action_spec']=action_spec\n",
        "   \n",
        "  # Split data.\n",
        "  n_train = min(n_train, full_data.size)\n",
        "  logging.info('n_train %d.', n_train)\n",
        "  if use_seed_for_data:\n",
        "    rand = np.random.RandomState(seed)\n",
        "  else:\n",
        "    rand = np.random.RandomState(0)\n",
        "  shuffled_indices = shuffle_indices_with_steps(\n",
        "      n=full_data.size, steps=shuffle_steps, rand=rand)\n",
        "  train_indices = shuffled_indices[:n_train]\n",
        "  train_data = full_data.create_view(train_indices)\n",
        "\n",
        "  # Create agent.\n",
        "  agent_flags = Flags(\n",
        "      observation_spec=observation_spec,\n",
        "      action_spec=action_spec,\n",
        "      model_params=model_params,\n",
        "      optimizers=optimizers,\n",
        "      batch_size=batch_size,\n",
        "      weight_decays=weight_decays,\n",
        "      update_freq=update_freq,\n",
        "      update_rate=update_rate,\n",
        "      discount=discount,\n",
        "      env_name=env_name,\n",
        "      train_data=train_data)\n",
        "  agent_args = Config(agent_flags).agent_args\n",
        "  agent = D2EAgent(**vars(agent_args)) #ATTENTION: Debugg ====> should it be D2EAgent here??\n",
        "  agent_ckpt_name = os.path.join(log_dir, 'agent')\n",
        "\n",
        "  # Restore agent from checkpoint if there exists one.\n",
        "  if os.path.exists('{}.index'.format(agent_ckpt_name)):\n",
        "    logging.info('Checkpoint found at %s.', agent_ckpt_name)\n",
        "    torch.load(agent, agent_ckpt_name)\n",
        "\n",
        "  # Train agent.\n",
        "  train_summary_dir = os.path.join(log_dir, 'train')\n",
        "  eval_summary_dir = os.path.join(log_dir, 'eval')\n",
        "  train_summary_writer = SummaryWriter(\n",
        "      logdir=train_summary_dir)\n",
        "  eval_summary_writers = collections.OrderedDict()\n",
        "  for policy_key in agent.test_policies.keys():\n",
        "    eval_summary_writer = SummaryWriter(\n",
        "        logdir=os.path.join(eval_summary_dir, policy_key))\n",
        "    eval_summary_writers[policy_key] = eval_summary_writer\n",
        "  eval_results = []\n",
        "\n",
        "  time_st_total = time.time()\n",
        "  time_st = time.time()\n",
        "  step = agent.global_step\n",
        "  timed_at_step = step\n",
        "  while step < total_train_steps:\n",
        "    agent.train_step()\n",
        "    step = agent.global_step\n",
        "    if step % summary_freq == 0 or step == total_train_steps:\n",
        "      agent.write_train_summary(train_summary_writer)\n",
        "    if step % print_freq == 0 or step == total_train_steps:\n",
        "      agent.print_train_info()\n",
        "    if step % eval_freq == 0 or step == total_train_steps:\n",
        "      time_ed = time.time()\n",
        "      time_cost = time_ed - time_st\n",
        "      logging.info(\n",
        "          'Training at %.4g steps/s.', (step - timed_at_step) / time_cost)\n",
        "      eval_result, eval_infos = eval_policies(\n",
        "          env, agent.test_policies, n_eval_episodes)\n",
        "      eval_results.append([step] + eval_result)\n",
        "      logging.info('Testing at step %d:', step)\n",
        "      for policy_key, policy_info in eval_infos.items():\n",
        "        logging.info(utils.get_summary_str(\n",
        "            step=None, info=policy_info, prefix=policy_key+': '))\n",
        "        utils.write_summary(eval_summary_writers[policy_key], policy_info, step)\n",
        "      time_st = time.time()\n",
        "      timed_at_step = step\n",
        "    if step % save_freq == 0:\n",
        "       agent.checkpoint_path=agent_ckpt_name\n",
        "       agent._build_checkpointer()\n",
        "       #for k,v in agent.__dict__.items():\n",
        "       #   if k in ['_agent_module','_q_fns','_p_fn','_c_fn']:\n",
        "       #      print(v)\n",
        "       #torch.save(agent, agent_ckpt_name,pickle_protocol=pickle.HIGHEST_PROTOCOL)\n",
        "       logging.info('Agent saved at %s.', agent_ckpt_name)\n",
        "\n",
        "  agent._build_checkpointer()\n",
        "  time_cost = time.time() - time_st_total\n",
        "  logging.info('Training finished, time cost %.4gs.', time_cost)\n",
        "  return torch.tensor(eval_results)\n",
        "if not os.path.exists('/content/gdrive/My Drive/offlinerl/learn'):\n",
        "  os.makedirs('/content/gdrive/My Drive/offlinerl/learn')\n",
        "else:\n",
        "  pass\n",
        "\n",
        "##############################\n",
        "#/content/gdrive/My Drive/offlinerl/data\n",
        "\n",
        "###train_offline.py#/content/gdrive/My Drive/offlinerl/data/Pendulum-v1/example/0/data_Pendulum-v1.pt\n",
        "parser = argparse.ArgumentParser(description='BRAC')\n",
        "parser.add_argument('--data_root_offlinerl_dir', type=dir_path, default='/content/gdrive/My Drive/offlinerl/data/',\n",
        "                     help='Root directory for data.')\n",
        "parser.add_argument('--data_sub_offlinerl_dir',type=str, default=None, help= '')\n",
        "parser.add_argument('--test_srcdir', type=str, default='/content/gdrive/My Drive', help='directory for saving test data.')\n",
        "parser.add_argument('--data_name', type=str, default='eps1',help= 'data name.')\n",
        "parser.add_argument('--data_file_name', type=str, default='',help= 'data checkpoint file name.')\n",
        "\n",
        "# Flags for offline training.\n",
        "parser.add_argument('--root_dir',type=dir_path, default= '/content/gdrive/My Drive/offlinerl/learn',\n",
        "                    help='Root directory for writing logs/summaries/checkpoints.')\n",
        "parser.add_argument('--sub_dir', type=str, default='0', help='')\n",
        "\n",
        "parser.add_argument('--agent_name',  type=str, default='BRAC', help='agent name.')\n",
        "parser.add_argument('--env_name', type=str, default='HalfCheetah-v2',help = 'env name.')\n",
        "parser.add_argument('--seed', type=int, default=0, help='random seed, mainly for training samples.')\n",
        "parser.add_argument('--total_train_steps', type=int, default=int(5e5), help='')\n",
        "parser.add_argument('--n_eval_episodes',type=int, default= 20,help= '')\n",
        "parser.add_argument('--n_train', type=int, default=int(1e6),help= '')\n",
        "parser.add_argument(\"--gin_file\", type=str, default=[], nargs='*', help = 'Paths to the gin-config files.')\n",
        "\n",
        "parser.add_argument('--gin_bindings', type=str, default=[], nargs='*', help = 'Gin binding parameters.')\n",
        "args = parser.parse_args()\n",
        "\n",
        "\n",
        "def main(args):\n",
        "  logging.set_verbosity(logging.INFO)\n",
        "  \n",
        "  # Setup data file path.\n",
        "  data_dir = os.path.join(\n",
        "      args.data_root_offlinerl_dir,\n",
        "      args.env_name,\n",
        "      args.data_name,\n",
        "      args.data_sub_offlinerl_dir,\n",
        "      )\n",
        "  data_file = os.path.join(\n",
        "      data_dir, args.data_file_name)\n",
        "  logging.info('Data directory %s.', args.data_root_offlinerl_dir)\n",
        "  # Setup log dir.\n",
        "  if args.sub_dir == 'auto':\n",
        "    sub_dir = get_datetime()\n",
        "  else:\n",
        "    sub_dir = args.sub_dir\n",
        "  log_dir = os.path.join(\n",
        "      args.data_root_offlinerl_dir,\n",
        "      args.env_name,\n",
        "      args.data_name,\n",
        "      'n'+str(args.n_train),\n",
        "      args.agent_name,\n",
        "      sub_dir,\n",
        "      str(args.seed),\n",
        "      )\n",
        "  \n",
        "  if not os.path.exists(log_dir):\n",
        "     os.makedirs(log_dir)\n",
        "  else:\n",
        "    pass\n",
        "  train_eval_offline(\n",
        "      log_dir=log_dir,\n",
        "      data_file=data_file,\n",
        "      env_name=args.env_name,\n",
        "      n_train=args.n_train,\n",
        "      total_train_steps=args.total_train_steps,\n",
        "      n_eval_episodes=args.n_eval_episodes,\n",
        "      )\n",
        "\n",
        "def  Train_offline_brac(args):\n",
        "    \n",
        "    data_dir = 'offlinerl/data'\n",
        "    #args.test_srcdir = os.getcwd()\n",
        "    args.data_root_offlinerl_dir = os.path.join(args.test_srcdir, data_dir)\n",
        "    args.data_sub_offlinerl_dir = '0'\n",
        "    args.env_name = 'Pendulum-v1'\n",
        "    args.data_name = 'example'\n",
        "    args.agent_name = 'BRAC'\n",
        "    args.gin_bindings = [\n",
        "        'train_eval_offline.model_params=((200, 200),)',\n",
        "        'train_eval_offline.optimizers=((\"adam\", 5e-4),)']\n",
        "    args.n_train = 100\n",
        "    args.n_eval_episodes = 20\n",
        "    args.total_train_steps = 100  # Short training.\n",
        "     \n",
        "    main(args)  # Just test that it runs.\n",
        "if __name__ == \"__main__\":\n",
        "  #args = parser.parse_args(sys.argv[1:])\n",
        "  args, unknown = parser.parse_known_args()\n",
        "  gin.parse_config_files_and_bindings([], args.gin_bindings,finalize_config=False, skip_unknown=True, print_includes_and_imports=True)\n",
        "  Train_offline_brac(args)\n",
        "  gin.clear_config()\n",
        "  gin.config._REGISTRY.clear()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_9MDTjZZTe5"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\n",
        "# For security purposes, please check the contents of collect_env.py before running it.\n",
        "!python collect_env.py"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMIkkAH/X9XGq9zs8oElGtU",
      "collapsed_sections": [],
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.7.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "e41d4561028816b5428aebb83a83c059bdf12e0224e471a2c21e6a82f3a5abf0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
