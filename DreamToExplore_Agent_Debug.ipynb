{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2017soft/AIME/blob/start-with-brac/DreamToExplore_Agent_Debug.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swo1QNejwSmS",
        "outputId": "6c0581b9-0ed0-4627-b08c-de0355a62ad5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:16: DeprecationWarning: invalid escape sequence \\ \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: CUDA_VISIBLE_DEVICES=0\n",
            "Mounted at /content/gdrive\n",
            "cp: cannot open '/content/gdrive/My Drive/torchsample/setup.py.gdoc' for reading: Operation not supported\n",
            "running build_ext\n",
            "cythoning gsl.pyx to gsl.cpp\n",
            "error: /content/gdrive/My Drive/gsl.pyx\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.7/dist-packages (2.5.1)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.19.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: expecttest in /usr/local/lib/python3.7/dist-packages (0.1.3)\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.argv = sys.argv[:1]\n",
        "%set_env CUDA_VISIBLE_DEVICES=0\n",
        "from google.colab import drive\n",
        "drive.flush_and_unmount()\n",
        "drive.mount('/content/gdrive')\n",
        "os.chdir('/content/gdrive/My Drive')\n",
        "!cp /content/gdrive/My\\ Drive/EIM.py /content\n",
        "!cp /content/gdrive/My\\ Drive/planner_regularizer_EIM.py /content\n",
        "!cp -r /content/gdrive/My\\ Drive/torchsample /content/\n",
        "!cp /content/gdrive/My\\ Drive/ObstacleData.py /content/\n",
        "!python /content/gdrive/My\\ Drive/setup.py build_ext --swig-opts=\"-modern -I../include\"\n",
        "!pip install tensorboardX\n",
        "import torchsample\n",
        "sys.path.append('/content/gdrive/My\\ Drive')\n",
        "import utils_planner as utils\n",
        "!pip install expecttest\n",
        "from tensorboardX import SummaryWriter\n",
        "import torch\n",
        "from EIM import ConditionalMixtureEIM, RecorderKeys, TrainIterationRecMod, ConfigInitialRecMod, DRERecMod, ComponentUpdateRecMod, WeightUpdateRecMod, Recorder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "if subprocess.run('nvidia-smi').returncode:\n",
        "  raise RuntimeError(\n",
        "      'Cannot communicate with GPU. '\n",
        "      'Make sure you are using a GPU Colab runtime. '\n",
        "      'Go to the Runtime menu and select Choose runtime type.')\n",
        "\n",
        "print('Installing dm_control...')\n",
        "!pip install -q dm_control>=1.0.5\n",
        "\n",
        "# Configure dm_control to use the EGL rendering backend (requires GPU)\n",
        "%env MUJOCO_GL=egl\n",
        "\n",
        "print('Checking that the dm_control installation succeeded...')\n",
        "try:\n",
        "  from dm_control import suite\n",
        "  env = suite.load('cartpole', 'swingup')\n",
        "  pixels = env.physics.render()\n",
        "except Exception as e:\n",
        "  raise e from RuntimeError(\n",
        "      'Something went wrong during installation. Check the shell output above '\n",
        "      'for more information.\\n'\n",
        "      'If using a hosted Colab runtime, make sure you enable GPU acceleration '\n",
        "      'by going to the Runtime menu and selecting \"Choose runtime type\".')\n",
        "else:\n",
        "  del pixels, suite\n",
        "\n",
        "!echo Installed dm_control $(pip show dm_control | grep -Po \"(?<=Version: ).+\")\n",
        "#plotting EIM against Mojuco \n",
        "# The basic mujoco wrapper.\n",
        "from dm_control import mujoco\n",
        "# Access to enums and MuJoCo library functions.\n",
        "from dm_control.mujoco.wrapper.mjbindings import enums\n",
        "from dm_control.mujoco.wrapper.mjbindings import mjlib\n",
        "###\n",
        "import torch\n",
        "import scipy.interpolate as inter\n",
        "# PyMJCF\n",
        "from dm_control import mjcf\n",
        "\n",
        "# Composer high level imports\n",
        "from dm_control import composer\n",
        "from dm_control.composer.observation import observable\n",
        "from dm_control.composer import variation\n",
        "\n",
        "# Imports for Composer tutorial example\n",
        "from dm_control.composer.variation import distributions\n",
        "from dm_control.composer.variation import noises\n",
        "from dm_control.locomotion.arenas import floors\n",
        "\n",
        "# Control Suite\n",
        "from dm_control import suite\n",
        "\n",
        "# Run through corridor example\n",
        "from dm_control.locomotion.walkers import cmu_humanoid\n",
        "from dm_control.locomotion.arenas import corridors as corridor_arenas\n",
        "from dm_control.locomotion.tasks import corridors as corridor_tasks\n",
        "\n",
        "# Soccer\n",
        "from dm_control.locomotion import soccer\n",
        "\n",
        "# Manipulation\n",
        "from dm_control import manipulation\n",
        "\n",
        "# General\n",
        "import copy\n",
        "import os\n",
        "import itertools\n",
        "import numpy as np\n",
        "import glob\n",
        "# Graphics-related\n",
        "import matplotlib\n",
        "import matplotlib.animation as animation\n",
        "import matplotlib.pyplot as plt\n",
        "from EIM import Colors, ModelRecModWithModelVis\n",
        "import PIL.Image\n",
        "from dm_control import mujoco\n",
        "from dm_control.rl import control\n",
        "from dm_control.suite import base\n",
        "from dm_control.suite import common\n",
        "from dm_control.utils import containers\n",
        "from dm_control.utils import rewards\n",
        "from dm_control.utils import io as resources\n",
        "from IPython.display import HTML\n",
        "import torch.utils.data as data\n",
        "import re\n",
        "def substringFinder(words):\n",
        "    words.sort(key=lambda x:len(x))\n",
        "    search = words.pop(0)\n",
        "    s_len = len(search)\n",
        "    for ln in range(s_len, 0, -1):\n",
        "        for start in range(0, s_len-ln+1):\n",
        "            cand = search[start:start+ln]\n",
        "            for word in words:\n",
        "                if cand not in word:\n",
        "                    break\n",
        "            else:\n",
        "                return cand\n",
        "    return False\n",
        "\n",
        "\n",
        "\n",
        "# Use svg backend for figure rendering\n",
        "fig, ax = plt.subplots()\n",
        "image_format = 'svg' # e.g .png, .svg, etc.\n",
        "image_name = 'myimage.svg'\n",
        "\n",
        "fig.savefig(image_name, format=image_format, dpi=1200)\n",
        "\n",
        "\n",
        "# Font sizes\n",
        "SMALL_SIZE = 8\n",
        "MEDIUM_SIZE = 10\n",
        "BIGGER_SIZE = 12\n",
        "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
        "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
        "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
        "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
        "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
        "\n",
        "def display_video(frames, framerate=30, return_anim=False, filename=None):\n",
        "    #https://github.com/raymond-van/planet/blob/ebe8632967ac9f638b78d6e4fc822e0740b96c86/utils.py\n",
        "    height, width, _ = frames[0].shape\n",
        "    dpi = 70\n",
        "    orig_backend = matplotlib.get_backend()\n",
        "    matplotlib.use('Agg')  # Switch to headless 'Agg' to inhibit figure rendering.\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(width / dpi, height / dpi), dpi=dpi)\n",
        "    matplotlib.use(orig_backend)  # Switch back to the original backend.\n",
        "    ax.set_axis_off()\n",
        "    ax.set_aspect('equal')\n",
        "    ax.set_position([0, 0, 1, 1])\n",
        "    im = ax.imshow(frames[0])\n",
        "    def update(frame):\n",
        "      im.set_data(frame)\n",
        "      return [im]\n",
        "    interval = 1000/framerate\n",
        "    anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,\n",
        "                                   interval=interval, blit=True, repeat=False)\n",
        "    if is_notebook():\n",
        "        # print(\"displaying in notebook\")\n",
        "        if return_anim:\n",
        "            return anim\n",
        "        else: \n",
        "            return HTML(anim.to_jshtml())\n",
        "    else:\n",
        "        if filename is not None:\n",
        "           writervideo = animation.FFMpegWriter(fps=60) \n",
        "           anim.save(filename+\".mp4\", writer=writervideo)\n",
        "        else:\n",
        "           print(\"displaying in script\")\n",
        "           return anim\n",
        "\n",
        "# Check if running from notebook or python script\n",
        "def is_notebook():\n",
        "    try:\n",
        "        if os.environ.get('COLAB_NOTEBOOK_TEST', True):\n",
        "            return True   # notebook\n",
        "        else:\n",
        "            return False # script\n",
        "    except NameError:\n",
        "        return False\n",
        "\n",
        "def display_img(img):\n",
        "    if type(img) == torch.Tensor:\n",
        "        fig = plt.figure()\n",
        "        plt.imshow(img.permute(1,2,0))\n",
        "    else:\n",
        "        return PIL.Image.fromarray(img)\n",
        "    \n",
        "# Load the environment\n",
        "\n",
        "#cheetah, hopper, walker, swimmer, humaoid\n",
        "def xyz2pixels(xyz, camera_matrix):\n",
        "    \"\"\" Project 3D locations to pixel locations using the camera matrix \"\"\" \n",
        "    #https://github.com/rinuboney/FPAC/blob/b918755d9137fcfb77f2a855db1d3c661444bdf0/utils.py\n",
        "    xyzs = np.ones((xyz.shape[0], xyz.shape[1]+1))\n",
        "    xyzs[:, :xyz.shape[1]] = xyz\n",
        "    xs, ys, s = camera_matrix.dot(xyzs.T)\n",
        "    x, y = xs/s, ys/s\n",
        "    return x, y\n",
        "\n",
        "def get_positions(physics,duration):\n",
        "    framerate = 60\n",
        "    timevals = []\n",
        "    velocity = []\n",
        "    video_frames = []\n",
        "    # Simulate and save data\n",
        "    index_pos= physics.named.data.geom_xpos.axes.row.names\n",
        "    col_names=['x', 'y', 'z']\n",
        "    positions=[]\n",
        "    frames = []\n",
        "    physics.reset()\n",
        "    while physics.data.time < duration:\n",
        "      physics.step()\n",
        "      timevals.append(physics.data.time)\n",
        "      velocity.append(physics.data.qvel.copy())\n",
        "      positions.append(physics.named.data.qpos.copy())\n",
        "      data_pos=np.zeros((len(index_pos),len(col_names)))\n",
        "      for index, row_name in enumerate(index_pos):\n",
        "          for column, column_name in enumerate(col_names):\n",
        "              data_pos[index,column]=physics.named.data.geom_xpos[row_name, column_name]\n",
        "      frames.append(data_pos)\n",
        "      if len(video_frames) < (physics.data.time) * framerate:\n",
        "         pixels = physics.render(camera_id=1)\n",
        "         video_frames.append(pixels)\n",
        "\n",
        "    A = np.stack(frames, axis=2)\n",
        "    \"\"\"\n",
        "    dpi = 100\n",
        "    width = 480*2\n",
        "    height = 640\n",
        "    figsize = (width / dpi, height / dpi)\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "    _, ax = plt.subplots(2, 3, figsize=figsize, dpi=dpi, sharex=True)\n",
        "    ax[0,0].plot(timevals, angular_velocity)\n",
        "    ax[0,0].set_title('velocity')\n",
        "    ax[0,0].set_ylabel('meter / second')\n",
        "    for i in range(len(index_pos)):\n",
        "        ax[1,1].plot(timevals, A[i,0,:])\n",
        "    ax[1,1].set_xlabel('time (seconds)')\n",
        "    ax[1,1].set_ylabel('meters')\n",
        "    _ = ax[1,1].set_title('X')\n",
        "    for i in range(len(index_pos)):\n",
        "        ax[0,1].plot(timevals, A[i,1,:])\n",
        "    ax[0,1].set_xlabel('time (seconds)')\n",
        "    ax[0,1].set_ylabel('meters')\n",
        "    _ = ax[0,1].set_title('Y')\n",
        "    for i in range(len(index_pos)):\n",
        "        ax[1,0].plot(timevals, A[i,2,:])\n",
        "    ax[1,0].set_xlabel('time (seconds)')\n",
        "    ax[1,0].set_ylabel('meters')\n",
        "    _ = ax[1,0].set_title('height')\n",
        "    ax[1,2].plot(timevals, stem_height)\n",
        "    ax[1,2].set_xlabel('time (seconds)')\n",
        "    ax[1,2].set_ylabel('meters')\n",
        "    _ = ax[1,2].set_title('position')\n",
        "    fig.patch.set_visible(False)\n",
        "    ax[0,2].axis('off')\n",
        "    plt.tight_layout() \n",
        "    plt.show()   \n",
        "    \"\"\"\n",
        "    return positions, velocity, A , display_video(video_frames, framerate)\n",
        "\n",
        "class MujocoDataset(data.Dataset):\n",
        "\n",
        "    def __init__(self, observation, next_observation):\n",
        "        super(MujocoDataset, self).__init__()\n",
        "        self.dataset1 = observation\n",
        "        self.dataset2 =next_observation\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset1)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        obs1=self.dataset1[index]\n",
        "        obs2=self.dataset2[index]\n",
        "        return obs1, obs2\n",
        "    \n",
        "class MujocoData:\n",
        "\n",
        "    def __init__(self, observation, next_observation, num_obstacles=2, samples_per_context=None, seed=0):\n",
        "        self.data =(observation, next_observation)\n",
        "        self._num_obstacles = num_obstacles\n",
        "        self._rng = np.random.RandomState(seed)\n",
        "        self._num_obstacles = num_obstacles\n",
        "        self._context_dim = 2 * num_obstacles\n",
        "        self._sample_dim = 2 * num_obstacles #+ 2\n",
        "        if samples_per_context is not None:\n",
        "           self._samples_per_context = samples_per_context\n",
        "        else:\n",
        "          self._samples_per_context = observation.shape[0]//2\n",
        "        mujoco_dataset=MujocoDataset(observation, next_observation)\n",
        "        self.train_samples =data.DataLoader(mujoco_dataset, batch_size=observation.shape[0], shuffle=True)\n",
        "        self.test_samples = data.DataLoader(mujoco_dataset, batch_size=observation.shape[0], shuffle=True)\n",
        "    def get_spline(self, x):\n",
        "        x_ext = np.zeros(2 * self._num_obstacles + 4, dtype=x.dtype)\n",
        "        x_ext[0] = 0.0\n",
        "        x_ext[1] = 0.5\n",
        "        x_ext[2:-2] = (x + 1) / 2\n",
        "        x_ext[-2] = 1.0\n",
        "        x_ext[-1] = 0.5 #'(x[-1] + 1) / 2\n",
        "        k = \"quadratic\" if self._num_obstacles == 1 else \"cubic\"\n",
        "        return inter.interp1d(x_ext[::2], x_ext[1::2], kind=k)\n",
        "\n",
        "class MujocoModelRecMod(ModelRecModWithModelVis):\n",
        "      def __init__(self, mujoco_data, env_name, train_samples, test_samples, true_log_density=None, eval_fn=None,\n",
        "                 test_log_iters=50, save_log_iters=50):\n",
        "        super().__init__( train_samples, test_samples, true_log_density, eval_fn, test_log_iters, save_log_iters)\n",
        "        self._data = mujoco_data\n",
        "        self._env_name= env_name\n",
        "        \n",
        "      def get_model_and_assets(self):\n",
        "          \"\"\"Returns a tuple containing the model XML string and a dict of assets.\"\"\"\n",
        "          root_dir = '/home/memole/dm_control/lib/python3.8/site-packages/dm_control/suite/'\n",
        "          self.environment=dict()\n",
        "          for files in glob.glob(os.path.join(root_dir, '*.xml')):\n",
        "              head, tail = os.path.split(files)\n",
        "              x=substringFinder([self._env_name.lower(), tail])\n",
        "              if not isinstance(x, bool):\n",
        "                 if len(x)>=3:\n",
        "                    self.environment[x]=files\n",
        "          if len(self.environment)>0:\n",
        "              d = max(self.environment, key=len)\n",
        "              xml = resources.GetResource(self.environment[d])\n",
        "              return xml, common.ASSETS\n",
        "          else: \n",
        "             raise ValueError('There is no environment here.')\n",
        "         \n",
        "      def gt_keypoints(self, h, w):\n",
        "          \"\"\" Extract 2D pixel locations of objects in the environment \"\"\"\n",
        "          camera_matrix = mujoco.Camera(self.physics, height=h, width=w, camera_id=1).matrix\n",
        "          xyz = self.physics.named.data.geom_xpos.copy()\n",
        "          \"\"\"\n",
        "          head_pos = self.physics.named.data.geom_xpos['head']\n",
        "          head_mat = self.physics.named.data.geom_xmat['head'].reshape(3, 3)\n",
        "          head_size = self.physics.named.model.geom_size['head']\n",
        "          offsets = np.array([-1, 1]) * head_size[:, None]\n",
        "          xyz_local = np.stack(itertools.product(*offsets)).T\n",
        "          xyz_global = head_pos[:, None] + head_mat @ xyz_local   \n",
        "          # # Camera matrices multiply homogenous [x, y, z, 1] vectors.\n",
        "          corners_homogeneous = np.ones((4, xyz_global.shape[1]), dtype=float)\n",
        "          corners_homogeneous[:3, :] = xyz_global\n",
        "          uh, vh, zh = camera_matrix @ corners_homogeneous\n",
        "          # x and y are in the pixel coordinate system of the head.\n",
        "          xh = uh / zh\n",
        "          yh = vh / zh\n",
        "          max_width = physics.model.vis.global_.offwidth\n",
        "          max_height = physics.model.vis.global_.offheight\n",
        "          mujoco.MovableCamera(physics, height=max_height, width=max_width)\n",
        "          \"\"\"\n",
        "          return xyz2pixels(xyz, camera_matrix)\n",
        "\n",
        "      def _plot_model(self, model, title):\n",
        "            x_plt = np.arange(0, 1, 1e-2)\n",
        "            color = Colors()\n",
        "            self.physics = mujoco.Physics.from_xml_string(*self.get_model_and_assets())\n",
        "            contexts =[]\n",
        "            pixels   =[]\n",
        "            # Visualize the joint axis.\n",
        "            scene_option = mujoco.wrapper.core.MjvOption()\n",
        "            scene_option.flags[enums.mjtVisFlag.mjVIS_JOINT] = True\n",
        "            observation, next_observation=self._data\n",
        "            with self.physics.reset_context():\n",
        "               \"\"\"Returns a copy of the generalized positions (system configuration).\"\"\"\n",
        "               #position=physics.data.qpos[:]\n",
        "               pos_size = self.physics.data.qpos[:].shape[0]\n",
        "               \"\"\"Returns a copy of the generalized velocities.\"\"\"\n",
        "               #velocity=physics.data.qvel[:]\n",
        "               #observation = np.concatenate([position, velocity]).ravel()\n",
        "               self.physics.data.qpos[:], self.physics.data.qvel[:] = observation[:pos_size, ...], observation[pos_size:, ...]\n",
        "               pixels.append(self.physics.render(scene_option=scene_option, camera_id=1, segmentation=True))\n",
        "               height, width, _ = pixels[0].shape\n",
        "               \"\"\"Observations consist of an OrderedDict containing one or more NumPy arrays\"\"\"\n",
        "               contexts.append( self.gt_keypoints(height, width))\n",
        "               self.physics.data.qpos[:], self.physics.data.qvel[:] = next_observation[:pos_size, ...], next_observation[pos_size:, ...]\n",
        "               pixels.append(self.physics.render(scene_option=scene_option, camera_id=1, segmentation=True))\n",
        "               contexts.append( self.gt_keypoints(height, width))\n",
        "            fig, ax = plt.subplots(1, 1)\n",
        "            PIL.Image.fromarray(pixels[0][:,:,0])\n",
        "            for i in range(len(contexts)):\n",
        "                context= contexts[i,i+1]\n",
        "                lines=[]\n",
        "                for k, c in enumerate(model.components):\n",
        "                    m = (c.mean(context)[0] + 1) / 2\n",
        "                    cov = c.covar(context)[0]\n",
        "                    mx, my = m[::2], m[1::2]\n",
        "                    plt.scatter(200 * mx, 100 * my, c=color(k))\n",
        "                    for j in range(mx.shape[0]):\n",
        "                       mean = np.array([mx[j], my[j]])\n",
        "                       cov_j = cov[2 * j: 2 * (j + 1), 2 * j: 2 * (j + 1)]\n",
        "                       plt_cx, plt_cy = self._draw_2d_covariance(mean, cov_j, 1, return_raw=True)\n",
        "                       plt.plot(200 * plt_cx, 100 * plt_cy, c=color(k), linestyle=\"dotted\", linewidth=2)\n",
        "                    for j in range(2):\n",
        "                        s = np.array(c.sample(contexts[i:i + 1]))\n",
        "                        spline = self._data.get_spline(s[0])\n",
        "                        l, = plt.plot(200 * x_plt, 100 * spline(x_plt), c=color(k), linewidth=1)\n",
        "     \n",
        "                lines.append(l)\n",
        "                weights = model.gating_distribution.probabilities(context)[0]\n",
        "                strs = [\"{:.3f}\".format(weights[j]) for j in range(model.num_components)]\n",
        "                plt.legend(lines, strs, loc=1)\n",
        "                plt.gca().set_axis_off()\n",
        "                plt.gca().set_xlim(0, 200)\n",
        "                plt.gca().set_ylim(0, 100)\n",
        "            plt.savefig(\"EIM_out_imgs/\" + re.sub(r\"\\s+\", '-', title) + \".png\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "e4Fu5syCNlJf",
        "outputId": "a337bc72-057c-4eec-d54a-7ecf6ea1f6c1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing dm_control...\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.2+zzzcolab20220719082949 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.5 which is incompatible.\n",
            "tensorflow-metadata 1.10.0 requires protobuf<4,>=3.13, but you have protobuf 4.21.5 which is incompatible.\n",
            "tensorboardx 2.5.1 requires protobuf<=3.20.1,>=3.8.0, but you have protobuf 4.21.5 which is incompatible.\n",
            "google-cloud-bigquery-storage 1.1.2 requires protobuf<4.0.0dev, but you have protobuf 4.21.5 which is incompatible.\n",
            "google-api-core 1.31.6 requires protobuf<4.0.0dev,>=3.12.0; python_version > \"3\", but you have protobuf 4.21.5 which is incompatible.\u001b[0m\n",
            "env: MUJOCO_GL=egl\n",
            "Checking that the dm_control installation succeeded...\n",
            "Installed dm_control 1.0.7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAANT0lEQVR4nO3cYYjkd33H8ffHO1NpjKb0VpC706T00njYQtIlTRFqirZc8uDugUXuIFgleGAbKVWEFEuU+MiGWhCu1ZOKVdAYfSALntwDjQTEC7chNXgXItvTeheFrDHNk6Ax7bcPZtKdrneZf3Zndy/7fb/gYP7/+e3Mlx97752d2ZlUFZKk7e8VWz2AJGlzGHxJasLgS1ITBl+SmjD4ktSEwZekJqYGP8lnkzyZ5PuXuD5JPplkKcmjSW6c/ZiSpPUa8gj/c8CBF7n+VmDf+N9R4F/WP5YkadamBr+qHgR+/iJLDgGfr5FTwNVJXj+rASVJs7FzBrexGzg/cXxhfO6nqxcmOcrotwCuvPLKP7z++utncPeS1MfDDz/8s6qaW8vXziL4g1XVceA4wPz8fC0uLm7m3UvSy16S/1zr187ir3SeAPZOHO8Zn5MkXUZmEfwF4F3jv9a5GXimqn7t6RxJ0taa+pROki8BtwC7klwAPgK8EqCqPgWcAG4DloBngfds1LCSpLWbGvyqOjLl+gL+emYTSZI2hO+0laQmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqYlBwU9yIMnjSZaS3HWR69+Q5IEkjyR5NMltsx9VkrQeU4OfZAdwDLgV2A8cSbJ/1bK/B+6vqhuAw8A/z3pQSdL6DHmEfxOwVFXnquo54D7g0Ko1BbxmfPm1wE9mN6IkaRaGBH83cH7i+ML43KSPArcnuQCcAN5/sRtKcjTJYpLF5eXlNYwrSVqrWb1oewT4XFXtAW4DvpDk1267qo5X1XxVzc/Nzc3oriVJQwwJ/hPA3onjPeNzk+4A7geoqu8CrwJ2zWJASdJsDAn+aWBfkmuTXMHoRdmFVWt+DLwNIMmbGAXf52wk6TIyNfhV9TxwJ3ASeIzRX+OcSXJPkoPjZR8E3pvke8CXgHdXVW3U0JKkl27nkEVVdYLRi7GT5+6euHwWeMtsR5MkzZLvtJWkJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNTEo+EkOJHk8yVKSuy6x5p1JziY5k+SLsx1TkrReO6ctSLIDOAb8GXABOJ1koarOTqzZB/wd8JaqejrJ6zZqYEnS2gx5hH8TsFRV56rqOeA+4NCqNe8FjlXV0wBV9eRsx5QkrdeQ4O8Gzk8cXxifm3QdcF2S7yQ5leTAxW4oydEki0kWl5eX1zaxJGlNZvWi7U5gH3ALcAT4TJKrVy+qquNVNV9V83NzczO6a0nSEEOC/wSwd+J4z/jcpAvAQlX9qqp+CPyA0Q8ASdJlYkjwTwP7klyb5ArgMLCwas3XGD26J8kuRk/xnJvhnJKkdZoa/Kp6HrgTOAk8BtxfVWeS3JPk4HjZSeCpJGeBB4APVdVTGzW0JOmlS1VtyR3Pz8/X4uLilty3JL1cJXm4qubX8rW+01aSmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmBgU/yYEkjydZSnLXi6x7R5JKMj+7ESVJszA1+El2AMeAW4H9wJEk+y+y7irgb4CHZj2kJGn9hjzCvwlYqqpzVfUccB9w6CLrPgZ8HPjFDOeTJM3IkODvBs5PHF8Yn/s/SW4E9lbV11/shpIcTbKYZHF5efklDytJWrt1v2ib5BXAJ4APTltbVcerar6q5ufm5tZ715Kkl2BI8J8A9k4c7xmfe8FVwJuBbyf5EXAzsOALt5J0eRkS/NPAviTXJrkCOAwsvHBlVT1TVbuq6pqqugY4BRysqsUNmViStCZTg19VzwN3AieBx4D7q+pMknuSHNzoASVJs7FzyKKqOgGcWHXu7kusvWX9Y0mSZs132kpSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmhgU/CQHkjyeZCnJXRe5/gNJziZ5NMk3k7xx9qNKktZjavCT7ACOAbcC+4EjSfavWvYIMF9VfwB8FfiHWQ8qSVqfIY/wbwKWqupcVT0H3AccmlxQVQ9U1bPjw1PAntmOKUlaryHB3w2cnzi+MD53KXcA37jYFUmOJllMsri8vDx8SknSus30RdsktwPzwL0Xu76qjlfVfFXNz83NzfKuJUlT7Byw5glg78TxnvG5/yfJ24EPA2+tql/OZjxJ0qwMeYR/GtiX5NokVwCHgYXJBUluAD4NHKyqJ2c/piRpvaYGv6qeB+4ETgKPAfdX1Zkk9yQ5OF52L/Bq4CtJ/j3JwiVuTpK0RYY8pUNVnQBOrDp398Tlt894LknSjPlOW0lqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpoYFPwkB5I8nmQpyV0Xuf43knx5fP1DSa6Z9aCSpPWZGvwkO4BjwK3AfuBIkv2rlt0BPF1Vvwv8E/DxWQ8qSVqfIY/wbwKWqupcVT0H3AccWrXmEPBv48tfBd6WJLMbU5K0XjsHrNkNnJ84vgD80aXWVNXzSZ4Bfhv42eSiJEeBo+PDXyb5/lqG3oZ2sWqvGnMvVrgXK9yLFb+31i8cEvyZqarjwHGAJItVNb+Z93+5ci9WuBcr3IsV7sWKJItr/dohT+k8AeydON4zPnfRNUl2Aq8FnlrrUJKk2RsS/NPAviTXJrkCOAwsrFqzAPzl+PJfAN+qqprdmJKk9Zr6lM74Ofk7gZPADuCzVXUmyT3AYlUtAP8KfCHJEvBzRj8Upjm+jrm3G/dihXuxwr1Y4V6sWPNexAfiktSD77SVpCYMviQ1seHB92MZVgzYiw8kOZvk0STfTPLGrZhzM0zbi4l170hSSbbtn+QN2Ysk7xx/b5xJ8sXNnnGzDPg/8oYkDyR5ZPz/5LatmHOjJflskicv9V6ljHxyvE+PJrlx0A1X1Yb9Y/Qi738AvwNcAXwP2L9qzV8BnxpfPgx8eSNn2qp/A/fiT4HfHF9+X+e9GK+7CngQOAXMb/XcW/h9sQ94BPit8fHrtnruLdyL48D7xpf3Az/a6rk3aC/+BLgR+P4lrr8N+AYQ4GbgoSG3u9GP8P1YhhVT96KqHqiqZ8eHpxi952E7GvJ9AfAxRp/L9IvNHG6TDdmL9wLHquppgKp6cpNn3CxD9qKA14wvvxb4ySbOt2mq6kFGf/F4KYeAz9fIKeDqJK+fdrsbHfyLfSzD7kutqarngRc+lmG7GbIXk+5g9BN8O5q6F+NfUfdW1dc3c7AtMOT74jrguiTfSXIqyYFNm25zDdmLjwK3J7kAnADevzmjXXZeak+ATf5oBQ2T5HZgHnjrVs+yFZK8AvgE8O4tHuVysZPR0zq3MPqt78Ekv19V/7WlU22NI8Dnquofk/wxo/f/vLmq/merB3s52OhH+H4sw4ohe0GStwMfBg5W1S83abbNNm0vrgLeDHw7yY8YPUe5sE1fuB3yfXEBWKiqX1XVD4EfMPoBsN0M2Ys7gPsBquq7wKsYfbBaN4N6stpGB9+PZVgxdS+S3AB8mlHst+vztDBlL6rqmaraVVXXVNU1jF7POFhVa/7QqMvYkP8jX2P06J4kuxg9xXNuM4fcJEP24sfA2wCSvIlR8Jc3dcrLwwLwrvFf69wMPFNVP532RRv6lE5t3McyvOwM3It7gVcDXxm/bv3jqjq4ZUNvkIF70cLAvTgJ/HmSs8B/Ax+qqm33W/DAvfgg8Jkkf8voBdx3b8cHiEm+xOiH/K7x6xUfAV4JUFWfYvT6xW3AEvAs8J5Bt7sN90qSdBG+01aSmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElq4n8BzPZcum6w2goAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MoUIxTZjaXut",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "414a3826-4f0f-4665-f523-d6b62a715206"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fail to import hypothesis in common_utils, tests are not derandomized\n"
          ]
        }
      ],
      "source": [
        "from math import inf\n",
        "import torch\n",
        "from torch import jit\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.distributions.normal import Normal\n",
        "from torch.distributions import transforms as tT\n",
        "from torch.distributions.transformed_distribution import TransformedDistribution\n",
        "from torch.testing._internal.common_utils import TestCase\n",
        "from torch.autograd import Variable\n",
        "from collections import Counter\n",
        "import collections\n",
        "from absl import logging\n",
        "import numpy as np\n",
        "import os\n",
        "import gin\n",
        "  \n",
        "local_device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "LOG_STD_MIN = torch.tensor(-5, dtype=torch.float64, device=local_device,requires_grad=False)\n",
        "\n",
        "LOG_STD_MAX = torch.tensor(2 , dtype=torch.float64, device=local_device,requires_grad=False)\n",
        "\n",
        "def get_spec_means_mags(spec):\n",
        "  means = (spec.maximum + spec.minimum) / 2.0\n",
        "  mags = (spec.maximum - spec.minimum) / 2.0\n",
        "  means = Variable(torch.tensor(means).type(torch.FloatTensor), requires_grad=False)\n",
        "  mags  = Variable(torch.tensor(mags).type(torch.FloatTensor), requires_grad=False)\n",
        "  return means, mags\n",
        "\n",
        "class Split(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    models a split in the network. works with convolutional models (not FC).\n",
        "    specify out channels for the model to divide by n_parts.\n",
        "    \"\"\"\n",
        "    def __init__(self, module, n_parts: int, dim=1):\n",
        "        super().__init__()\n",
        "        self._n_parts = n_parts\n",
        "        self._dim = dim\n",
        "        self._module = module\n",
        "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "        self.to(device=self.device)\n",
        "    def forward(self, inputs):\n",
        "        output = self._module(inputs)\n",
        "        if output.ndim==1:\n",
        "           result=torch.hsplit(output, self._n_parts )\n",
        "        else:\n",
        "           chunk_size = output.shape[self._dim] // self._n_parts\n",
        "           result =torch.split(output, chunk_size, dim=self._dim)\n",
        "\n",
        "        return result\n",
        "      \n",
        "###############################################\n",
        "##################  Networks  #################\n",
        "###############################################\n",
        "class ActorNetwork(nn.Module):\n",
        "  \"\"\"Actor network.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      latent_spec,\n",
        "      action_spec,\n",
        "      fc_layer_params=(),\n",
        "      ):\n",
        "    super(ActorNetwork, self).__init__()\n",
        "    self._action_spec = action_spec\n",
        "    self._layers = nn.ModuleList()\n",
        "    for hidden_size in fc_layer_params:\n",
        "        if len(self._layers)==0:\n",
        "           self._layers.append(nn.Linear(latent_spec.shape[0], hidden_size))\n",
        "        else:\n",
        "           self._layers.append(nn.Linear(hidden_size, hidden_size))\n",
        "        self._layers.append(nn.ReLU())\n",
        "    output_layer = nn.Linear(hidden_size,self._action_spec.shape[0] * 2)\n",
        "    #output_layer = nn.LazyLinear(self._action_spec.shape[0] * 2)\n",
        "    self._layers.append(output_layer)\n",
        "    \n",
        "    self._action_means, self._action_mags = get_spec_means_mags(\n",
        "        self._action_spec)\n",
        "    self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    self.to(device=self.device)\n",
        "\n",
        "  def to(self, *args, **kwargs):\n",
        "      super().to(*args, **kwargs)\n",
        "\n",
        "  @property\n",
        "  def action_spec(self):\n",
        "      return self._action_spec\n",
        "\n",
        "  def _get_outputs(self, state):\n",
        "      h = state\n",
        "      \n",
        "      for l in nn.Sequential(*(list(self._layers.children())[:-1])):\n",
        "          h = l(h)\n",
        "\n",
        "      self._mean_logvar_layers = Split(\n",
        "         self._layers[-1],\n",
        "         n_parts=2,\n",
        "      )\n",
        "      mean, log_std = self._mean_logvar_layers(h)\n",
        "      \n",
        "      a_tanh_mode = torch.tanh(mean) * self._action_mags + self._action_means\n",
        "      log_std = torch.tanh(log_std)\n",
        "      log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)\n",
        "      std = torch.exp(log_std)\n",
        "      #base_distribution = torch.normal(0.0, 1.0)\n",
        "      #transforms = torch.distributions.transforms.ComposeTransform([torch.distributions.transforms.AffineTransform(loc=self._action_means, scale=self._action_mag, event_dim=mean.shape[-1]), torch.nn.Tanh(),torch.distributions.transforms.AffineTransform(loc=mean, scale=std, event_dim=mean.shape[-1])])\n",
        "      #a_distribution = torch.distributions.transformed_distribution.TransformedDistribution(base_distribution, transforms)\n",
        "      a_distribution = TransformedDistribution(\n",
        "                        base_distribution=Normal(loc=torch.full_like(mean, 0).to(device=self.device), \n",
        "                                                 scale=torch.full_like(mean, 1).to(device=self.device)), \n",
        "                        transforms=tT.ComposeTransform([\n",
        "                                   tT.AffineTransform(loc=self._action_means.to(device=self.device), scale=self._action_mags.to(device=self.device), event_dim=mean.shape[-1]), \n",
        "                                   tT.TanhTransform(cache_size=1),\n",
        "                                   tT.AffineTransform(loc=mean, scale=std, event_dim=mean.shape[-1])]))\n",
        "      #https://www.ccoderun.ca/programming/doxygen/pytorch/classtorch_1_1distributions_1_1transformed__distribution_1_1TransformedDistribution.html\n",
        "      return a_distribution, a_tanh_mode\n",
        "\n",
        "  def get_log_density(self, state, action):\n",
        "    a_dist, _ = self._get_outputs(state.to(device=self.device))\n",
        "    log_density = a_dist.log_prob(action.to(device=self.device))\n",
        "    return log_density\n",
        "\n",
        "  @property\n",
        "  def weights(self):\n",
        "    w_list = []\n",
        "    for l in self._layers:\n",
        "      w_list.append(l.weight[0])\n",
        "    return w_list\n",
        "\n",
        "  def __call__(self, state):\n",
        "    a_dist, a_tanh_mode = self._get_outputs(state.to(device=self.device))\n",
        "    a_sample = a_dist.sample()\n",
        "    log_pi_a = a_dist.log_prob(a_sample)\n",
        "    return a_tanh_mode, a_sample, log_pi_a\n",
        "\n",
        "  def sample_n(self, state, n=1):\n",
        "    a_dist, a_tanh_mode = self._get_outputs(state)\n",
        "    a_sample = a_dist.sample([n])\n",
        "    log_pi_a = a_dist.log_prob(a_sample)\n",
        "    return a_tanh_mode, a_sample, log_pi_a\n",
        "\n",
        "  def sample(self, state):\n",
        "    return self.sample_n(state, n=1)[1][0]\n",
        "\n",
        "class CriticNetwork(nn.Module):\n",
        "    \"\"\"Critic Network.\"\"\"\n",
        "    def __init__(\n",
        "      self,\n",
        "      latent_spec,\n",
        "      action_spec,\n",
        "      fc_layer_params=(),\n",
        "      ):\n",
        "      super(CriticNetwork, self).__init__()\n",
        "      self._action_spec = action_spec\n",
        "      self._layers = nn.ModuleList()\n",
        "      for hidden_size in fc_layer_params:\n",
        "          if len(self._layers)==0:\n",
        "              \n",
        "              self._layers.append(nn.Linear(latent_spec.shape[0]+action_spec.shape[0], hidden_size))\n",
        "          else:\n",
        "              self._layers.append(nn.Linear(hidden_size, hidden_size))\n",
        "          self._layers.append(nn.ReLU())\n",
        "      output_layer = nn.Linear(hidden_size,1)\n",
        "      self._layers.append(output_layer)\n",
        "      self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "      self.to(device=self.device)\n",
        "\n",
        "    def forward(self, state,action):\n",
        "        hidden = torch.cat([state.to(self.device),action.to(self.device)],dim=-1)\n",
        "        for l in self._layers:\n",
        "            hidden = l(hidden)\n",
        "        return hidden\n",
        "\n",
        "class ValueNetwork(nn.Module):\n",
        "    \"\"\"Value Network.\"\"\"\n",
        "    def __init__(\n",
        "      self,\n",
        "      latent_spec,\n",
        "      fc_layer_params=(),\n",
        "      ):\n",
        "      super(ValueNetwork, self).__init__()\n",
        "      self._latent_spec = latent_spec\n",
        "      self._layers = nn.ModuleList()\n",
        "      for hidden_size in fc_layer_params:\n",
        "          if len(self._layers)==0:\n",
        "              \n",
        "              self._layers.append(nn.Linear(latent_spec.shape[0], hidden_size))\n",
        "          else:\n",
        "              self._layers.append(nn.Linear(hidden_size, hidden_size))\n",
        "          self._layers.append(nn.ReLU())\n",
        "      output_layer = nn.Linear(hidden_size,1)\n",
        "      self._layers.append(output_layer)\n",
        "      self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "      self.to(device=self.device)\n",
        "\n",
        "    def forward(self, state):\n",
        "        hidden = state.to(device=self.device)\n",
        "        for l in self._layers:\n",
        "            hidden = l(hidden)\n",
        "        return hidden\n",
        "\n",
        "class Flags(object):\n",
        "\n",
        "  def __init__(self, **kwargs):\n",
        "    for key, val in kwargs.items():\n",
        "      if kwargs.get(key) is not None:\n",
        "         setattr(self, key, val)\n",
        "      else:\n",
        "         pass\n",
        "\n",
        "def get_modules(model_params, observation_spec, action_spec):\n",
        "  \"\"\"Gets pytorch modules for Q-function, policy, and discriminator.\"\"\"\n",
        "  model_params, n_q_fns = model_params\n",
        "  \n",
        "  if len(model_params) == 1:\n",
        "    model_params = tuple([model_params[0]] * 4)\n",
        "  elif len(model_params) < 4:\n",
        "    raise ValueError('Bad model parameters %s.' % model_params)\n",
        "  def q_net_factory():\n",
        "    return CriticNetwork(\n",
        "        observation_spec, \n",
        "        action_spec,\n",
        "        fc_layer_params=model_params[0])\n",
        "  def p_net_factory():\n",
        "    return ActorNetwork(\n",
        "        observation_spec,\n",
        "        action_spec,\n",
        "        fc_layer_params=model_params[1])\n",
        "  def c_net_factory():\n",
        "    return CriticNetwork(\n",
        "        observation_spec, \n",
        "        action_spec,\n",
        "        fc_layer_params=model_params[2])\n",
        "  def v_net_factory():\n",
        "    return ValueNetwork(\n",
        "        observation_spec, \n",
        "        fc_layer_params=model_params[3])\n",
        "  modules_list = utils.Flags(\n",
        "      q_net_factory=q_net_factory,\n",
        "      p_net_factory=p_net_factory,\n",
        "      c_net_factory=c_net_factory,\n",
        "      v_net_factory=v_net_factory,\n",
        "      n_q_fns=n_q_fns,\n",
        "      )\n",
        "  return modules_list\n",
        "#######################################\n",
        "################ AGENT ################\n",
        "#######################################\n",
        "ALPHA_MAX = 500.0\n",
        "class GeneralAgent(nn.Module):\n",
        "  \"\"\"Tensorflow module for agent.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      modules_list=None,\n",
        "      ):\n",
        "    super(GeneralAgent, self).__init__()\n",
        "    self._modules_list = modules_list\n",
        "    self._build_modules()\n",
        "    self.device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    self.to(self.device)\n",
        "\n",
        "  def _build_modules(self):\n",
        "    pass\n",
        "\n",
        "\n",
        "\n",
        "class AgentModule(GeneralAgent):\n",
        "  \"\"\"Pytorch module for BRAC dual agent.\"\"\"\n",
        "  def __init__(self,modules_list):\n",
        "    # invoking the __init__ of the parent class\n",
        "    super().__init__(modules_list)\n",
        "\n",
        "  def _build_modules(self):\n",
        "    self._q_nets = list()  \n",
        "    n_q_fns = self._modules_list.n_q_fns\n",
        "    for _ in range(n_q_fns):\n",
        "      self._q_nets.append(\n",
        "          [self._modules_list.q_net_factory(),  # Learned Q-value.\n",
        "           self._modules_list.q_net_factory(),]  # Target Q-value.\n",
        "          )\n",
        "    self._p_net = self._modules_list.p_net_factory()\n",
        "    self._c_net = self._modules_list.c_net_factory()\n",
        "    self._v_net = self._modules_list.v_net_factory() #value network\n",
        "    self._alpha_var = torch.tensor(1.0, requires_grad=True)\n",
        "    self._alpha_entropy_var = torch.tensor(1.0, requires_grad=True)\n",
        "\n",
        "  def get_alpha(self, alpha_max=ALPHA_MAX):\n",
        "    return utils.clip_v2(\n",
        "        self._alpha_var, 0.0, alpha_max)\n",
        "\n",
        "  def get_alpha_entropy(self):\n",
        "    return utils.relu_v2(self._alpha_entropy_var)\n",
        "\n",
        "  def assign_alpha(self, alpha):\n",
        "    self._alpha_var=torch.tensor(alpha, requires_grad=True)\n",
        "\n",
        "  def assign_alpha_entropy(self, alpha):\n",
        "    self._alpha_entropy_var=torch.tensor(alpha, requires_grad=True)\n",
        "\n",
        "  @property\n",
        "  def a_variables(self):\n",
        "    return [self._alpha_var]\n",
        "\n",
        "  @property\n",
        "  def ae_variables(self):\n",
        "    return [self._alpha_entropy_var]\n",
        "\n",
        "  @property\n",
        "  def q_nets(self):\n",
        "    return self._q_nets\n",
        "\n",
        "  @property\n",
        "  def q_source_weights(self):\n",
        "    q_weights = []\n",
        "    for q_net, _ in self._q_nets:\n",
        "      for name, param in q_net._layers.named_parameters():\n",
        "          if 'weight' in name:\n",
        "             q_weights += list(param)\n",
        "    return q_weights\n",
        "\n",
        "  @property\n",
        "  def q_target_weights(self):\n",
        "    q_weights = []\n",
        "    for _, q_net in self._q_nets:\n",
        "      for name, param in q_net._layers.named_parameters():\n",
        "          if 'weight' in name:\n",
        "             q_weights += list(param)\n",
        "    return q_weights\n",
        "\n",
        "  @property\n",
        "  def q_source_variables(self):\n",
        "    vars_=[]\n",
        "    for q_net, _ in self._q_nets:\n",
        "        for param in q_net._layers:\n",
        "            if not isinstance(param, nn.ReLU):\n",
        "               vars_+=list(param.parameters())\n",
        "    return vars_\n",
        " \n",
        "  @property\n",
        "  def q_target_variables(self):\n",
        "    vars_ = []\n",
        "    for _, q_net in self._q_nets:\n",
        "        for param in q_net._layers:\n",
        "            if not isinstance(param, nn.ReLU):\n",
        "               vars_+=list(param.parameters())\n",
        "    return vars_\n",
        "\n",
        "  @property\n",
        "  def p_net(self):\n",
        "    return self._p_net\n",
        "\n",
        "  def p_fn(self, s):\n",
        "    return self._p_net(s)\n",
        "\n",
        "  @property\n",
        "  def v_net(self):\n",
        "    return self._v_net\n",
        "\n",
        "  @property\n",
        "  def v_variables(self):\n",
        "    vars_ = []\n",
        "    for param in self._v_net._layers:\n",
        "        if not isinstance(param, nn.ReLU):\n",
        "               vars_+=list(param.parameters())\n",
        "    return vars_\n",
        "\n",
        "  @property\n",
        "  def v_weights(self):\n",
        "    v_weights = []\n",
        "    for name, param in self._v_net._layers.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            v_weights += list(param)\n",
        "    return v_weights\n",
        "\n",
        "  @property\n",
        "  def p_weights(self):\n",
        "    p_weights = []\n",
        "    for name, param in self._p_net._layers.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            p_weights += list(param)\n",
        "    return p_weights\n",
        "\n",
        "\n",
        "  @property\n",
        "  def p_variables(self):\n",
        "    vars_=[]\n",
        "    for param in self._p_net._layers:\n",
        "        if not isinstance(param, nn.ReLU):\n",
        "           vars_+=list(param.parameters())\n",
        "    return vars_\n",
        "\n",
        "  @property\n",
        "  def c_net(self):\n",
        "    return self._c_net\n",
        "\n",
        "  @property\n",
        "  def c_weights(self):\n",
        "    c_weights = []\n",
        "    for name, param in self._c_net._layers.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            c_weights += list(param)\n",
        "    return c_weights\n",
        "\n",
        "  @property\n",
        "  def c_variables(self):\n",
        "    vars_=[]\n",
        "    for param in self._c_net._layers:\n",
        "        if not isinstance(param, nn.ReLU):\n",
        "           vars_+=list(param.parameters())\n",
        "    return vars_\n",
        "\n",
        "class Agent(object):\n",
        "  \"\"\"Class for learning policy and interacting with environment.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      observation_spec=None,\n",
        "      action_spec=None,\n",
        "      time_step_spec=None,\n",
        "      modules=None,\n",
        "      optimizers= ((0.001, 0.5, 0.99),),\n",
        "      batch_size=64,\n",
        "      weight_decays=(0.5,),\n",
        "      update_freq=1,\n",
        "      update_rate=0.005,\n",
        "      discount=0.99,\n",
        "      env_name='HalfCheetah-v2',\n",
        "      train_data=None,\n",
        "      resume=False, \n",
        "      device=None\n",
        "      ):\n",
        "    self._latent_spec = observation_spec\n",
        "    self._action_spec = action_spec\n",
        "    self._time_step_spec = time_step_spec\n",
        "    self._modules_list = modules\n",
        "    self._optimizers = optimizers\n",
        "    self._batch_size = batch_size\n",
        "    self._weight_decays = weight_decays\n",
        "    self._train_data = train_data\n",
        "    self._update_freq = update_freq\n",
        "    self._update_rate = update_rate\n",
        "    self._discount = discount\n",
        "    self._env_name = env_name\n",
        "    self._resume = resume \n",
        "    if device is None:\n",
        "       self.device = torch.device(\n",
        "                    'cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    else:\n",
        "       self.device=device\n",
        "    \n",
        "    directory = os.getcwd()\n",
        "    checkpoint_dir=directory+\"/run\"\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "      os.makedirs(checkpoint_dir)\n",
        "    self.checkpoint_path = os.path.join(checkpoint_dir, \"checkpoint.pth\")\n",
        "    self._build_agent()\n",
        "\n",
        "  def _build_agent(self):\n",
        "    \"\"\"Builds agent components.\"\"\"\n",
        "    if len(self._weight_decays) == 1:\n",
        "       self._weight_decays = tuple([self._weight_decays[0]] * 4)\n",
        "    self._build_fns()\n",
        "    train_batch = self._get_train_batch()\n",
        "    self._global_step = torch.tensor(0.0, requires_grad=False)\n",
        "    self._init_vars(train_batch)\n",
        "    self._build_optimizers()\n",
        "    self._train_info = collections.OrderedDict()\n",
        "    self._checkpointer = self._build_checkpointer()\n",
        "    self._test_policies = collections.OrderedDict()\n",
        "    self._build_test_policies()\n",
        "    self._online_policy = self._build_online_policy()\n",
        "    \n",
        "\n",
        "  def _build_fns(self):\n",
        "    self._agent_module = AgentModule(modules_list=self._modules_list)\n",
        "\n",
        "  def _get_vars(self):\n",
        "    return []\n",
        "\n",
        "  def _build_optimizers(self):\n",
        "     opt = self._optimizers[0]\n",
        "     self._optimizer = torch.optim.Adam(\n",
        "            self._agent_module.parameters(),\n",
        "            lr= opt[0],\n",
        "            betas=(opt[1], opt[2]),\n",
        "        )\n",
        "    \n",
        "\n",
        "  def _build_loss(self, batch):\n",
        "    raise NotImplementedError\n",
        "\n",
        "  def _update_target_vars(self):\n",
        "      #?\n",
        "      # requires self._vars_learning and self._vars_target as state_dict`s\n",
        "      for var_name, var_t in self._vars_target.items():\n",
        "          updated_val = (self._update_rate\n",
        "                    * self._vars_learning[var_name].data\n",
        "                    + (1.0 - self._update_rate) * var_t.data)\n",
        "          var_t.data.copy_(updated_val)\n",
        "\n",
        "  def _build_test_policies(self):\n",
        "      raise NotImplementedError\n",
        "\n",
        "  def _build_online_policy(self):\n",
        "      return None\n",
        "  \n",
        "  #def _random_policy_fn(self, state):\n",
        "  #    return self._action_spec.sample(), None\n",
        "\n",
        "\n",
        "  @property\n",
        "  def test_policies(self):\n",
        "    return self._test_policies\n",
        "\n",
        "  @property\n",
        "  def online_policy(self):\n",
        "    return self._online_policy\n",
        "\n",
        "  def _get_train_batch(self):\n",
        "    \"\"\"Samples and constructs batch of transitions.\"\"\"\n",
        "    batch_indices = np.random.choice(self._train_data.size, self._batch_size)\n",
        "    batch_ = self._train_data.get_batch(batch_indices)\n",
        "    transition_batch = batch_\n",
        "    batch = dict(\n",
        "        s1=transition_batch.s1,\n",
        "        s2=transition_batch.s2,\n",
        "        r=transition_batch.reward,\n",
        "        dsc=transition_batch.discount,\n",
        "        a1=transition_batch.a1,\n",
        "        a2=transition_batch.a2,\n",
        "        )\n",
        "    return batch\n",
        "\n",
        "            \n",
        "  def _train_step(self):\n",
        "      train_batch = self._get_train_batch()\n",
        "      loss = self._build_loss(train_batch)\n",
        "      self._optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      self._optimizer.step()\n",
        "      self._global_step += 1\n",
        "      if self._global_step % self._update_freq == 0:\n",
        "          self._update_target_vars()\n",
        "    \n",
        "\n",
        "  def _init_vars(self, batch):\n",
        "      pass\n",
        "\n",
        "  def _get_source_target_vars(self):\n",
        "      return [], []\n",
        "\n",
        "  def _update_target_fns(self, source_vars, target_vars):\n",
        "    utils.soft_variables_update(\n",
        "        source_vars,\n",
        "        target_vars,\n",
        "        tau=self._update_rate)\n",
        "\n",
        "  def print_train_info(self):\n",
        "      summary_str = utils.get_summary_str(\n",
        "                step=self._global_step, info=self._train_info)\n",
        "      logging.info(summary_str)\n",
        "    \n",
        "\n",
        "  def write_train_summary(self, summary_writer):\n",
        "    info = self._train_info\n",
        "    step = self._global_step.numpy()\n",
        "    utils.write_summary(summary_writer, info,step)\n",
        "\n",
        "  def _build_checkpointer(self):\n",
        "      #?save\n",
        "      pass\n",
        "\n",
        "  def _load_checkpoint(self):\n",
        "      #?restore\n",
        "      pass\n",
        "\n",
        "  @property\n",
        "  def global_step(self):\n",
        "    return self._global_step.numpy()\n",
        "  \n",
        "################# Policies #################\n",
        "class GaussianRandomSoftPolicy(nn.Module):\n",
        "  \"\"\"Adds Gaussian noise to actor's action.\"\"\"\n",
        "\n",
        "  def __init__(self, a_network, std=0.1, clip_eps=1e-3):\n",
        "    super(GaussianRandomSoftPolicy, self).__init__()\n",
        "    self._a_network = a_network\n",
        "    self._std = std\n",
        "    self._clip_eps = clip_eps\n",
        "    self.device = torch.device(\n",
        "                    'cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    self.to(device=self.device)\n",
        "  def __call__(self, observation, state=()):\n",
        "    action = self._a_network(observation.to(device=self.device))[1]\n",
        "    noise = torch.normal(mean=torch.zeros(action.shape), std=self._std).to(device=self.device)\n",
        "    action = action + noise\n",
        "    spec = self._a_network.action_spec\n",
        "    action = torch.clamp(action, spec.minimum + self._clip_eps,\n",
        "                              spec.maximum - self._clip_eps)\n",
        "    return action, state\n",
        "  \n",
        "class DeterministicSoftPolicy(nn.Module):\n",
        "  \"\"\"Returns mode of policy distribution.\"\"\"\n",
        "\n",
        "  def __init__(self, a_network):\n",
        "    super(DeterministicSoftPolicy, self).__init__()\n",
        "    self._a_network = a_network\n",
        "    self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    self.to(device=self.device)\n",
        "\n",
        "\n",
        "  def __call__(self, latent_states):\n",
        "    action = self._a_network(latent_states)[0]\n",
        "    return action\n",
        "\n",
        "class RandomSoftPolicy(nn.Module):\n",
        "  \"\"\"Returns sample from policy distribution.\"\"\"\n",
        "\n",
        "  def __init__(self, a_network):\n",
        "    super(RandomSoftPolicy, self).__init__()\n",
        "    self._a_network = a_network\n",
        "    self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    self.to(device=self.device)\n",
        "\n",
        "\n",
        "  def __call__(self, latent_states):\n",
        "    action = self._a_network(latent_states)[1]\n",
        "    return action\n",
        "\n",
        "class MaxQSoftPolicy(nn.Module):\n",
        "  \"\"\"Samples a few actions from policy, returns the one with highest Q-value.\"\"\"\n",
        "\n",
        "  def __init__(self, a_network, q_network, n=10):\n",
        "    super(MaxQSoftPolicy, self).__init__()\n",
        "    self._a_network = a_network\n",
        "    self._q_network = q_network\n",
        "    self._n = n\n",
        "    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    self.to(self.device)\n",
        "  def __call__(self, latent_state):\n",
        "    \n",
        "    \n",
        "    actions = self._a_network.sample_n(latent_state.to(device=self.device), self._n)[1]\n",
        "    batch_size = actions.shape[-1]\n",
        "    actions_ = torch.reshape(actions, [self._n * batch_size, -1])\n",
        "    states_ = torch.tile(latent_state[None].to(self.device), (self._n, 1, 1))\n",
        "    states_ = torch.reshape(states_, [self._n * batch_size, -1])\n",
        "    qvals = self._q_network(states_, actions_)\n",
        "    qvals = torch.reshape(qvals, [self._n, batch_size]).to(device=self.device)\n",
        "    a_indices = torch.argmax(qvals, dim=0).to(device=self.device)\n",
        "    gather_indices = torch.stack(\n",
        "        [a_indices, torch.arange(batch_size, dtype=torch.int64).to(device=self.device)], dim=-1)\n",
        "    action = utils.gather_nd(actions, gather_indices)\n",
        "    return action\n",
        "######################################################\n",
        "###################### D2E Agent ##################### \n",
        "######################################################\n",
        "\n",
        "\n",
        "\n",
        "######################Define Agent#################### \n",
        "gin.clear_config()\n",
        "gin.config._REGISTRY.clear()\n",
        "@gin.configurable\n",
        "class D2EAgent(Agent):\n",
        "  \"\"\"dual agent class.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      alpha=1.0,\n",
        "      alpha_max=ALPHA_MAX,\n",
        "      train_alpha=False,\n",
        "      value_penalty=True,\n",
        "      target_divergence=0.0,\n",
        "      alpha_entropy=0.0,\n",
        "      train_alpha_entropy=False,\n",
        "      target_entropy=None,\n",
        "      EIM_config = ConditionalMixtureEIM.get_default_config(),\n",
        "      divergence_name='kl',\n",
        "      warm_start=2000,\n",
        "      c_iter=3,\n",
        "      ensemble_q_lambda=1.0,\n",
        "      **kwargs):\n",
        "    self._alpha = alpha\n",
        "    self._alpha_max = alpha_max\n",
        "    self._train_alpha = train_alpha\n",
        "    self._value_penalty = value_penalty\n",
        "    self._target_divergence = target_divergence\n",
        "    self._divergence_name = divergence_name\n",
        "    self._train_alpha_entropy = train_alpha_entropy\n",
        "    self._alpha_entropy = alpha_entropy\n",
        "    self._target_entropy = target_entropy\n",
        "    self._EIM_config = EIM_config\n",
        "    self._warm_start = warm_start\n",
        "    self._c_iter = c_iter\n",
        "    self._ensemble_q_lambda = ensemble_q_lambda\n",
        "    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    super(D2EAgent, self).__init__(**kwargs)\n",
        "  def _build_fns(self):\n",
        "    self._agent_module = AgentModule(modules_list=self._modules_list)\n",
        "    self._q_fns = self._agent_module.q_nets\n",
        "    self._p_fn = self._agent_module.p_fn\n",
        "    self._c_fn = self._agent_module.c_net\n",
        "    self._v_fn = self._agent_module.v_net #adding value network\n",
        "    self._divergence = utils.get_divergence(\n",
        "        name=self._divergence_name, \n",
        "        c=self._c_fn,\n",
        "        device=self.device)\n",
        "    #################################################################################################################################\n",
        "    ############ Setting Up Configuration parameters of the EIM model to compute desity ratio of the transition function ############\n",
        "    #################################################################################################################################\n",
        "    self._EIM_config.train_epochs = 20\n",
        "    print(self._latent_spec.shape)\n",
        "    self._EIM_config.num_components = self._latent_spec.shape[0]//2 #number of dimension of latent space\n",
        "\n",
        "    self._EIM_config.components_net_hidden_layers = [30, 30]\n",
        "    self._EIM_config.components_batch_size = self._latent_spec.shape[0] #is it a correct batch size???\n",
        "    self._EIM_config.components_num_epochs = 10\n",
        "    self._EIM_config.components_net_reg_loss_fact = 0.0\n",
        "    self._EIM_config.components_net_drop_prob = 0.0\n",
        "\n",
        "    self._EIM_config.gating_net_hidden_layers = [30, 30]\n",
        "    self._EIM_config.gating_batch_size = self._latent_spec.shape[0] #???\n",
        "    self._EIM_config.gating_num_epochs = 10\n",
        "    self._EIM_config.gating_net_reg_loss_fact = 0.0\n",
        "    self._EIM_config.gating_net_drop_prob = 0.0\n",
        "\n",
        "    self._EIM_config.dre_reg_loss_fact = 0.0005\n",
        "    self._EIM_config.dre_early_stopping = True\n",
        "    self._EIM_config.dre_drop_prob = 0.0\n",
        "    self._EIM_config.dre_num_iters =  10\n",
        "    self._EIM_config.dre_batch_size = self._latent_spec.shape[0] ###???\n",
        "    self._EIM_config.dre_hidden_layers = [30, 30]\n",
        "    ########################################################################################################################\n",
        "    ##########################################   END of EIM cofiguration SETUP    ##########################################\n",
        "    ########################################################################################################################\n",
        "    self._agent_module.assign_alpha(self._alpha)\n",
        "    if self._target_entropy is None:\n",
        "      self._target_entropy = - self._action_spec.shape[0]\n",
        "    self._get_alpha_entropy = self._agent_module.get_alpha_entropy\n",
        "    self._agent_module.assign_alpha_entropy(self._alpha_entropy)\n",
        "\n",
        "  def _get_alpha(self):\n",
        "    return self._agent_module.get_alpha(\n",
        "        alpha_max=self._alpha_max)\n",
        "\n",
        "  def _get_q_vars(self):\n",
        "    return self._agent_module.q_source_variables\n",
        "\n",
        "  def _get_p_vars(self):\n",
        "    return self._agent_module.p_variables\n",
        "\n",
        "  def _get_c_vars(self):\n",
        "    return self._agent_module.c_variables\n",
        "\n",
        "  def _get_v_vars(self):\n",
        "    return self._agent_module.v_variables\n",
        "  \n",
        "  def _get_q_weight_norm(self):\n",
        "    weights = self._agent_module.q_source_weights\n",
        "    norms = []\n",
        "    for w in weights:\n",
        "      norm = torch.sum(torch.square(w))\n",
        "      norms.append(norm)\n",
        "    return torch.stack(norms).sum(dim=0)\n",
        "\n",
        "  def _get_p_weight_norm(self):\n",
        "    weights = self._agent_module.p_weights\n",
        "    norms = []\n",
        "    for w in weights:\n",
        "      norm = torch.sum(torch.square(w))\n",
        "      norms.append(norm)\n",
        "    return torch.stack(norms).sum(dim=0)\n",
        "\n",
        "  def _get_c_weight_norm(self):\n",
        "    weights = self._agent_module.c_weights\n",
        "    norms = []\n",
        "    for w in weights:\n",
        "      norm = torch.sum(torch.square(w))\n",
        "      norms.append(norm)\n",
        "    return torch.stack(norms).sum(dim=0)\n",
        "\n",
        "  def _get_v_weight_norm(self):\n",
        "    weights = self._agent_module.v_weights\n",
        "    norms = []\n",
        "    for w in weights:\n",
        "      norm = torch.sum(torch.square(w))\n",
        "      norms.append(norm)\n",
        "    return torch.stack(norms).sum(dim=0)\n",
        "\n",
        "\n",
        "  def ensemble_q(self, qs):\n",
        "    lambda_ = self._ensemble_q_lambda\n",
        "    return (lambda_ *torch.min(qs, dim=-1).values\n",
        "            + (1 - lambda_) * torch.max(qs, dim=-1).values)\n",
        "\n",
        "  def _ensemble_q2_target(self, q2_targets):\n",
        "    return self.ensemble_q(q2_targets)\n",
        "\n",
        "  def _ensemble_q1(self, q1s):\n",
        "    return self.ensemble_q(q1s)\n",
        "\n",
        "  def _build_q_loss(self, batch):\n",
        "    s1 = batch['s1']\n",
        "    s2 = batch['s2']\n",
        "    a1 = batch['a1']\n",
        "    a2_b = batch['a2']\n",
        "    r = batch['r']\n",
        "    dsc = batch['dsc']\n",
        "    _, a2_p, log_pi_a2_p = self._p_fn(s2.to(device=self.device))\n",
        "    q2_targets = []\n",
        "    q1_preds = []\n",
        "    for q_fn, q_fn_target in self._q_fns:\n",
        "      q2_target_ = q_fn_target(s2.to(device=self.device), a2_p)\n",
        "      q1_pred = q_fn(s1.to(device=self.device), a1.to(device=self.device))\n",
        "      q1_preds.append(q1_pred)\n",
        "      q2_targets.append(q2_target_)\n",
        "    q2_targets = torch.stack(q2_targets, dim=-1)\n",
        "    q2_target = self._ensemble_q2_target(q2_targets)\n",
        "    div_estimate = self._divergence.dual_estimate(\n",
        "        s2.to(device=self.device), a2_p, a2_b.to(device=self.device))\n",
        "    \n",
        "    v2_target = q2_target - self._v_fn(s2.to(device=self.device))\n",
        "    if self._value_penalty:\n",
        "       v2_target = v2_target - self._get_alpha()[0] * div_estimate\n",
        "    with torch.no_grad():\n",
        "         q1_target = r.to(device=self.device) + dsc.to(device=self.device) * self._discount * v2_target\n",
        "    q_losses = []\n",
        "    for q1_pred in q1_preds:\n",
        "      q_loss_ = torch.mean(torch.square(q1_pred - q1_target))\n",
        "      q_losses.append(q_loss_)\n",
        "    q_loss = torch.sum(torch.FloatTensor(q_losses))\n",
        "    q_w_norm = self._get_q_weight_norm()\n",
        "    norm_loss = self._weight_decays[0] * q_w_norm\n",
        "    loss = q_loss + norm_loss\n",
        "\n",
        "    info = collections.OrderedDict()\n",
        "    info['q_loss'] = q_loss\n",
        "    info['q_norm'] = q_w_norm\n",
        "    info['r_mean'] = torch.mean(r)\n",
        "    info['dsc_mean'] = torch.mean(dsc)\n",
        "    info['q2_target_mean'] = torch.mean(q2_target)\n",
        "    info['q1_target_mean'] = torch.mean(q1_target)\n",
        "\n",
        "    return loss, info\n",
        "\n",
        "  def _build_p_loss(self, batch):\n",
        "    s = batch['s1']\n",
        "    a_b = batch['a1']\n",
        "    _, a_p, log_pi_a_p = self._p_fn(s)\n",
        "    v1  = self._v_fn(s.to(device=self.device))\n",
        "    q1s = []\n",
        "    for q_fn, _ in self._q_fns:\n",
        "      q1_ = q_fn(s, a_p)\n",
        "      q1s.append(q1_)\n",
        "    q1s = torch.stack(q1s, dim=-1)\n",
        "    q1 = self._ensemble_q1(q1s)\n",
        "    div_estimate = self._divergence.dual_estimate(\n",
        "        s, a_p, a_b)\n",
        "    q_start = torch.gt(self._global_step, self._warm_start).type(torch.float32)\n",
        "    p_loss = torch.mean(\n",
        "        self._get_alpha_entropy()[0] * log_pi_a_p\n",
        "        + self._get_alpha()[0] * div_estimate + v1\n",
        "        - q1 * q_start)\n",
        "    p_w_norm = self._get_p_weight_norm()\n",
        "    norm_loss = self._weight_decays[1] * p_w_norm\n",
        "    loss = p_loss + norm_loss\n",
        "\n",
        "    info = collections.OrderedDict()\n",
        "    info['p_loss'] = p_loss\n",
        "    info['p_norm'] = p_w_norm\n",
        "\n",
        "    return loss, info\n",
        "  def _recorder_EIM(self,batch):\n",
        "      s1  = batch['s1']\n",
        "      s2  = batch['s2']\n",
        "      \"\"\"configure experiment\"\"\"\n",
        "      plot_realtime = True\n",
        "      plot_save = False\n",
        "      data=MujocoData(s1,s2)\n",
        "      \"\"\"Recording\"\"\"\n",
        "      recorder_dict = {\n",
        "         RecorderKeys.TRAIN_ITER: TrainIterationRecMod(),\n",
        "         RecorderKeys.INITIAL: ConfigInitialRecMod(),\n",
        "         RecorderKeys.MODEL: MujocoModelRecMod(data,\n",
        "                                               self._env_name,\n",
        "                                               train_samples=data.train_samples,\n",
        "                                               test_samples=data.test_samples),\n",
        "         RecorderKeys.DRE: DRERecMod(torch.from_numpy(np.asarray(data.train_samples))),\n",
        "         RecorderKeys.COMPONENT_UPDATE: ComponentUpdateRecMod(plot=True, summarize=False)}\n",
        "      if self._EIM_config.num_components > 1:\n",
        "         recorder_dict[RecorderKeys.WEIGHTS_UPDATE] = WeightUpdateRecMod(plot=True)\n",
        "\n",
        "\n",
        "      return Recorder(recorder_dict, plot_realtime=plot_realtime, save=plot_save, save_path=\"rec\")\n",
        "\n",
        "\n",
        "  def _build_v_loss(self, batch):\n",
        "    s1  = batch['s1']\n",
        "    a_b = batch['a1']\n",
        "    s2  = batch['s2']\n",
        "    _, a_p, _ = self._p_fn(s1.to(device=self.device))\n",
        "    v_target = self._v_fn(s2.to(device=self.device))\n",
        "    q1_target = []\n",
        "    for q_fn, q_fn_target in self._q_fns:\n",
        "      q1_ = q_fn_target(s1.to(device=self.device), a_b.to(device=self.device))\n",
        "      q1_target.append(q1_)\n",
        "    q1_target = torch.stack(q1_target, dim=-1)\n",
        "    q1_target_ensemble = self.ensemble_q(q1_target)\n",
        "    div_estimate = self._divergence.dual_estimate(\n",
        "        s1.to(device=self.device), a_p, a_b.to(device=self.device))\n",
        "    #########################\n",
        "    #input_data next_state (s2) and state (s1) \n",
        "    self._EIM_model = ConditionalMixtureEIM(self._EIM_config, train_samples=(s1, s2), seed=torch.initial_seed())\n",
        "    EIM_gate_res=0\n",
        "    if self._EIM_model._model.num_components > 1:\n",
        "       EIM_gate_res =self._EIM_model.update_gating()\n",
        "    \n",
        "    EIM_res = self._EIM_model.update_components()\n",
        "    #Equation 20 in Dream to Explore paper\n",
        "    v_loss = torch.mean(q1_target_ensemble - v_target\n",
        "        - self._get_alpha()[0] * (div_estimate \n",
        "        + EIM_res + EIM_gate_res))\n",
        "    v_w_norm = self._get_v_weight_norm()\n",
        "    norm_loss = self._weight_decays[4] * v_w_norm\n",
        "    loss = v_loss + norm_loss\n",
        "\n",
        "    info = collections.OrderedDict()\n",
        "    info['value_loss'] = v_loss\n",
        "    info['vale_norm'] = v_w_norm\n",
        "    return loss, info\n",
        "\n",
        "  def _build_c_loss(self, batch):\n",
        "    s = batch['s1']\n",
        "    a_b = batch['a1']\n",
        "    _, a_p, _ = self._p_fn(s)\n",
        "    c_loss = self._divergence.dual_critic_loss(\n",
        "        s, a_p, a_b)\n",
        "    c_w_norm = self._get_c_weight_norm()\n",
        "    norm_loss = self._weight_decays[2] * c_w_norm\n",
        "    loss = c_loss + norm_loss\n",
        "\n",
        "    info = collections.OrderedDict()\n",
        "    info['c_loss'] = c_loss\n",
        "    info['c_norm'] = c_w_norm\n",
        "\n",
        "    return loss, info\n",
        "\n",
        "  def _build_a_loss(self, batch):\n",
        "    s = batch['s1']\n",
        "    a_b = batch['a1']\n",
        "    _, a_p, _ = self._p_fn(s)\n",
        "    alpha = self._get_alpha()\n",
        "    div_estimate = self._divergence.dual_estimate(\n",
        "        s, a_p, a_b)\n",
        "    a_loss = - torch.mean(alpha * (div_estimate - self._target_divergence))\n",
        "\n",
        "    info = collections.OrderedDict()\n",
        "    info['a_loss'] = a_loss\n",
        "    info['alpha'] = alpha\n",
        "    info['div_mean'] = torch.mean(div_estimate)\n",
        "    info['div_std'] = torch.std(div_estimate)\n",
        "\n",
        "    return a_loss, info\n",
        "\n",
        "  def _build_ae_loss(self, batch):\n",
        "    s = batch['s1']\n",
        "    _, _, log_pi_a = self._p_fn(s)\n",
        "    alpha = self._get_alpha_entropy()\n",
        "    ae_loss = torch.mean(alpha * (- log_pi_a - self._target_entropy))\n",
        "\n",
        "    info = collections.OrderedDict()\n",
        "    info['ae_loss'] = ae_loss\n",
        "    info['alpha_entropy'] = alpha\n",
        "\n",
        "    return ae_loss, info\n",
        "\n",
        "  def _get_source_target_vars(self):\n",
        "    return (self._agent_module.q_source_variables,\n",
        "            self._agent_module.q_target_variables)\n",
        "\n",
        "  def _build_optimizers(self):\n",
        "    opts = self._optimizers\n",
        "    \n",
        "    if len(opts) == 1:\n",
        "      opts = tuple([opts[0]] * 4)\n",
        "    elif len(opts) < 4:\n",
        "      raise ValueError('Bad optimizers %s.' % opts)\n",
        "      \n",
        "    self._q_optimizer = torch.optim.Adam(self._get_q_vars(),lr=opts[0][0], betas=(opts[0][1],opts[0][2]), weight_decay=self._weight_decays[0])\n",
        "    self._p_optimizer = torch.optim.Adam(self._get_p_vars(),lr=opts[1][0], betas=(opts[1][1],opts[1][2]), weight_decay=self._weight_decays[1])\n",
        "    self._c_optimizer = torch.optim.Adam(self._get_c_vars(),lr=opts[2][0], betas=(opts[2][1],opts[2][2]), weight_decay=self._weight_decays[2])\n",
        "    self._v_optimizer = torch.optim.Adam(self._get_v_vars(),lr=opts[3][0], betas=(opts[3][1],opts[3][2]), weight_decay=self._weight_decays[3])\n",
        "    self._a_optimizer = torch.optim.Adam(self._a_vars, lr=opts[4][0], betas=(opts[4][1],opts[4][2]))\n",
        "    self._ae_optimizer = torch.optim.Adam(self._ae_vars, lr=opts[4][0], betas=(opts[4][1],opts[4][2]))\n",
        "\n",
        "  def _optimize_step(self, batch):\n",
        "    info = collections.OrderedDict()\n",
        "    if torch.equal(self._global_step % torch.tensor(self._update_freq), torch.tensor(0,dtype=torch.float32)):\n",
        "      source_vars, target_vars = self._get_source_target_vars()\n",
        "      self._update_target_fns(source_vars, target_vars)\n",
        "    # Update policy network parameter\n",
        "    #https://bit.ly/3Bno0GC\n",
        "    # policy network's update should be done before updating q network, or there will make some errors\n",
        "    self._agent_module.p_net.train()\n",
        "    self._p_optimizer.zero_grad()\n",
        "    policy_loss,_=self._build_p_loss(batch)\n",
        "    policy_loss.backward(retain_graph=True)\n",
        "    self._p_optimizer.step()\n",
        "    # Update value network\n",
        "    self._agent_module.v_net.train()\n",
        "    self._v_optimizer.zero_grad()\n",
        "    value_loss,_=self._build_v_loss(batch)\n",
        "    value_loss.backward(retain_graph=True)\n",
        "    self._v_optimizer.step()\n",
        "    # Update q networks parameter\n",
        "    for q_fn, q_fn_target in self._q_fns:\n",
        "        q_fn.train()\n",
        "        q_fn_target.train()\n",
        "    self._q_optimizer.zero_grad()\n",
        "    q_losses,q_info= self._build_q_loss(batch)\n",
        "    q_losses.backward()\n",
        "    self._q_optimizer.step()\n",
        "    #Update critic network parameter\n",
        "    self._agent_module.c_net.train()\n",
        "    self._c_optimizer.zero_grad()\n",
        "    critic_loss,_= self._build_c_loss( batch)\n",
        "    critic_loss.backward()\n",
        "    self._c_optimizer.step()\n",
        "    #Train EIM to compute transition ratio\n",
        "    self._EIM_model.train_emm()\n",
        "    \n",
        "    if self._train_alpha:\n",
        "       self._a_optimizer.zero_grad()\n",
        "       a_loss,a_info = self._build_a_loss( batch)\n",
        "       a_loss.backward()\n",
        "       self._a_optimizer.step()\n",
        "    if self._train_alpha_entropy:\n",
        "       self._ae_optimizer.zero_grad()\n",
        "       ae_loss,ae_info = self._build_ae_loss( batch)\n",
        "       ae_loss.backward()\n",
        "       self._ae_optimizer.step()\n",
        "    #1)policy loss\n",
        "    info[\"policy_loss\"]=policy_loss.cpu().item()\n",
        "    #2)Q loss\n",
        "    info[\"Q_loss\"]=q_losses.cpu().item()\n",
        "    info[\"reward_mean\"]= q_info[\"r_mean\"].cpu().item()\n",
        "    info[\"dsc_mean\"]= q_info[\"dsc_mean\"].cpu().item()\n",
        "    info[\"q1_target_mean\"]=q_info[\"q1_target_mean\"].cpu().item()\n",
        "    info[\"q2_target_mean\"]=q_info[\"q2_target_mean\"].cpu().item()\n",
        "        #) value loss\n",
        "    info[\"value_loss\"]= value_loss.cpu().item()\n",
        "    #7) critic loss\n",
        "    info[\"critic_loss\"]=critic_loss.cpu().item()\n",
        "    #8)alpha loss\n",
        "    if self._train_alpha:\n",
        "       info[\"alpha_loss\"]=a_loss.cpu().item()\n",
        "       self._agent_module.assign_alpha(a_info[\"alpha\"])\n",
        "    #10)alpha entropy loss\n",
        "    if self._train_alpha_entropy:\n",
        "       info[\"alpha_entropy_loss\"]=ae_loss.cpu().item()\n",
        "       self._agent_module.assign_alpha_entropy(ae_info[\"alpha_entropy\"])\n",
        "    return info\n",
        "  \n",
        "  def _extra_c_step(self, batch):\n",
        "      self._c_optimizer.zero_grad()\n",
        "      critic_loss,_ = self._build_c_loss( batch)\n",
        "      critic_loss.backward()\n",
        "      self._c_optimizer.step()\n",
        "\n",
        "  def train_step(self):\n",
        "    train_batch = self._get_train_batch()\n",
        "    info = self._optimize_step(train_batch)\n",
        "    for _ in range(self._c_iter - 1):\n",
        "      train_batch = self._get_train_batch()\n",
        "      self._extra_c_step(train_batch)\n",
        "    for key, val in info.items():\n",
        "      self._train_info[key] = val\n",
        "    self._global_step.add(1)\n",
        "\n",
        "\n",
        "  def _build_test_policies(self):\n",
        "    policy = DeterministicSoftPolicy(\n",
        "        a_network=self._agent_module.p_net)\n",
        "    self._test_policies['main'] = policy\n",
        "    policy = MaxQSoftPolicy(\n",
        "        a_network=self._agent_module.p_net,\n",
        "        q_network=self._agent_module.q_nets[0][0],\n",
        "        )\n",
        "    self._test_policies['max_q'] = policy\n",
        "\n",
        "  def _build_online_policy(self):\n",
        "    return RandomSoftPolicy(\n",
        "        a_network=self._agent_module.p_net,\n",
        "        )\n",
        "\n",
        "  def _init_vars(self, batch):\n",
        "    self._build_q_loss(batch)\n",
        "    self._build_p_loss(batch)\n",
        "    self._build_v_loss(batch)\n",
        "    self._build_c_loss(batch)\n",
        "    self._q_vars = self._get_q_vars()\n",
        "    self._p_vars = self._get_p_vars()\n",
        "    self._c_vars = self._get_c_vars()\n",
        "    self._v_vars = self._get_v_vars()\n",
        "    self._a_vars = self._agent_module.a_variables\n",
        "    self._ae_vars = self._agent_module.ae_variables\n",
        "\n",
        "  def _build_checkpointer(self):\n",
        "      checkpoint = {\n",
        "            \"policy_net\": self._agent_module.p_net.state_dict(),\n",
        "            \"critic_net\": self._agent_module.c_net.state_dict(),\n",
        "            \"value_net\" : self._agent_module.v_net.state_dict(),\n",
        "            \"q_optimizer\": self._q_optimizer.state_dict(),\n",
        "            \"critic_optimizer\": self._c_optimizer.state_dict(),\n",
        "            \"policy_optimizer\": self._p_optimizer.state_dict(),\n",
        "            \"value_optimizer\": self._v_optimizer.state_dict(),\n",
        "            \"train_step\": self._global_step\n",
        "      }\n",
        "      for q_fn, q_fn_target in self._q_fns:\n",
        "          checkpoint[\"q_net\"]        = q_fn.state_dict()\n",
        "          checkpoint[\"q_net_target\"] = q_fn_target.state_dict()\n",
        "      if self._train_alpha:\n",
        "          checkpoint[\"alpha\"] = self._alpha_var\n",
        "          checkpoint[\"alpha_optimizer\"] = self._a_optimizer.state_dict()\n",
        "      if self._train_alpha_entropy:\n",
        "          checkpoint[\"alpha_entropy\"] = self._alpha_entropy_var\n",
        "          checkpoint[\"alpha_entropy_optimizer\"] = self._ae_optimizer.state_dict()   \n",
        "      torch.save(checkpoint, self.checkpoint_path)\n",
        "    \n",
        "  def _load_checkpoint(self):\n",
        "        checkpoint = torch.load(self.checkpoint_path, map_location=self.device)  # can load gpu's data on cpu machine\n",
        "        for q_fn, q_fn_target in self._q_fns:\n",
        "            q_fn.load_state_dict(checkpoint[\"q_net\"])\n",
        "            q_fn_target.load_state_dict(checkpoint[\"q_net_target\"])\n",
        "        self._agent_module.p_net.load_state_dict(checkpoint[\"policy_net\"])\n",
        "        self._agent_module.c_net.load_state_dict(checkpoint[\"critic_net\"])\n",
        "        self._agent_module.v_net.load_state_dict(checkpoint[\"value_net\"])\n",
        "        self._p_optimizer.load_state_dict(checkpoint[\"policy_optimizer1\"])\n",
        "        self._q_optimizer.load_state_dict(checkpoint[\"q_optimizer\"])\n",
        "        self._c_optimizer.load_state_dict(checkpoint[\"critic_optimizer\"])\n",
        "        self._v_optimizer.load_state_dict(checkpoint[\"value_optimizer\"])\n",
        "        self._global_step = checkpoint[\"train_step\"]\n",
        "        if self._train_alpha:\n",
        "            self._alpha_var = checkpoint[\"alpha\"]\n",
        "            self._a_optimizer.load_state_dict(checkpoint[\"alpha_optimizer\"])\n",
        "        if self._train_alpha_entropy:\n",
        "            self._alpha_entropy_var=checkpoint[\"alpha_entropy\"]\n",
        "            self._ae_optimizer.load_state_dict(checkpoint[\"alpha_entropy_optimizer\"])\n",
        "        print(\"load checkpoint from \\\"\" + self.checkpoint_path +\n",
        "              \"\\\" at \" + str(self._global_step) + \" time step\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8RdRdJBuz1_",
        "outputId": "bc775a61-fe31-4d82-d696-7af5e2db511d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selecting previously unselected package libgl1-mesa-glx:amd64.\n",
            "(Reading database ... 155685 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libgl1-mesa-glx_20.0.8-0ubuntu1~18.04.1_amd64.deb ...\n",
            "Unpacking libgl1-mesa-glx:amd64 (20.0.8-0ubuntu1~18.04.1) ...\n",
            "Selecting previously unselected package libglew2.0:amd64.\n",
            "Preparing to unpack .../1-libglew2.0_2.0.0-5_amd64.deb ...\n",
            "Unpacking libglew2.0:amd64 (2.0.0-5) ...\n",
            "Selecting previously unselected package libglew-dev:amd64.\n",
            "Preparing to unpack .../2-libglew-dev_2.0.0-5_amd64.deb ...\n",
            "Unpacking libglew-dev:amd64 (2.0.0-5) ...\n",
            "Selecting previously unselected package libglfw3:amd64.\n",
            "Preparing to unpack .../3-libglfw3_3.2.1-1_amd64.deb ...\n",
            "Unpacking libglfw3:amd64 (3.2.1-1) ...\n",
            "Selecting previously unselected package patchelf.\n",
            "Preparing to unpack .../4-patchelf_0.9-1_amd64.deb ...\n",
            "Unpacking patchelf (0.9-1) ...\n",
            "Selecting previously unselected package libosmesa6:amd64.\n",
            "Preparing to unpack .../5-libosmesa6_20.0.8-0ubuntu1~18.04.1_amd64.deb ...\n",
            "Unpacking libosmesa6:amd64 (20.0.8-0ubuntu1~18.04.1) ...\n",
            "Selecting previously unselected package libosmesa6-dev:amd64.\n",
            "Preparing to unpack .../6-libosmesa6-dev_20.0.8-0ubuntu1~18.04.1_amd64.deb ...\n",
            "Unpacking libosmesa6-dev:amd64 (20.0.8-0ubuntu1~18.04.1) ...\n",
            "Setting up patchelf (0.9-1) ...\n",
            "Setting up libosmesa6:amd64 (20.0.8-0ubuntu1~18.04.1) ...\n",
            "Setting up libglfw3:amd64 (3.2.1-1) ...\n",
            "Setting up libgl1-mesa-glx:amd64 (20.0.8-0ubuntu1~18.04.1) ...\n",
            "Setting up libglew2.0:amd64 (2.0.0-5) ...\n",
            "Setting up libglew-dev:amd64 (2.0.0-5) ...\n",
            "Setting up libosmesa6-dev:amd64 (20.0.8-0ubuntu1~18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.5) ...\n",
            "mujoco210\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mujoco-py<2.2,>=2.1\n",
            "  Downloading mujoco_py-2.1.2.14-py3-none-any.whl (2.4 MB)\n",
            "\u001b[K     || 2.4 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: imageio>=2.1.2 in /usr/local/lib/python3.7/dist-packages (from mujoco-py<2.2,>=2.1) (2.9.0)\n",
            "Requirement already satisfied: Cython>=0.27.2 in /usr/local/lib/python3.7/dist-packages (from mujoco-py<2.2,>=2.1) (0.29.32)\n",
            "Requirement already satisfied: cffi>=1.10 in /usr/local/lib/python3.7/dist-packages (from mujoco-py<2.2,>=2.1) (1.15.1)\n",
            "Requirement already satisfied: glfw>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from mujoco-py<2.2,>=2.1) (2.5.5)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from mujoco-py<2.2,>=2.1) (1.21.6)\n",
            "Collecting fasteners~=0.15\n",
            "  Downloading fasteners-0.17.3-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.10->mujoco-py<2.2,>=2.1) (2.21)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio>=2.1.2->mujoco-py<2.2,>=2.1) (7.1.2)\n",
            "Installing collected packages: fasteners, mujoco-py\n",
            "Successfully installed fasteners-0.17.3 mujoco-py-2.1.2.14\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: llvmlite in /usr/local/lib/python3.7/dist-packages (0.39.0)\n",
            "/env/python\n",
            "['/content', '/env/python', '/usr/lib/python37.zip', '/usr/lib/python3.7', '/usr/lib/python3.7/lib-dynload', '', '/usr/local/lib/python3.7/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.7/dist-packages/IPython/extensions', '/root/.ipython', '/content/gdrive/My\\\\ Drive']\n",
            "['/content', '/root/.mujoco/', '/env/python', '/usr/lib/python37.zip', '/usr/lib/python3.7', '/usr/lib/python3.7/lib-dynload', '', '/usr/local/lib/python3.7/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.7/dist-packages/IPython/extensions', '/root/.ipython', '/content/gdrive/My\\\\ Drive']\n",
            "Compiling /usr/local/lib/python3.7/dist-packages/mujoco_py/cymj.pyx because it depends on /usr/local/lib/python3.7/dist-packages/mujoco_py/pxd/mujoco.pxd.\n",
            "[1/1] Cythonizing /usr/local/lib/python3.7/dist-packages/mujoco_py/cymj.pyx\n",
            "running build_ext\n",
            "building 'mujoco_py.cymj' extension\n",
            "creating /usr/local/lib/python3.7/dist-packages/mujoco_py/generated/_pyxbld_2.1.2.14_37_linuxgpuextensionbuilder\n",
            "creating /usr/local/lib/python3.7/dist-packages/mujoco_py/generated/_pyxbld_2.1.2.14_37_linuxgpuextensionbuilder/temp.linux-x86_64-3.7\n",
            "creating /usr/local/lib/python3.7/dist-packages/mujoco_py/generated/_pyxbld_2.1.2.14_37_linuxgpuextensionbuilder/temp.linux-x86_64-3.7/usr\n",
            "creating /usr/local/lib/python3.7/dist-packages/mujoco_py/generated/_pyxbld_2.1.2.14_37_linuxgpuextensionbuilder/temp.linux-x86_64-3.7/usr/local\n",
            "creating /usr/local/lib/python3.7/dist-packages/mujoco_py/generated/_pyxbld_2.1.2.14_37_linuxgpuextensionbuilder/temp.linux-x86_64-3.7/usr/local/lib\n",
            "creating /usr/local/lib/python3.7/dist-packages/mujoco_py/generated/_pyxbld_2.1.2.14_37_linuxgpuextensionbuilder/temp.linux-x86_64-3.7/usr/local/lib/python3.7\n",
            "creating /usr/local/lib/python3.7/dist-packages/mujoco_py/generated/_pyxbld_2.1.2.14_37_linuxgpuextensionbuilder/temp.linux-x86_64-3.7/usr/local/lib/python3.7/dist-packages\n",
            "creating /usr/local/lib/python3.7/dist-packages/mujoco_py/generated/_pyxbld_2.1.2.14_37_linuxgpuextensionbuilder/temp.linux-x86_64-3.7/usr/local/lib/python3.7/dist-packages/mujoco_py\n",
            "creating /usr/local/lib/python3.7/dist-packages/mujoco_py/generated/_pyxbld_2.1.2.14_37_linuxgpuextensionbuilder/temp.linux-x86_64-3.7/usr/local/lib/python3.7/dist-packages/mujoco_py/gl\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/mujoco_py -I/root/.mujoco/mujoco210/include -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -I/usr/local/lib/python3.7/dist-packages/mujoco_py/vendor/egl -I/usr/include/python3.7m -c /usr/local/lib/python3.7/dist-packages/mujoco_py/cymj.c -o /usr/local/lib/python3.7/dist-packages/mujoco_py/generated/_pyxbld_2.1.2.14_37_linuxgpuextensionbuilder/temp.linux-x86_64-3.7/usr/local/lib/python3.7/dist-packages/mujoco_py/cymj.o -fopenmp -w\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/mujoco_py -I/root/.mujoco/mujoco210/include -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -I/usr/local/lib/python3.7/dist-packages/mujoco_py/vendor/egl -I/usr/include/python3.7m -c /usr/local/lib/python3.7/dist-packages/mujoco_py/gl/eglshim.c -o /usr/local/lib/python3.7/dist-packages/mujoco_py/generated/_pyxbld_2.1.2.14_37_linuxgpuextensionbuilder/temp.linux-x86_64-3.7/usr/local/lib/python3.7/dist-packages/mujoco_py/gl/eglshim.o -fopenmp -w\n",
            "creating /usr/local/lib/python3.7/dist-packages/mujoco_py/generated/_pyxbld_2.1.2.14_37_linuxgpuextensionbuilder/lib.linux-x86_64-3.7\n",
            "creating /usr/local/lib/python3.7/dist-packages/mujoco_py/generated/_pyxbld_2.1.2.14_37_linuxgpuextensionbuilder/lib.linux-x86_64-3.7/mujoco_py\n",
            "x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 /usr/local/lib/python3.7/dist-packages/mujoco_py/generated/_pyxbld_2.1.2.14_37_linuxgpuextensionbuilder/temp.linux-x86_64-3.7/usr/local/lib/python3.7/dist-packages/mujoco_py/cymj.o /usr/local/lib/python3.7/dist-packages/mujoco_py/generated/_pyxbld_2.1.2.14_37_linuxgpuextensionbuilder/temp.linux-x86_64-3.7/usr/local/lib/python3.7/dist-packages/mujoco_py/gl/eglshim.o -L/root/.mujoco/mujoco210/bin -Wl,--enable-new-dtags,-R/root/.mujoco/mujoco210/bin -lmujoco210 -lglewegl -o /usr/local/lib/python3.7/dist-packages/mujoco_py/generated/_pyxbld_2.1.2.14_37_linuxgpuextensionbuilder/lib.linux-x86_64-3.7/mujoco_py/cymj.cpython-37m-x86_64-linux-gnu.so -fopenmp\n",
            "/root/.mujoco\n",
            "Cloning into 'mujoco-py'...\n",
            "remote: Enumerating objects: 2183, done.\u001b[K\n",
            "remote: Counting objects: 100% (35/35), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 2183 (delta 7), reused 20 (delta 1), pack-reused 2148\u001b[K\n",
            "Receiving objects: 100% (2183/2183), 5.70 MiB | 28.06 MiB/s, done.\n",
            "Resolving deltas: 100% (1350/1350), done.\n",
            "/root/.mujoco/mujoco-py\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///root/.mujoco/mujoco-py\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Missing build requirements in pyproject.toml for file:///root/.mujoco/mujoco-py.\u001b[0m\n",
            "\u001b[33m  WARNING: The project does not specify a build backend, and pip cannot fall back to setuptools without 'wheel'.\u001b[0m\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Cython>=0.27.2 in /usr/local/lib/python3.7/dist-packages (from mujoco-py==2.1.2.14) (0.29.32)\n",
            "Requirement already satisfied: imageio>=2.1.2 in /usr/local/lib/python3.7/dist-packages (from mujoco-py==2.1.2.14) (2.9.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from mujoco-py==2.1.2.14) (1.21.6)\n",
            "Requirement already satisfied: fasteners~=0.15 in /usr/local/lib/python3.7/dist-packages (from mujoco-py==2.1.2.14) (0.17.3)\n",
            "Requirement already satisfied: glfw>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from mujoco-py==2.1.2.14) (2.5.5)\n",
            "Requirement already satisfied: cffi>=1.10 in /usr/local/lib/python3.7/dist-packages (from mujoco-py==2.1.2.14) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.10->mujoco-py==2.1.2.14) (2.21)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio>=2.1.2->mujoco-py==2.1.2.14) (7.1.2)\n",
            "Installing collected packages: mujoco-py\n",
            "  Attempting uninstall: mujoco-py\n",
            "    Found existing installation: mujoco-py 2.1.2.14\n",
            "    Uninstalling mujoco-py-2.1.2.14:\n",
            "      Successfully uninstalled mujoco-py-2.1.2.14\n",
            "  Running setup.py develop for mujoco-py\n",
            "Successfully installed mujoco-py\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sk-video\n",
            "  Downloading sk_video-1.1.10-py2.py3-none-any.whl (2.3 MB)\n",
            "\u001b[K     || 2.3 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sk-video) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sk-video) (1.7.3)\n",
            "Installing collected packages: sk-video\n",
            "Successfully installed sk-video-1.1.10\n"
          ]
        }
      ],
      "source": [
        "#########################\n",
        "#shorturl.at/oprZ1\n",
        "#from train_eval_utils\n",
        "from typing import Callable\n",
        "\n",
        "# TODO(wuyifan): external version for loading environments\n",
        "import gym\n",
        "import os\n",
        "\n",
        "# Get the prereqs\n",
        "!apt-get -qq update\n",
        "!apt-get -qq install -y libosmesa6-dev libgl1-mesa-glx libglfw3 libgl1-mesa-dev libglew-dev patchelf\n",
        "# Get Mujoco\n",
        "!mkdir ~/.mujoco\n",
        "!wget -q https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz -O mujoco.tar.gz\n",
        "!tar -zxf mujoco.tar.gz -C \"$HOME/.mujoco\"\n",
        "!ls $HOME/.mujoco\n",
        "!rm mujoco.tar.gz\n",
        "# Add it to the actively loaded path and the bashrc path (these only do so much)\n",
        "!echo 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HOME/.mujoco/mujoco210/bin:/usr/lib/nvidia' >> \"/root/.bashrc\" \n",
        "!echo 'export LD_PRELOAD=$LD_PRELOAD:/usr/lib/x86_64-linux-gnu/libGLEW.so' >> \"/root/.bashrc\" \n",
        "# THE ANNOYING ONE, FORCE IT INTO LDCONFIG SO WE ACTUALLY GET ACCESS TO IT THIS SESSION\n",
        "!echo \"/root/.mujoco/mujoco210/bin\" > /etc/ld.so.conf.d/mujoco_ld_lib_path.conf\n",
        "!ldconfig\n",
        "# Install Mujoco-py\n",
        "!pip3 install -U 'mujoco-py<2.2,>=2.1'\n",
        "# run once\n",
        "!touch .mujoco_setup_complete\n",
        "!pip install llvmlite\n",
        "_mujoco_run_once = False\n",
        "if not _mujoco_run_once:\n",
        "  # Add it to the actively loaded path and the bashrc path (these only do so much)\n",
        "  try:\n",
        "    os.environ['LD_LIBRARY_PATH']=os.environ['LD_LIBRARY_PATH'] + ':/root/.mujoco/mujoco210/bin:/usr/lib/nvidia:/usr/local/nvidia/lib:/usr/local/nvidia/lib64'\n",
        "  except KeyError:\n",
        "    os.environ['LD_LIBRARY_PATH']='/root/.mujoco/mujoco210/bin'\n",
        "  try:\n",
        "    os.environ['LD_PRELOAD']=os.environ['LD_PRELOAD'] + ':/usr/lib/x86_64-linux-gnu/libGLEW.so'\n",
        "  except KeyError:\n",
        "    os.environ['LD_PRELOAD']='/usr/lib/x86_64-linux-gnu/libGLEW.so'\n",
        "  # presetup so we don't see output on first env initialization\n",
        "  os.environ['MUJOCO_PY_MUJOCO_PATH'] = '/root/.mujoco/mujoco210'\n",
        "  os.environ['MUJOCO_PY_MJKEY_PATH'] = '/root/.mujoco/mjkey.txt'\n",
        "  !echo $PYTHONPATH\n",
        "  print(sys.path)\n",
        "  sys.path.insert(1, \"/root/.mujoco/\")\n",
        "  print(sys.path)\n",
        "  import mujoco_py\n",
        "  _mujoco_run_once = True\n",
        "###############\n",
        "MJC_PATH = '/root/.mujoco'\n",
        "%cd $MJC_PATH\n",
        "if not os.path.exists('mujoco-py'):\n",
        "  !git clone https://github.com/openai/mujoco-py.git\n",
        "%cd mujoco-py\n",
        "%pip install -e .\n",
        "\n",
        "## cythonize at the first import\n",
        "import mujoco_py\n",
        "!pip install sk-video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "H5OabnefsUIX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3aa010cb-9bd3-4e6b-edb5-84bd5979eaae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting cnest\n",
            "  Downloading cnest-1.0.4.tar.gz (7.3 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11<3.0.0,>=2.6.2\n",
            "  Using cached pybind11-2.10.0-py3-none-any.whl (213 kB)\n",
            "Building wheels for collected packages: cnest\n",
            "  Building wheel for cnest (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cnest: filename=cnest-1.0.4-cp37-cp37m-manylinux_2_27_x86_64.whl size=860534 sha256=8518413524ea1ba2aadb8275a8a87f23bce73e63333db8615d10b8292fa7fa6b\n",
            "  Stored in directory: /root/.cache/pip/wheels/37/33/5d/1d8dd2d4742ab406f7bb0b6adf07226038d35ee1818003d993\n",
            "Successfully built cnest\n",
            "Installing collected packages: pybind11, cnest\n",
            "Successfully installed cnest-1.0.4 pybind11-2.10.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: einops\n",
            "Successfully installed einops-0.4.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sacred\n",
            "  Downloading sacred-0.8.2-py2.py3-none-any.whl (106 kB)\n",
            "\u001b[K     || 106 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from sacred) (1.14.1)\n",
            "Collecting GitPython\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     || 181 kB 31.1 MB/s \n",
            "\u001b[?25hCollecting colorama>=0.4\n",
            "  Downloading colorama-0.4.5-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: packaging>=18.0 in /usr/local/lib/python3.7/dist-packages (from sacred) (21.3)\n",
            "Collecting docopt<1.0,>=0.3\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "Collecting py-cpuinfo>=4.0\n",
            "  Downloading py-cpuinfo-8.0.0.tar.gz (99 kB)\n",
            "\u001b[K     || 99 kB 12.5 MB/s \n",
            "\u001b[?25hCollecting jsonpickle<2.0,>=1.2\n",
            "  Downloading jsonpickle-1.5.2-py2.py3-none-any.whl (37 kB)\n",
            "Collecting munch<3.0,>=2.0.2\n",
            "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonpickle<2.0,>=1.2->sacred) (4.12.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from munch<3.0,>=2.0.2->sacred) (1.15.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=18.0->sacred) (2.4.7)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     || 63 kB 2.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython->sacred) (4.1.1)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->jsonpickle<2.0,>=1.2->sacred) (3.8.1)\n",
            "Building wheels for collected packages: docopt, py-cpuinfo\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=3219dc1ac932f12e225ca14b775ec9f6d26953285c9a4a6d352777daf9550aff\n",
            "  Stored in directory: /root/.cache/pip/wheels/72/b0/3f/1d95f96ff986c7dfffe46ce2be4062f38ebd04b506c77c81b9\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-cpuinfo: filename=py_cpuinfo-8.0.0-py3-none-any.whl size=22257 sha256=b7186679498924c150dbd407822195fe9d637959704961fc55829ea82669b881\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/f1/1f/041add21dc9c4220157f1bd2bd6afe1f1a49524c3396b94401\n",
            "Successfully built docopt py-cpuinfo\n",
            "Installing collected packages: smmap, gitdb, py-cpuinfo, munch, jsonpickle, GitPython, docopt, colorama, sacred\n",
            "Successfully installed GitPython-3.1.27 colorama-0.4.5 docopt-0.6.2 gitdb-4.0.9 jsonpickle-1.5.2 munch-2.5.0 py-cpuinfo-8.0.0 sacred-0.8.2 smmap-5.0.0\n",
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n",
            "-rw------- 1 root root 9987 Aug  8 14:52 '/content/gdrive/My Drive/alf_gym_wrapper.py'\n",
            "data  learn  Pendulum-v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/data_structures.py:51: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "  if isinstance(default_values, collections.Mapping):\n"
          ]
        }
      ],
      "source": [
        "from absl import app\n",
        "from absl import flags\n",
        "from absl import logging\n",
        "\n",
        "import gin\n",
        "import time\n",
        "\n",
        "!pip install cnest\n",
        "!pip install einops\n",
        "!pip install sacred\n",
        "#https://github.com/Haichao-Zhang/alf_randperm_reproduce/blob/d34cb1287a110135566afaa353ceb73faa2f3f91/alf/environments\n",
        "drive.mount('/content/gdrive/')\n",
        "\n",
        "sys.path.append('/content/gdrive/My Drive')\n",
        "sys.path.insert(1, r'/content/gdrive/My Drive/data_structures')\n",
        "sys.path.insert(0,'/content/drive/My Drive/ColabNotebooks')\n",
        "!ls -ltr '/content/gdrive/My Drive/alf_gym_wrapper.py'\n",
        "!cp /content/gdrive/My\\ Drive/alf_gym_wrapper.py /content\n",
        "!cp /content/gdrive/My\\ Drive/alf_environment.py /content\n",
        "!cp /content/gdrive/My\\ Drive/data_structures.py /content\n",
        "!cp /content/gdrive/My\\ Drive/nest.py /content\n",
        "!cp /content/gdrive/My\\ Drive/tensor_specs.py /content\n",
        "import alf_gym_wrapper\n",
        "import importlib\n",
        "\n",
        "def dir_path(path):\n",
        "    if os.path.isdir(path):\n",
        "        return path\n",
        "    else:\n",
        "        raise argparse.ArgumentTypeError(f\"readable_dir:{path} is not a valid path\")\n",
        "\n",
        "!ls /content/gdrive/My\\ Drive/offlinerl\n",
        "\n",
        "import argparse\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lxFp3PVUwQiR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04ba1b62-63b0-40ea-9ec7-062d67fcf58b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:318: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "INFO:absl:14285, 7142, 28573\n",
            "INFO:absl:Testing policy randwalk...\n",
            "/content/tensor_specs.py:290: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  res=torch.tensor(full_shape,dtype=sampling_dtype).uniform_(float(minval), float(maxval))\n",
            "INFO:absl:Return mean -624.8, std 352.4.\n",
            "INFO:absl:Collecting data from policy randwalk...\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:357: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "INFO:absl:(100/14285) steps collected at 547.3 steps/s.\n",
            "INFO:absl:(200/14285) steps collected at 541.1 steps/s.\n",
            "INFO:absl:(300/14285) steps collected at 475 steps/s.\n",
            "INFO:absl:(400/14285) steps collected at 904.6 steps/s.\n",
            "INFO:absl:(500/14285) steps collected at 938.9 steps/s.\n",
            "INFO:absl:(600/14285) steps collected at 837 steps/s.\n",
            "INFO:absl:(700/14285) steps collected at 914.3 steps/s.\n",
            "INFO:absl:(800/14285) steps collected at 884.9 steps/s.\n",
            "INFO:absl:(900/14285) steps collected at 881.7 steps/s.\n",
            "INFO:absl:(1000/14285) steps collected at 836.1 steps/s.\n",
            "INFO:absl:(1100/14285) steps collected at 888 steps/s.\n",
            "INFO:absl:(1200/14285) steps collected at 914.2 steps/s.\n",
            "INFO:absl:(1300/14285) steps collected at 922.1 steps/s.\n",
            "INFO:absl:(1400/14285) steps collected at 873 steps/s.\n",
            "INFO:absl:(1500/14285) steps collected at 857.9 steps/s.\n",
            "INFO:absl:(1600/14285) steps collected at 904.5 steps/s.\n",
            "INFO:absl:(1700/14285) steps collected at 923.5 steps/s.\n",
            "INFO:absl:(1800/14285) steps collected at 905.5 steps/s.\n",
            "INFO:absl:(1900/14285) steps collected at 912.2 steps/s.\n",
            "INFO:absl:(2000/14285) steps collected at 884.4 steps/s.\n",
            "INFO:absl:(2100/14285) steps collected at 825.8 steps/s.\n",
            "INFO:absl:(2200/14285) steps collected at 891.2 steps/s.\n",
            "INFO:absl:(2300/14285) steps collected at 860.3 steps/s.\n",
            "INFO:absl:(2400/14285) steps collected at 922.5 steps/s.\n",
            "INFO:absl:(2500/14285) steps collected at 888.7 steps/s.\n",
            "INFO:absl:(2600/14285) steps collected at 920.8 steps/s.\n",
            "INFO:absl:(2700/14285) steps collected at 892.7 steps/s.\n",
            "INFO:absl:(2800/14285) steps collected at 945.6 steps/s.\n",
            "INFO:absl:(2900/14285) steps collected at 948.5 steps/s.\n",
            "INFO:absl:(3000/14285) steps collected at 883 steps/s.\n",
            "INFO:absl:(3100/14285) steps collected at 938.4 steps/s.\n",
            "INFO:absl:(3200/14285) steps collected at 935 steps/s.\n",
            "INFO:absl:(3300/14285) steps collected at 863.6 steps/s.\n",
            "INFO:absl:(3400/14285) steps collected at 855 steps/s.\n",
            "INFO:absl:(3500/14285) steps collected at 883.4 steps/s.\n",
            "INFO:absl:(3600/14285) steps collected at 923.8 steps/s.\n",
            "INFO:absl:(3700/14285) steps collected at 917.2 steps/s.\n",
            "INFO:absl:(3800/14285) steps collected at 904.1 steps/s.\n",
            "INFO:absl:(3900/14285) steps collected at 887.2 steps/s.\n",
            "INFO:absl:(4000/14285) steps collected at 939 steps/s.\n",
            "INFO:absl:(4100/14285) steps collected at 868.5 steps/s.\n",
            "INFO:absl:(4200/14285) steps collected at 916.8 steps/s.\n",
            "INFO:absl:(4300/14285) steps collected at 838.7 steps/s.\n",
            "INFO:absl:(4400/14285) steps collected at 883.4 steps/s.\n",
            "INFO:absl:(4500/14285) steps collected at 892.9 steps/s.\n",
            "INFO:absl:(4600/14285) steps collected at 882.1 steps/s.\n",
            "INFO:absl:(4700/14285) steps collected at 908.4 steps/s.\n",
            "INFO:absl:(4800/14285) steps collected at 897.9 steps/s.\n",
            "INFO:absl:(4900/14285) steps collected at 853.2 steps/s.\n",
            "INFO:absl:(5000/14285) steps collected at 886.6 steps/s.\n",
            "INFO:absl:(5100/14285) steps collected at 870.5 steps/s.\n",
            "INFO:absl:(5200/14285) steps collected at 853.2 steps/s.\n",
            "INFO:absl:(5300/14285) steps collected at 901.7 steps/s.\n",
            "INFO:absl:(5400/14285) steps collected at 901.1 steps/s.\n",
            "INFO:absl:(5500/14285) steps collected at 913.7 steps/s.\n",
            "INFO:absl:(5600/14285) steps collected at 940.2 steps/s.\n",
            "INFO:absl:(5700/14285) steps collected at 893.9 steps/s.\n",
            "INFO:absl:(5800/14285) steps collected at 932.7 steps/s.\n",
            "INFO:absl:(5900/14285) steps collected at 917.8 steps/s.\n",
            "INFO:absl:(6000/14285) steps collected at 925.9 steps/s.\n",
            "INFO:absl:(6100/14285) steps collected at 862.4 steps/s.\n",
            "INFO:absl:(6200/14285) steps collected at 921.3 steps/s.\n",
            "INFO:absl:(6300/14285) steps collected at 925.6 steps/s.\n",
            "INFO:absl:(6400/14285) steps collected at 963.3 steps/s.\n",
            "INFO:absl:(6500/14285) steps collected at 888 steps/s.\n",
            "INFO:absl:(6600/14285) steps collected at 927.9 steps/s.\n",
            "INFO:absl:(6700/14285) steps collected at 953.7 steps/s.\n",
            "INFO:absl:(6800/14285) steps collected at 898.5 steps/s.\n",
            "INFO:absl:(6900/14285) steps collected at 909.7 steps/s.\n",
            "INFO:absl:(7000/14285) steps collected at 877.8 steps/s.\n",
            "INFO:absl:(7100/14285) steps collected at 880.7 steps/s.\n",
            "INFO:absl:(7200/14285) steps collected at 940.6 steps/s.\n",
            "INFO:absl:(7300/14285) steps collected at 931.4 steps/s.\n",
            "INFO:absl:(7400/14285) steps collected at 966 steps/s.\n",
            "INFO:absl:(7500/14285) steps collected at 927.6 steps/s.\n",
            "INFO:absl:(7600/14285) steps collected at 892.2 steps/s.\n",
            "INFO:absl:(7700/14285) steps collected at 938.1 steps/s.\n",
            "INFO:absl:(7800/14285) steps collected at 947.6 steps/s.\n",
            "INFO:absl:(7900/14285) steps collected at 934.1 steps/s.\n",
            "INFO:absl:(8000/14285) steps collected at 851.3 steps/s.\n",
            "INFO:absl:(8100/14285) steps collected at 888.9 steps/s.\n",
            "INFO:absl:(8200/14285) steps collected at 954.9 steps/s.\n",
            "INFO:absl:(8300/14285) steps collected at 947.4 steps/s.\n",
            "INFO:absl:(8400/14285) steps collected at 902.6 steps/s.\n",
            "INFO:absl:(8500/14285) steps collected at 932.8 steps/s.\n",
            "INFO:absl:(8600/14285) steps collected at 917.5 steps/s.\n",
            "INFO:absl:(8700/14285) steps collected at 905.5 steps/s.\n",
            "INFO:absl:(8800/14285) steps collected at 905.8 steps/s.\n",
            "INFO:absl:(8900/14285) steps collected at 855.7 steps/s.\n",
            "INFO:absl:(9000/14285) steps collected at 876.9 steps/s.\n",
            "INFO:absl:(9100/14285) steps collected at 843.5 steps/s.\n",
            "INFO:absl:(9200/14285) steps collected at 935.8 steps/s.\n",
            "INFO:absl:(9300/14285) steps collected at 946 steps/s.\n",
            "INFO:absl:(9400/14285) steps collected at 947.8 steps/s.\n",
            "INFO:absl:(9500/14285) steps collected at 910.8 steps/s.\n",
            "INFO:absl:(9600/14285) steps collected at 947.9 steps/s.\n",
            "INFO:absl:(9700/14285) steps collected at 906.5 steps/s.\n",
            "INFO:absl:(9800/14285) steps collected at 883.9 steps/s.\n",
            "INFO:absl:(9900/14285) steps collected at 864.2 steps/s.\n",
            "INFO:absl:(10000/14285) steps collected at 911.6 steps/s.\n",
            "INFO:absl:(10100/14285) steps collected at 895.9 steps/s.\n",
            "INFO:absl:(10200/14285) steps collected at 844.2 steps/s.\n",
            "INFO:absl:(10300/14285) steps collected at 824.8 steps/s.\n",
            "INFO:absl:(10400/14285) steps collected at 889.4 steps/s.\n",
            "INFO:absl:(10500/14285) steps collected at 915.1 steps/s.\n",
            "INFO:absl:(10600/14285) steps collected at 913.9 steps/s.\n",
            "INFO:absl:(10700/14285) steps collected at 921.8 steps/s.\n",
            "INFO:absl:(10800/14285) steps collected at 839.2 steps/s.\n",
            "INFO:absl:(10900/14285) steps collected at 893.5 steps/s.\n",
            "INFO:absl:(11000/14285) steps collected at 911 steps/s.\n",
            "INFO:absl:(11100/14285) steps collected at 863.4 steps/s.\n",
            "INFO:absl:(11200/14285) steps collected at 901.1 steps/s.\n",
            "INFO:absl:(11300/14285) steps collected at 861.6 steps/s.\n",
            "INFO:absl:(11400/14285) steps collected at 883 steps/s.\n",
            "INFO:absl:(11500/14285) steps collected at 929.5 steps/s.\n",
            "INFO:absl:(11600/14285) steps collected at 890 steps/s.\n",
            "INFO:absl:(11700/14285) steps collected at 770.6 steps/s.\n",
            "INFO:absl:(11800/14285) steps collected at 880.8 steps/s.\n",
            "INFO:absl:(11900/14285) steps collected at 918.7 steps/s.\n",
            "INFO:absl:(12000/14285) steps collected at 875.8 steps/s.\n",
            "INFO:absl:(12100/14285) steps collected at 894.6 steps/s.\n",
            "INFO:absl:(12200/14285) steps collected at 863.7 steps/s.\n",
            "INFO:absl:(12300/14285) steps collected at 913.2 steps/s.\n",
            "INFO:absl:(12400/14285) steps collected at 888.3 steps/s.\n",
            "INFO:absl:(12500/14285) steps collected at 904.3 steps/s.\n",
            "INFO:absl:(12600/14285) steps collected at 848 steps/s.\n",
            "INFO:absl:(12700/14285) steps collected at 941.7 steps/s.\n",
            "INFO:absl:(12800/14285) steps collected at 877.4 steps/s.\n",
            "INFO:absl:(12900/14285) steps collected at 863.7 steps/s.\n",
            "INFO:absl:(13000/14285) steps collected at 852.8 steps/s.\n",
            "INFO:absl:(13100/14285) steps collected at 866.4 steps/s.\n",
            "INFO:absl:(13200/14285) steps collected at 884.1 steps/s.\n",
            "INFO:absl:(13300/14285) steps collected at 846.3 steps/s.\n",
            "INFO:absl:(13400/14285) steps collected at 905.3 steps/s.\n",
            "INFO:absl:(13500/14285) steps collected at 844.8 steps/s.\n",
            "INFO:absl:(13600/14285) steps collected at 928.2 steps/s.\n",
            "INFO:absl:(13700/14285) steps collected at 901 steps/s.\n",
            "INFO:absl:(13800/14285) steps collected at 848.1 steps/s.\n",
            "INFO:absl:(13900/14285) steps collected at 882.3 steps/s.\n",
            "INFO:absl:(14000/14285) steps collected at 851.8 steps/s.\n",
            "INFO:absl:(14100/14285) steps collected at 902.9 steps/s.\n",
            "INFO:absl:(14200/14285) steps collected at 869.9 steps/s.\n",
            "INFO:absl:(14285/14285) steps collected at 907.4 steps/s.\n",
            "INFO:absl:Testing policy p1_pure...\n",
            "INFO:absl:Return mean -385.4, std 287.3.\n",
            "INFO:absl:Collecting data from policy p1_pure...\n",
            "INFO:absl:(100/7142) steps collected at 192.2 steps/s.\n",
            "INFO:absl:(200/7142) steps collected at 259.5 steps/s.\n",
            "INFO:absl:(300/7142) steps collected at 288.8 steps/s.\n",
            "INFO:absl:(400/7142) steps collected at 285.9 steps/s.\n",
            "INFO:absl:(500/7142) steps collected at 286.5 steps/s.\n",
            "INFO:absl:(600/7142) steps collected at 282 steps/s.\n",
            "INFO:absl:(700/7142) steps collected at 280.3 steps/s.\n",
            "INFO:absl:(800/7142) steps collected at 290.1 steps/s.\n",
            "INFO:absl:(900/7142) steps collected at 285 steps/s.\n",
            "INFO:absl:(1000/7142) steps collected at 290.1 steps/s.\n",
            "INFO:absl:(1100/7142) steps collected at 284.5 steps/s.\n",
            "INFO:absl:(1200/7142) steps collected at 286.6 steps/s.\n",
            "INFO:absl:(1300/7142) steps collected at 282.9 steps/s.\n",
            "INFO:absl:(1400/7142) steps collected at 278.6 steps/s.\n",
            "INFO:absl:(1500/7142) steps collected at 288.2 steps/s.\n",
            "INFO:absl:(1600/7142) steps collected at 288 steps/s.\n",
            "INFO:absl:(1700/7142) steps collected at 285.8 steps/s.\n",
            "INFO:absl:(1800/7142) steps collected at 283.2 steps/s.\n",
            "INFO:absl:(1900/7142) steps collected at 284.8 steps/s.\n",
            "INFO:absl:(2000/7142) steps collected at 285.2 steps/s.\n",
            "INFO:absl:(2100/7142) steps collected at 286.9 steps/s.\n",
            "INFO:absl:(2200/7142) steps collected at 283.9 steps/s.\n",
            "INFO:absl:(2300/7142) steps collected at 280.7 steps/s.\n",
            "INFO:absl:(2400/7142) steps collected at 288.6 steps/s.\n",
            "INFO:absl:(2500/7142) steps collected at 279.7 steps/s.\n",
            "INFO:absl:(2600/7142) steps collected at 286.7 steps/s.\n",
            "INFO:absl:(2700/7142) steps collected at 284.1 steps/s.\n",
            "INFO:absl:(2800/7142) steps collected at 282.7 steps/s.\n",
            "INFO:absl:(2900/7142) steps collected at 288 steps/s.\n",
            "INFO:absl:(3000/7142) steps collected at 303.3 steps/s.\n",
            "INFO:absl:(3100/7142) steps collected at 276.1 steps/s.\n",
            "INFO:absl:(3200/7142) steps collected at 291.2 steps/s.\n",
            "INFO:absl:(3300/7142) steps collected at 290.4 steps/s.\n",
            "INFO:absl:(3400/7142) steps collected at 279.2 steps/s.\n",
            "INFO:absl:(3500/7142) steps collected at 276.4 steps/s.\n",
            "INFO:absl:(3600/7142) steps collected at 285 steps/s.\n",
            "INFO:absl:(3700/7142) steps collected at 281.5 steps/s.\n",
            "INFO:absl:(3800/7142) steps collected at 288.8 steps/s.\n",
            "INFO:absl:(3900/7142) steps collected at 282.5 steps/s.\n",
            "INFO:absl:(4000/7142) steps collected at 286.8 steps/s.\n",
            "INFO:absl:(4100/7142) steps collected at 292.5 steps/s.\n",
            "INFO:absl:(4200/7142) steps collected at 289.3 steps/s.\n",
            "INFO:absl:(4300/7142) steps collected at 284.2 steps/s.\n",
            "INFO:absl:(4400/7142) steps collected at 293.1 steps/s.\n",
            "INFO:absl:(4500/7142) steps collected at 286.1 steps/s.\n",
            "INFO:absl:(4600/7142) steps collected at 286.1 steps/s.\n",
            "INFO:absl:(4700/7142) steps collected at 288.2 steps/s.\n",
            "INFO:absl:(4800/7142) steps collected at 281.7 steps/s.\n",
            "INFO:absl:(4900/7142) steps collected at 271.7 steps/s.\n",
            "INFO:absl:(5000/7142) steps collected at 288.4 steps/s.\n",
            "INFO:absl:(5100/7142) steps collected at 273.5 steps/s.\n",
            "INFO:absl:(5200/7142) steps collected at 289.6 steps/s.\n",
            "INFO:absl:(5300/7142) steps collected at 292.2 steps/s.\n",
            "INFO:absl:(5400/7142) steps collected at 283.7 steps/s.\n",
            "INFO:absl:(5500/7142) steps collected at 284.6 steps/s.\n",
            "INFO:absl:(5600/7142) steps collected at 292.2 steps/s.\n",
            "INFO:absl:(5700/7142) steps collected at 280 steps/s.\n",
            "INFO:absl:(5800/7142) steps collected at 290.8 steps/s.\n",
            "INFO:absl:(5900/7142) steps collected at 278.5 steps/s.\n",
            "INFO:absl:(6000/7142) steps collected at 275.3 steps/s.\n",
            "INFO:absl:(6100/7142) steps collected at 277.4 steps/s.\n",
            "INFO:absl:(6200/7142) steps collected at 288.2 steps/s.\n",
            "INFO:absl:(6300/7142) steps collected at 273.1 steps/s.\n",
            "INFO:absl:(6400/7142) steps collected at 280.7 steps/s.\n",
            "INFO:absl:(6500/7142) steps collected at 277.6 steps/s.\n",
            "INFO:absl:(6600/7142) steps collected at 283.8 steps/s.\n",
            "INFO:absl:(6700/7142) steps collected at 287.8 steps/s.\n",
            "INFO:absl:(6800/7142) steps collected at 278.8 steps/s.\n",
            "INFO:absl:(6900/7142) steps collected at 286.3 steps/s.\n",
            "INFO:absl:(7000/7142) steps collected at 295.4 steps/s.\n",
            "INFO:absl:(7100/7142) steps collected at 278.1 steps/s.\n",
            "INFO:absl:(7142/7142) steps collected at 291 steps/s.\n",
            "INFO:absl:Testing policy p1_gaussian...\n",
            "INFO:absl:Return mean -625.9, std 374.9.\n",
            "INFO:absl:Collecting data from policy p1_gaussian...\n",
            "INFO:absl:(100/28573) steps collected at 266.7 steps/s.\n",
            "INFO:absl:(200/28573) steps collected at 276.2 steps/s.\n",
            "INFO:absl:(300/28573) steps collected at 274.4 steps/s.\n",
            "INFO:absl:(400/28573) steps collected at 270.6 steps/s.\n",
            "INFO:absl:(500/28573) steps collected at 278 steps/s.\n",
            "INFO:absl:(600/28573) steps collected at 280.4 steps/s.\n",
            "INFO:absl:(700/28573) steps collected at 267.7 steps/s.\n",
            "INFO:absl:(800/28573) steps collected at 283 steps/s.\n",
            "INFO:absl:(900/28573) steps collected at 269.9 steps/s.\n",
            "INFO:absl:(1000/28573) steps collected at 275 steps/s.\n",
            "INFO:absl:(1100/28573) steps collected at 274.1 steps/s.\n",
            "INFO:absl:(1200/28573) steps collected at 274.8 steps/s.\n",
            "INFO:absl:(1300/28573) steps collected at 277.5 steps/s.\n",
            "INFO:absl:(1400/28573) steps collected at 276.8 steps/s.\n",
            "INFO:absl:(1500/28573) steps collected at 272.2 steps/s.\n",
            "INFO:absl:(1600/28573) steps collected at 284.9 steps/s.\n",
            "INFO:absl:(1700/28573) steps collected at 273.6 steps/s.\n",
            "INFO:absl:(1800/28573) steps collected at 272.3 steps/s.\n",
            "INFO:absl:(1900/28573) steps collected at 270 steps/s.\n",
            "INFO:absl:(2000/28573) steps collected at 279.6 steps/s.\n",
            "INFO:absl:(2100/28573) steps collected at 271.6 steps/s.\n",
            "INFO:absl:(2200/28573) steps collected at 273.2 steps/s.\n",
            "INFO:absl:(2300/28573) steps collected at 269.6 steps/s.\n",
            "INFO:absl:(2400/28573) steps collected at 278.5 steps/s.\n",
            "INFO:absl:(2500/28573) steps collected at 279.8 steps/s.\n",
            "INFO:absl:(2600/28573) steps collected at 268.2 steps/s.\n",
            "INFO:absl:(2700/28573) steps collected at 282.5 steps/s.\n",
            "INFO:absl:(2800/28573) steps collected at 276.8 steps/s.\n",
            "INFO:absl:(2900/28573) steps collected at 273 steps/s.\n",
            "INFO:absl:(3000/28573) steps collected at 282.3 steps/s.\n",
            "INFO:absl:(3100/28573) steps collected at 275.6 steps/s.\n",
            "INFO:absl:(3200/28573) steps collected at 275.3 steps/s.\n",
            "INFO:absl:(3300/28573) steps collected at 274.4 steps/s.\n",
            "INFO:absl:(3400/28573) steps collected at 274 steps/s.\n",
            "INFO:absl:(3500/28573) steps collected at 273.6 steps/s.\n",
            "INFO:absl:(3600/28573) steps collected at 280.9 steps/s.\n",
            "INFO:absl:(3700/28573) steps collected at 267.2 steps/s.\n",
            "INFO:absl:(3800/28573) steps collected at 283.5 steps/s.\n",
            "INFO:absl:(3900/28573) steps collected at 275.8 steps/s.\n",
            "INFO:absl:(4000/28573) steps collected at 277 steps/s.\n",
            "INFO:absl:(4100/28573) steps collected at 287.3 steps/s.\n",
            "INFO:absl:(4200/28573) steps collected at 282.1 steps/s.\n",
            "INFO:absl:(4300/28573) steps collected at 274.2 steps/s.\n",
            "INFO:absl:(4400/28573) steps collected at 283.1 steps/s.\n",
            "INFO:absl:(4500/28573) steps collected at 270.4 steps/s.\n",
            "INFO:absl:(4600/28573) steps collected at 277.7 steps/s.\n",
            "INFO:absl:(4700/28573) steps collected at 259 steps/s.\n",
            "INFO:absl:(4800/28573) steps collected at 280.5 steps/s.\n",
            "INFO:absl:(4900/28573) steps collected at 279.3 steps/s.\n",
            "INFO:absl:(5000/28573) steps collected at 286.4 steps/s.\n",
            "INFO:absl:(5100/28573) steps collected at 271.5 steps/s.\n",
            "INFO:absl:(5200/28573) steps collected at 275.2 steps/s.\n",
            "INFO:absl:(5300/28573) steps collected at 276.8 steps/s.\n",
            "INFO:absl:(5400/28573) steps collected at 280.3 steps/s.\n",
            "INFO:absl:(5500/28573) steps collected at 289 steps/s.\n",
            "INFO:absl:(5600/28573) steps collected at 280.5 steps/s.\n",
            "INFO:absl:(5700/28573) steps collected at 273.7 steps/s.\n",
            "INFO:absl:(5800/28573) steps collected at 290.2 steps/s.\n",
            "INFO:absl:(5900/28573) steps collected at 279.6 steps/s.\n",
            "INFO:absl:(6000/28573) steps collected at 278.7 steps/s.\n",
            "INFO:absl:(6100/28573) steps collected at 285.7 steps/s.\n",
            "INFO:absl:(6200/28573) steps collected at 273.8 steps/s.\n",
            "INFO:absl:(6300/28573) steps collected at 274 steps/s.\n",
            "INFO:absl:(6400/28573) steps collected at 279.9 steps/s.\n",
            "INFO:absl:(6500/28573) steps collected at 279.2 steps/s.\n",
            "INFO:absl:(6600/28573) steps collected at 282.2 steps/s.\n",
            "INFO:absl:(6700/28573) steps collected at 282.6 steps/s.\n",
            "INFO:absl:(6800/28573) steps collected at 277.9 steps/s.\n",
            "INFO:absl:(6900/28573) steps collected at 268.7 steps/s.\n",
            "INFO:absl:(7000/28573) steps collected at 283.6 steps/s.\n",
            "INFO:absl:(7100/28573) steps collected at 285.8 steps/s.\n",
            "INFO:absl:(7200/28573) steps collected at 277.6 steps/s.\n",
            "INFO:absl:(7300/28573) steps collected at 278.1 steps/s.\n",
            "INFO:absl:(7400/28573) steps collected at 276.3 steps/s.\n",
            "INFO:absl:(7500/28573) steps collected at 272.6 steps/s.\n",
            "INFO:absl:(7600/28573) steps collected at 268.5 steps/s.\n",
            "INFO:absl:(7700/28573) steps collected at 273.4 steps/s.\n",
            "INFO:absl:(7800/28573) steps collected at 274.2 steps/s.\n",
            "INFO:absl:(7900/28573) steps collected at 273.6 steps/s.\n",
            "INFO:absl:(8000/28573) steps collected at 277.6 steps/s.\n",
            "INFO:absl:(8100/28573) steps collected at 274.3 steps/s.\n",
            "INFO:absl:(8200/28573) steps collected at 276.2 steps/s.\n",
            "INFO:absl:(8300/28573) steps collected at 269.2 steps/s.\n",
            "INFO:absl:(8400/28573) steps collected at 275.4 steps/s.\n",
            "INFO:absl:(8500/28573) steps collected at 272.3 steps/s.\n",
            "INFO:absl:(8600/28573) steps collected at 271.4 steps/s.\n",
            "INFO:absl:(8700/28573) steps collected at 276.5 steps/s.\n",
            "INFO:absl:(8800/28573) steps collected at 275.7 steps/s.\n",
            "INFO:absl:(8900/28573) steps collected at 272.9 steps/s.\n",
            "INFO:absl:(9000/28573) steps collected at 278.3 steps/s.\n",
            "INFO:absl:(9100/28573) steps collected at 267.8 steps/s.\n",
            "INFO:absl:(9200/28573) steps collected at 283.5 steps/s.\n",
            "INFO:absl:(9300/28573) steps collected at 276.7 steps/s.\n",
            "INFO:absl:(9400/28573) steps collected at 280.4 steps/s.\n",
            "INFO:absl:(9500/28573) steps collected at 275.7 steps/s.\n",
            "INFO:absl:(9600/28573) steps collected at 284.1 steps/s.\n",
            "INFO:absl:(9700/28573) steps collected at 273.6 steps/s.\n",
            "INFO:absl:(9800/28573) steps collected at 279.3 steps/s.\n",
            "INFO:absl:(9900/28573) steps collected at 282.5 steps/s.\n",
            "INFO:absl:(10000/28573) steps collected at 273.8 steps/s.\n",
            "INFO:absl:(10100/28573) steps collected at 278.9 steps/s.\n",
            "INFO:absl:(10200/28573) steps collected at 271.6 steps/s.\n",
            "INFO:absl:(10300/28573) steps collected at 271.4 steps/s.\n",
            "INFO:absl:(10400/28573) steps collected at 274.5 steps/s.\n",
            "INFO:absl:(10500/28573) steps collected at 275.6 steps/s.\n",
            "INFO:absl:(10600/28573) steps collected at 274.7 steps/s.\n",
            "INFO:absl:(10700/28573) steps collected at 276.5 steps/s.\n",
            "INFO:absl:(10800/28573) steps collected at 272.7 steps/s.\n",
            "INFO:absl:(10900/28573) steps collected at 270.2 steps/s.\n",
            "INFO:absl:(11000/28573) steps collected at 285.5 steps/s.\n",
            "INFO:absl:(11100/28573) steps collected at 272.3 steps/s.\n",
            "INFO:absl:(11200/28573) steps collected at 283.3 steps/s.\n",
            "INFO:absl:(11300/28573) steps collected at 282.1 steps/s.\n",
            "INFO:absl:(11400/28573) steps collected at 276.8 steps/s.\n",
            "INFO:absl:(11500/28573) steps collected at 282.5 steps/s.\n",
            "INFO:absl:(11600/28573) steps collected at 280.6 steps/s.\n",
            "INFO:absl:(11700/28573) steps collected at 273.4 steps/s.\n",
            "INFO:absl:(11800/28573) steps collected at 281.8 steps/s.\n",
            "INFO:absl:(11900/28573) steps collected at 283.3 steps/s.\n",
            "INFO:absl:(12000/28573) steps collected at 266 steps/s.\n",
            "INFO:absl:(12100/28573) steps collected at 281.5 steps/s.\n",
            "INFO:absl:(12200/28573) steps collected at 274.3 steps/s.\n",
            "INFO:absl:(12300/28573) steps collected at 278.3 steps/s.\n",
            "INFO:absl:(12400/28573) steps collected at 280.2 steps/s.\n",
            "INFO:absl:(12500/28573) steps collected at 261.9 steps/s.\n",
            "INFO:absl:(12600/28573) steps collected at 281.6 steps/s.\n",
            "INFO:absl:(12700/28573) steps collected at 276.5 steps/s.\n",
            "INFO:absl:(12800/28573) steps collected at 273 steps/s.\n",
            "INFO:absl:(12900/28573) steps collected at 284.1 steps/s.\n",
            "INFO:absl:(13000/28573) steps collected at 280 steps/s.\n",
            "INFO:absl:(13100/28573) steps collected at 271 steps/s.\n",
            "INFO:absl:(13200/28573) steps collected at 268.9 steps/s.\n",
            "INFO:absl:(13300/28573) steps collected at 276.1 steps/s.\n",
            "INFO:absl:(13400/28573) steps collected at 274.4 steps/s.\n",
            "INFO:absl:(13500/28573) steps collected at 281.4 steps/s.\n",
            "INFO:absl:(13600/28573) steps collected at 269.9 steps/s.\n",
            "INFO:absl:(13700/28573) steps collected at 282.9 steps/s.\n",
            "INFO:absl:(13800/28573) steps collected at 286.3 steps/s.\n",
            "INFO:absl:(13900/28573) steps collected at 268.4 steps/s.\n",
            "INFO:absl:(14000/28573) steps collected at 280.4 steps/s.\n",
            "INFO:absl:(14100/28573) steps collected at 280.8 steps/s.\n",
            "INFO:absl:(14200/28573) steps collected at 272.3 steps/s.\n",
            "INFO:absl:(14300/28573) steps collected at 279.8 steps/s.\n",
            "INFO:absl:(14400/28573) steps collected at 278.4 steps/s.\n",
            "INFO:absl:(14500/28573) steps collected at 277.1 steps/s.\n",
            "INFO:absl:(14600/28573) steps collected at 285.3 steps/s.\n",
            "INFO:absl:(14700/28573) steps collected at 275.3 steps/s.\n",
            "INFO:absl:(14800/28573) steps collected at 275.1 steps/s.\n",
            "INFO:absl:(14900/28573) steps collected at 276.6 steps/s.\n",
            "INFO:absl:(15000/28573) steps collected at 276.6 steps/s.\n",
            "INFO:absl:(15100/28573) steps collected at 275 steps/s.\n",
            "INFO:absl:(15200/28573) steps collected at 271.4 steps/s.\n",
            "INFO:absl:(15300/28573) steps collected at 272.5 steps/s.\n",
            "INFO:absl:(15400/28573) steps collected at 284.8 steps/s.\n",
            "INFO:absl:(15500/28573) steps collected at 290.5 steps/s.\n",
            "INFO:absl:(15600/28573) steps collected at 204.5 steps/s.\n",
            "INFO:absl:(15700/28573) steps collected at 128.8 steps/s.\n",
            "INFO:absl:(15800/28573) steps collected at 265.3 steps/s.\n",
            "INFO:absl:(15900/28573) steps collected at 279.3 steps/s.\n",
            "INFO:absl:(16000/28573) steps collected at 265 steps/s.\n",
            "INFO:absl:(16100/28573) steps collected at 273.4 steps/s.\n",
            "INFO:absl:(16200/28573) steps collected at 279 steps/s.\n",
            "INFO:absl:(16300/28573) steps collected at 266.5 steps/s.\n",
            "INFO:absl:(16400/28573) steps collected at 274.5 steps/s.\n",
            "INFO:absl:(16500/28573) steps collected at 271.7 steps/s.\n",
            "INFO:absl:(16600/28573) steps collected at 272.4 steps/s.\n",
            "INFO:absl:(16700/28573) steps collected at 267.7 steps/s.\n",
            "INFO:absl:(16800/28573) steps collected at 272.6 steps/s.\n",
            "INFO:absl:(16900/28573) steps collected at 273.5 steps/s.\n",
            "INFO:absl:(17000/28573) steps collected at 273.1 steps/s.\n",
            "INFO:absl:(17100/28573) steps collected at 261.9 steps/s.\n",
            "INFO:absl:(17200/28573) steps collected at 277.9 steps/s.\n",
            "INFO:absl:(17300/28573) steps collected at 267.7 steps/s.\n",
            "INFO:absl:(17400/28573) steps collected at 270.9 steps/s.\n",
            "INFO:absl:(17500/28573) steps collected at 279.3 steps/s.\n",
            "INFO:absl:(17600/28573) steps collected at 274.1 steps/s.\n",
            "INFO:absl:(17700/28573) steps collected at 273.9 steps/s.\n",
            "INFO:absl:(17800/28573) steps collected at 277 steps/s.\n",
            "INFO:absl:(17900/28573) steps collected at 272.9 steps/s.\n",
            "INFO:absl:(18000/28573) steps collected at 169.4 steps/s.\n",
            "INFO:absl:(18100/28573) steps collected at 150.4 steps/s.\n",
            "INFO:absl:(18200/28573) steps collected at 128 steps/s.\n",
            "INFO:absl:(18300/28573) steps collected at 228 steps/s.\n",
            "INFO:absl:(18400/28573) steps collected at 268.8 steps/s.\n",
            "INFO:absl:(18500/28573) steps collected at 265.5 steps/s.\n",
            "INFO:absl:(18600/28573) steps collected at 281.8 steps/s.\n",
            "INFO:absl:(18700/28573) steps collected at 276.4 steps/s.\n",
            "INFO:absl:(18800/28573) steps collected at 278.3 steps/s.\n",
            "INFO:absl:(18900/28573) steps collected at 276.4 steps/s.\n",
            "INFO:absl:(19000/28573) steps collected at 278.1 steps/s.\n",
            "INFO:absl:(19100/28573) steps collected at 275.1 steps/s.\n",
            "INFO:absl:(19200/28573) steps collected at 269.7 steps/s.\n",
            "INFO:absl:(19300/28573) steps collected at 273.1 steps/s.\n",
            "INFO:absl:(19400/28573) steps collected at 270.1 steps/s.\n",
            "INFO:absl:(19500/28573) steps collected at 264.6 steps/s.\n",
            "INFO:absl:(19600/28573) steps collected at 278.8 steps/s.\n",
            "INFO:absl:(19700/28573) steps collected at 266.5 steps/s.\n",
            "INFO:absl:(19800/28573) steps collected at 269.4 steps/s.\n",
            "INFO:absl:(19900/28573) steps collected at 267.3 steps/s.\n",
            "INFO:absl:(20000/28573) steps collected at 275.3 steps/s.\n",
            "INFO:absl:(20100/28573) steps collected at 276.6 steps/s.\n",
            "INFO:absl:(20200/28573) steps collected at 269.8 steps/s.\n",
            "INFO:absl:(20300/28573) steps collected at 270.5 steps/s.\n",
            "INFO:absl:(20400/28573) steps collected at 279.5 steps/s.\n",
            "INFO:absl:(20500/28573) steps collected at 277.2 steps/s.\n",
            "INFO:absl:(20600/28573) steps collected at 274.9 steps/s.\n",
            "INFO:absl:(20700/28573) steps collected at 275.1 steps/s.\n",
            "INFO:absl:(20800/28573) steps collected at 266.3 steps/s.\n",
            "INFO:absl:(20900/28573) steps collected at 274.3 steps/s.\n",
            "INFO:absl:(21000/28573) steps collected at 276.3 steps/s.\n",
            "INFO:absl:(21100/28573) steps collected at 260.6 steps/s.\n",
            "INFO:absl:(21200/28573) steps collected at 270.7 steps/s.\n",
            "INFO:absl:(21300/28573) steps collected at 266.8 steps/s.\n",
            "INFO:absl:(21400/28573) steps collected at 265.6 steps/s.\n",
            "INFO:absl:(21500/28573) steps collected at 274.5 steps/s.\n",
            "INFO:absl:(21600/28573) steps collected at 268.9 steps/s.\n",
            "INFO:absl:(21700/28573) steps collected at 275.7 steps/s.\n",
            "INFO:absl:(21800/28573) steps collected at 282.2 steps/s.\n",
            "INFO:absl:(21900/28573) steps collected at 273.3 steps/s.\n",
            "INFO:absl:(22000/28573) steps collected at 263.7 steps/s.\n",
            "INFO:absl:(22100/28573) steps collected at 279.2 steps/s.\n",
            "INFO:absl:(22200/28573) steps collected at 268.9 steps/s.\n",
            "INFO:absl:(22300/28573) steps collected at 273.1 steps/s.\n",
            "INFO:absl:(22400/28573) steps collected at 276.6 steps/s.\n",
            "INFO:absl:(22500/28573) steps collected at 277.1 steps/s.\n",
            "INFO:absl:(22600/28573) steps collected at 283.2 steps/s.\n",
            "INFO:absl:(22700/28573) steps collected at 274.3 steps/s.\n",
            "INFO:absl:(22800/28573) steps collected at 276.1 steps/s.\n",
            "INFO:absl:(22900/28573) steps collected at 281 steps/s.\n",
            "INFO:absl:(23000/28573) steps collected at 265 steps/s.\n",
            "INFO:absl:(23100/28573) steps collected at 273.8 steps/s.\n",
            "INFO:absl:(23200/28573) steps collected at 281.2 steps/s.\n",
            "INFO:absl:(23300/28573) steps collected at 260.9 steps/s.\n",
            "INFO:absl:(23400/28573) steps collected at 277.5 steps/s.\n",
            "INFO:absl:(23500/28573) steps collected at 273.4 steps/s.\n",
            "INFO:absl:(23600/28573) steps collected at 261.4 steps/s.\n",
            "INFO:absl:(23700/28573) steps collected at 271 steps/s.\n",
            "INFO:absl:(23800/28573) steps collected at 269 steps/s.\n",
            "INFO:absl:(23900/28573) steps collected at 255.8 steps/s.\n",
            "INFO:absl:(24000/28573) steps collected at 282.9 steps/s.\n",
            "INFO:absl:(24100/28573) steps collected at 270.6 steps/s.\n",
            "INFO:absl:(24200/28573) steps collected at 278 steps/s.\n",
            "INFO:absl:(24300/28573) steps collected at 283.9 steps/s.\n",
            "INFO:absl:(24400/28573) steps collected at 266.8 steps/s.\n",
            "INFO:absl:(24500/28573) steps collected at 271.4 steps/s.\n",
            "INFO:absl:(24600/28573) steps collected at 282.5 steps/s.\n",
            "INFO:absl:(24700/28573) steps collected at 270.3 steps/s.\n",
            "INFO:absl:(24800/28573) steps collected at 275.2 steps/s.\n",
            "INFO:absl:(24900/28573) steps collected at 265.3 steps/s.\n",
            "INFO:absl:(25000/28573) steps collected at 267.6 steps/s.\n",
            "INFO:absl:(25100/28573) steps collected at 284.9 steps/s.\n",
            "INFO:absl:(25200/28573) steps collected at 282.4 steps/s.\n",
            "INFO:absl:(25300/28573) steps collected at 274.9 steps/s.\n",
            "INFO:absl:(25400/28573) steps collected at 284 steps/s.\n",
            "INFO:absl:(25500/28573) steps collected at 270.5 steps/s.\n",
            "INFO:absl:(25600/28573) steps collected at 281.9 steps/s.\n",
            "INFO:absl:(25700/28573) steps collected at 280.5 steps/s.\n",
            "INFO:absl:(25800/28573) steps collected at 273.8 steps/s.\n",
            "INFO:absl:(25900/28573) steps collected at 284.7 steps/s.\n",
            "INFO:absl:(26000/28573) steps collected at 286.3 steps/s.\n",
            "INFO:absl:(26100/28573) steps collected at 273 steps/s.\n",
            "INFO:absl:(26200/28573) steps collected at 277.9 steps/s.\n",
            "INFO:absl:(26300/28573) steps collected at 279.2 steps/s.\n",
            "INFO:absl:(26400/28573) steps collected at 268.3 steps/s.\n",
            "INFO:absl:(26500/28573) steps collected at 283.7 steps/s.\n",
            "INFO:absl:(26600/28573) steps collected at 275.6 steps/s.\n",
            "INFO:absl:(26700/28573) steps collected at 252.8 steps/s.\n",
            "INFO:absl:(26800/28573) steps collected at 283.8 steps/s.\n",
            "INFO:absl:(26900/28573) steps collected at 268.1 steps/s.\n",
            "INFO:absl:(27000/28573) steps collected at 262.4 steps/s.\n",
            "INFO:absl:(27100/28573) steps collected at 270.9 steps/s.\n",
            "INFO:absl:(27200/28573) steps collected at 266.3 steps/s.\n",
            "INFO:absl:(27300/28573) steps collected at 281.2 steps/s.\n",
            "INFO:absl:(27400/28573) steps collected at 277.3 steps/s.\n",
            "INFO:absl:(27500/28573) steps collected at 275.1 steps/s.\n",
            "INFO:absl:(27600/28573) steps collected at 281.2 steps/s.\n",
            "INFO:absl:(27700/28573) steps collected at 271.3 steps/s.\n",
            "INFO:absl:(27800/28573) steps collected at 270 steps/s.\n",
            "INFO:absl:(27900/28573) steps collected at 280 steps/s.\n",
            "INFO:absl:(28000/28573) steps collected at 270.7 steps/s.\n",
            "INFO:absl:(28100/28573) steps collected at 279.3 steps/s.\n",
            "INFO:absl:(28200/28573) steps collected at 286.7 steps/s.\n",
            "INFO:absl:(28300/28573) steps collected at 271.6 steps/s.\n",
            "INFO:absl:(28400/28573) steps collected at 286.1 steps/s.\n",
            "INFO:absl:(28500/28573) steps collected at 282.1 steps/s.\n",
            "INFO:absl:(28573/28573) steps collected at 279.2 steps/s.\n",
            "INFO:absl:Finished: 50000 transitions collected, saved at /content/gdrive/My Drive/offlinerl/data/Pendulum-v1/example/0/data_Pendulum-v1.pt, time cost 152.2s.\n"
          ]
        }
      ],
      "source": [
        "import collections\n",
        "import numpy as np\n",
        "import os\n",
        "import gin\n",
        "import gym\n",
        "import mujoco_py\n",
        "from absl import app\n",
        "from absl import flags\n",
        "from absl import logging\n",
        "import tensor_specs\n",
        "import data_structures as ds\n",
        "import time\n",
        "import alf_gym_wrapper\n",
        "import nest\n",
        "import importlib  \n",
        "import imageio\n",
        "import mujoco_py\n",
        "from alf_environment import TimeLimit\n",
        "import torch.nn as nn\n",
        "import pickle\n",
        "class ContinuousRandomPolicy(nn.Module):\n",
        "  \"\"\"Samples actions uniformly at random.\"\"\"\n",
        "\n",
        "  def __init__(self, action_spec):\n",
        "    super(ContinuousRandomPolicy, self).__init__()\n",
        "    self._action_spec = action_spec\n",
        "\n",
        "  def __call__(self, observation, state=()):\n",
        "    action = tensor_specs.sample_bounded_spec(\n",
        "        self._action_spec, \n",
        "        #outer_dims=[observation.shape[0]]\n",
        "        )\n",
        "    return action, state\n",
        "\n",
        "PolicyConfig = collections.namedtuple(\n",
        "    'PolicyConfig', 'ptype, ckpt, wrapper, model_params')\n",
        "\n",
        "\n",
        "PTYPES = [\n",
        "   'randwalk',\n",
        "    'randinit',\n",
        "    'load',\n",
        "]\n",
        "\n",
        "WRAPPER_TYPES = [\n",
        "    'none',\n",
        "    'gaussian',\n",
        "]\n",
        "\n",
        "# params: (wrapper_type, *wrapper_params)\n",
        "# wrapper_type: none, eps, gaussian, gaussianeps\n",
        "#!rm -r /content/gdrive/My\\ Drive/testdata/Pendulum-v0/agent_partial_target\n",
        "\n",
        "def wrap_policy(a_net, wrapper):\n",
        "  \"\"\"Wraps actor network with desired randomization.\"\"\"\n",
        "  if wrapper[0] == 'none':\n",
        "    policy = RandomSoftPolicy(a_net)\n",
        "  elif wrapper[0] == 'gaussian':\n",
        "    policy = GaussianRandomSoftPolicy(\n",
        "        a_net, std=wrapper[1])\n",
        "  return policy\n",
        "\n",
        "\n",
        "def load_policy(policy_cfg, action_spec, observation_spec):\n",
        "  \"\"\"Loads policy based on config.\"\"\"\n",
        "  if policy_cfg.ptype not in PTYPES:\n",
        "    raise ValueError('Unknown policy type %s.' % policy_cfg.ptype)\n",
        "  if policy_cfg.ptype == 'randwalk':\n",
        "    policy = ContinuousRandomPolicy(action_spec)\n",
        "  elif policy_cfg.ptype in ['randinit', 'load']:\n",
        "    a_net = ActorNetwork(\n",
        "        observation_spec,\n",
        "        action_spec,\n",
        "        fc_layer_params=policy_cfg.model_params)\n",
        "    if policy_cfg.ptype == 'load' and os.path.exists(policy_cfg.ckpt):\n",
        "      logging.info('Loading policy from %s...', policy_cfg.ckpt)\n",
        "      a_net = torch.load(policy_cfg.ckpt)\n",
        "    policy = wrap_policy(a_net, policy_cfg.wrapper)\n",
        "  return policy\n",
        "\n",
        "\n",
        "def parse_policy_cfg(policy_cfg):\n",
        "  return PolicyConfig(*policy_cfg)\n",
        "\n",
        "def maybe_makedirs(log_dir):\n",
        "  import os.path\n",
        "  if not os.path.exists(log_dir):\n",
        "     os.mkdir(log_dir)\n",
        "#####################################\n",
        "#from train_eval_utils\n",
        "from typing import Callable\n",
        "Transition = collections.namedtuple(\n",
        "    'Transition', 's1, s2, a1, a2, discount, reward')\n",
        "\n",
        "import matplotlib.animation as animation\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "#animation \n",
        "def update_scene(num, frames, patch):\n",
        "    patch.set_data(frames[num])\n",
        "    return patch,\n",
        "\n",
        "def plot_animation(frames, repeat=False, interval=40):\n",
        "    plt.close()  # or else nbagg sometimes plots in the previous cell\n",
        "    fig = plt.figure()\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "    return animation.FuncAnimation(fig, update_scene, \n",
        "                                   fargs=(frames, patch),\n",
        "                                   frames=len(frames), \n",
        "                                   repeat=repeat, interval=interval)\n",
        "    \n",
        "def eval_policy_episodes(env, policy, n_episodes):\n",
        "  \"\"\"Evaluates policy performance.\"\"\"\n",
        "  results = []\n",
        "  \n",
        "  discount=[]\n",
        "  # For video / GIF.\n",
        "  for ep_idx in range(n_episodes):\n",
        "    time_step = env.reset()\n",
        "    total_rewards = 0.0\n",
        "    #env.render()\n",
        "    while not time_step.is_last():\n",
        "\n",
        "       action = policy(torch.from_numpy(time_step.observation))[0]  \n",
        "       if action.ndim<1:\n",
        "          time_step= env.step(action.unsqueeze(0).detach().cpu()) \n",
        "       else:  \n",
        "          time_step= env.step(action.detach().cpu())\n",
        "       total_rewards += time_step.reward or 0.\n",
        "       #img = env.render(mode = 'rgb_array')\n",
        "       #img = Image.fromarray(img,'RGB')\n",
        "       #img.save('/content/gdrive/My Drive/offlinerl/data/HalfCheetah_{}.jpg'.format(ep_idx))\n",
        "       results.append(total_rewards)\n",
        "       discount.append(time_step.discount)\n",
        "  results = torch.tensor(results)\n",
        "  return torch.mean(results).to(dtype=torch.float32), torch.std(results).to(dtype=torch.float32),torch.tensor(discount).to(torch.float32)\n",
        "\n",
        "\n",
        "def eval_policies(env, policies, n_episodes):\n",
        "  results_episode_return = []\n",
        "  infos = collections.OrderedDict()\n",
        "  for name, policy in policies.items():\n",
        "    mean, _ ,dsc = eval_policy_episodes(env, policy, n_episodes)\n",
        "    results_episode_return.append(mean)\n",
        "    infos[name] = collections.OrderedDict()\n",
        "    infos[name]['episode_mean'] = mean\n",
        "    infos[name]['discount'] = dsc\n",
        "  results = results_episode_return\n",
        "  return results, infos\n",
        "\n",
        "def map_structure(func: Callable, structure):\n",
        "    \n",
        "    if not callable(func):\n",
        "        raise TypeError(\"func must be callable, got: %s\" % func)\n",
        "\n",
        "    if isinstance(structure, list):\n",
        "        return [map_structure(func, item) for item in structure]\n",
        "\n",
        "    if isinstance(structure, dict):\n",
        "        return {key: map_structure(func, structure[key]) for key in structure}\n",
        "\n",
        "    return func(structure)\n",
        "\n",
        "\n",
        "###############\n",
        "\n",
        "MUJOCO_ENVS = [\n",
        "    \"Ant-v2\",\n",
        "    \"HalfCheetah-v2\",\n",
        "    \"Hopper-v2\",\n",
        "    \"Humanoid-v2\",\n",
        "    \"InvertedPendulum-v2\",\n",
        "    \"InvertedDoublePendulum-v2\",\n",
        "    \"Reacher-v2\",\n",
        "    \"Swimmer-v2\",\n",
        "    \"Walker2d-v2\"\n",
        "]\n",
        "MUJOCO_ENVS_LENNGTH={\"Ant-v2\":1000,\n",
        "    \"HalfCheetah-v2\":1000,\n",
        "    \"Hopper-v2\":1000,\n",
        "    \"Humanoid-v2\":1000,\n",
        "    \"InvertedPendulum-v2\":1000,\n",
        "    \"InvertedDoublePendulum-v2\":1000,\n",
        "    \"Reacher-v2\":50,\n",
        "    \"Swimmer-v2\":1000,\n",
        "    \"Walker2d-v2\":1000,\n",
        "    \"Pendulum-v1\":200\n",
        "    }\n",
        "\n",
        "\n",
        "def get_transition(time_step, next_time_step, action, next_action):\n",
        "  return Transition(\n",
        "      s1=time_step.observation,\n",
        "      s2=next_time_step.observation,\n",
        "      a1=action,\n",
        "      a2=next_action,\n",
        "      reward=next_time_step.reward,\n",
        "      discount=next_time_step.discount)\n",
        "  \n",
        "class DataCollector(object):\n",
        "  \"\"\"Class for collecting sequence of environment experience.\"\"\"\n",
        "\n",
        "  def __init__(self, env, policy, data):\n",
        "    self._env = env\n",
        "    self._policy = policy\n",
        "    self._data = data\n",
        "    self._saved_action = None\n",
        "\n",
        "  def collect_transition(self):\n",
        "    \"\"\"Collect single transition from environment.\"\"\"\n",
        "    time_step = self._env.current_time_step()\n",
        "    if self._saved_action is None:\n",
        "      self._saved_action = self._policy(torch.from_numpy(time_step.observation))[0]\n",
        "    action = self._saved_action\n",
        "    if action.ndim<1:\n",
        "          next_time_step = self._env.step(action.unsqueeze(0).detach().cpu())\n",
        "    else:\n",
        "          next_time_step = self._env.step(action.detach().cpu())\n",
        "    next_action = self._policy(torch.from_numpy(next_time_step.observation))[0]\n",
        "    self._saved_action = next_action\n",
        "    if not time_step.is_last():\n",
        "      transition = get_transition(time_step, next_time_step,\n",
        "                                  action, next_action)\n",
        "      \n",
        "      #time_step=time_step._replace(observation=np.expand_dims(time_step.observation, axis=0))\n",
        "\n",
        "      self._data.add_transitions(transition)\n",
        "      return 1\n",
        "    else:\n",
        "      return 0\n",
        "  \n",
        "#######################\n",
        "def gather(params, indices, axis = None):\n",
        "    if axis is None:\n",
        "        axis = 0\n",
        "    if axis < 0:\n",
        "        axis = len(params.shape) + axis\n",
        "    if axis == 0:\n",
        "        return params[indices]\n",
        "    elif axis == 1:\n",
        "        return params[:, indices]\n",
        "    elif axis == 2:\n",
        "        return params[:, :, indices]\n",
        "    elif axis == 3:\n",
        "        return params[:,:,:, indices]\n",
        "\n",
        "def scatter_update(tensor, indices, updates):\n",
        "    tensor = torch.tensor(tensor)\n",
        "    indices = torch.tensor(indices, dtype=torch.long)\n",
        "    updates = torch.tensor(updates)\n",
        "    tensor[indices] = updates\n",
        "    return tensor\n",
        "  \n",
        "class DatasetView(object):\n",
        "  \"\"\"Interface for reading from dataset.\"\"\"\n",
        "\n",
        "  def __init__(self, dataset, indices):\n",
        "    self._dataset = dataset\n",
        "    self._indices = indices\n",
        "\n",
        "  def get_batch(self, indices):\n",
        "    real_indices = self._indices[indices]\n",
        "    return self._dataset.get_batch(real_indices)\n",
        "\n",
        "  @property\n",
        "  def size(self):\n",
        "    return self._indices.shape[0]\n",
        "\n",
        "def save_copy(data, ckpt_name):\n",
        "  \"\"\"Creates a copy of the current data and save as a checkpoint.\"\"\"\n",
        "  new_data = Dataset(\n",
        "      observation_spec=data.config['observation_spec'],\n",
        "      action_spec=data.config['action_spec'],\n",
        "      size=data.size,\n",
        "      circular=False)\n",
        "  full_batch = data.get_batch(np.arange(data.size))\n",
        "  new_data.add_transitions(full_batch)\n",
        "  torch.save(new_data, ckpt_name+\".pt\")\n",
        "  \n",
        "class Dataset(nn.Module):\n",
        "  \"\"\"Tensorflow module of dataset of transitions.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      observation_spec,\n",
        "      action_spec,\n",
        "      size,\n",
        "      circular=True,\n",
        "      ):\n",
        "    super(Dataset, self).__init__()\n",
        "    self._size = size\n",
        "    self._circular = circular\n",
        "    obs_shape = list(observation_spec.shape)\n",
        "    obs_type = observation_spec.dtype\n",
        "    action_shape = list(action_spec.shape)\n",
        "    action_type = action_spec.dtype\n",
        "    self._s1 = self._zeros([size] + obs_shape, obs_type)\n",
        "    self._s2 = self._zeros([size] + obs_shape, obs_type)\n",
        "    self._a1 = self._zeros([size] + action_shape, action_type)\n",
        "    self._a2 = self._zeros([size] + action_shape, action_type)\n",
        "    self._discount = self._zeros([size], torch.float32)\n",
        "    self._reward = self._zeros([size], torch.float32)\n",
        "    self._data = Transition(\n",
        "        s1=self._s1, s2=self._s2, a1=self._a1, a2=self._a2,\n",
        "        discount=self._discount, reward=self._reward)\n",
        "    self._current_size = torch.tensor(0, dtype=torch.int32,requires_grad=False)\n",
        "    self._current_idx = torch.tensor(0, dtype=torch.int32, requires_grad=False)\n",
        "    self._capacity = torch.autograd.Variable(torch.tensor(self._size))\n",
        "    self._config = collections.OrderedDict(\n",
        "        observation_spec=observation_spec,\n",
        "        action_spec=action_spec,\n",
        "        size=size,\n",
        "        circular=circular)\n",
        "\n",
        "  @property\n",
        "  def config(self):\n",
        "    return self._config\n",
        "\n",
        "  def create_view(self, indices):\n",
        "    return DatasetView(self, indices)\n",
        "\n",
        "  def get_batch(self, indices):\n",
        "    indices = torch.LongTensor(indices)\n",
        "    def get_batch_(data_):\n",
        "      return gather(data_, indices)\n",
        "    transition_batch = nest.map_structure(get_batch_, self._data)\n",
        "    return transition_batch\n",
        "\n",
        "  @property\n",
        "  def data(self):\n",
        "    return self._data\n",
        "\n",
        "  @property\n",
        "  def capacity(self):\n",
        "    return self._size\n",
        "\n",
        "  @property\n",
        "  def size(self):\n",
        "    return self._current_size.numpy()\n",
        "\n",
        "  def _zeros(self, shape, dtype):\n",
        "    \"\"\"Create a variable initialized with zeros.\"\"\"\n",
        "    return torch.autograd.Variable(torch.zeros(shape, dtype = dtype))\n",
        "\n",
        "  def add_transitions(self, transitions,):\n",
        "    assert isinstance(transitions, Transition)\n",
        "    for i in transitions._fields:\n",
        "        attr=getattr(transitions,i)\n",
        "        if torch.is_tensor(attr):\n",
        "           attr=attr.detach().cpu()\n",
        "        #print(torch.is_tensor(attr))\n",
        "        transitions=transitions._replace(**{i:np.expand_dims(attr,axis=0)})\n",
        "    #transitions=transitions._replace(s1=np.expand_dims(transitions.s1,axis=0))\n",
        "    #print(f\"new transition:{transitions}\")\n",
        "    batch_size = transitions.s1.shape[0]\n",
        "    effective_batch_size = torch.minimum( torch.tensor(batch_size), torch.tensor(self._size - self._current_idx))\n",
        "    indices = self._current_idx + torch.arange(effective_batch_size)\n",
        "    for key in transitions._asdict().keys():\n",
        "        data = getattr(self._data, key)\n",
        "        batch = getattr(transitions, key)\n",
        "        data[indices]= torch.tensor(batch[:effective_batch_size])\n",
        "        #scatter_update(data, indices, batch[:effective_batch_size])\n",
        "    # Update size and index.\n",
        "    if torch.less(self._current_size, self._size):\n",
        "      self._current_size+=effective_batch_size\n",
        "    self._current_idx+=effective_batch_size\n",
        "    if self._circular:\n",
        "      if torch.greater_equal(self._current_idx, self._size):\n",
        "        self._current_idx=0\n",
        "#########################\n",
        "#utils.py\n",
        "def shuffle_indices_with_steps(n, steps=1, rand=None):\n",
        "  \"\"\"Randomly shuffling indices while keeping segments.\"\"\"\n",
        "  if steps == 0:\n",
        "    return np.arange(n)\n",
        "  if rand is None:\n",
        "    rand = np.random\n",
        "  n_segments = int(n // steps)\n",
        "  n_effective = n_segments * steps\n",
        "  batch_indices = rand.permutation(n_segments)\n",
        "  batches = np.arange(n_effective).reshape([n_segments, steps])\n",
        "  shuffled_batches = batches[batch_indices]\n",
        "  shuffled_indices = np.arange(n)\n",
        "  shuffled_indices[:n_effective] = shuffled_batches.reshape([-1])\n",
        "  return shuffled_indices\n",
        "\n",
        "\n",
        "#########################\n",
        "#collect_data.py\n",
        "\n",
        "def dir_path(path):\n",
        "    if os.path.isdir(path):\n",
        "        return path\n",
        "    else:\n",
        "        raise argparse.ArgumentTypeError(f\"readable_dir:{path} is not a valid path\")\n",
        "import shutil\n",
        "if not os.path.exists(\"/content/configs\"):\n",
        "   shutil.copytree('/content/gdrive/My Drive/configs', \"/content/configs\")\n",
        "#shutil.copy('/content/gdrive/My Drive/configs/D2E_example.py', \"/content\")\n",
        "\n",
        "if not os.path.exists('/content/gdrive/My Drive/offlinerl/data'):\n",
        "  os.makedirs('/content/gdrive/My Drive/offlinerl/data')\n",
        "else:\n",
        "  pass\n",
        "\n",
        "if not os.path.exists('/content/gdrive/My Drive/testdata/Pendulum-v0'):\n",
        "     os.makedirs('/content/gdrive/My Drive/testdata/Pendulum-v0/')\n",
        "else:\n",
        "  pass\n",
        "\n",
        "import argparse\n",
        "parser = argparse.ArgumentParser(description='BRAC')\n",
        "parser.add_argument('--root_offlinerl_dir', type=dir_path, default='/content/gdrive/My Drive/offlinerl/data', help='Root directory for saving data')\n",
        "parser.add_argument('--sub_offlinerl_dir', type=str, default=None, help='sub directory for saving data.')\n",
        "parser.add_argument('--test_srcdir', type=str, default=None, help='directory for saving test data.')\n",
        "parser.add_argument('--env_name', type=str, default='HalfCheetah-v2',help = 'env name.')\n",
        "parser.add_argument('--data_name', type=str, default='random', help = 'data name.')\n",
        "parser.add_argument('--env_loader', type=str, default='mujoco', help = 'env loader, suite/gym.')\n",
        "parser.add_argument('--config_dir', type=str, default='configs', help = 'config file dir.')\n",
        "parser.add_argument('--config_file', type=str, default='d2e_pure', help = 'config file name.')\n",
        "parser.add_argument('--policy_root_dir', type=str, default=None,help = 'Directory in which to find the behavior policy.')\n",
        "parser.add_argument('--n_samples', type=int, default=int(1e2), help = 'number of transitions to collect.')\n",
        "parser.add_argument('--n_eval_episodes', type=int, default=20, help = 'number episodes to eval each policy.')\n",
        "parser.add_argument(\"--gin_file\", type=str, default=[], nargs='*', help = 'Paths to the gin-config files.')\n",
        "\n",
        "parser.add_argument('--gin_bindings', type=str, default=[], nargs='*', help = 'Gin binding parameters.')\n",
        "args = parser.parse_args()\n",
        "if not os.path.exists(\"/content/gdrive/My Drive/offlinerl/data/HalfCheetah-v2/example/0\"):\n",
        "   os.makedirs(\"/content/gdrive/My Drive/offlinerl/data/HalfCheetah-v2/example/0\")\n",
        "if not os.path.exists(\"/content/gdrive/My Drive/offlinerl/data/Pendulum-v1/example/0\"):\n",
        "   os.makedirs(\"/content/gdrive/My Drive/offlinerl/data/Pendulum-v1/example/0\")\n",
        "def get_sample_counts(n, distr):\n",
        "  \"\"\"Provides size of each sub-dataset based on desired distribution.\"\"\"\n",
        "  distr = torch.tensor(distr)\n",
        "  distr = distr / torch.sum(distr)\n",
        "  counts = []\n",
        "  remainder = n\n",
        "  for i in range(distr.shape[0] - 1):\n",
        "    count = int(n * distr[i])\n",
        "    remainder -= count\n",
        "    counts.append(count)\n",
        "  counts.append(remainder)\n",
        "  return counts\n",
        "\n",
        "\n",
        "def collect_n_transitions(tf_env, policy, data, n, log_freq=1000):\n",
        "  \"\"\"Adds desired number of transitions to dataset.\"\"\"\n",
        "  collector = DataCollector(tf_env, policy, data)\n",
        "  time_st = time.time()\n",
        "  timed_at_step = 0\n",
        "  steps_collected = 0\n",
        "  while steps_collected < n:\n",
        "    count = collector.collect_transition()\n",
        "    steps_collected += count\n",
        "    if (steps_collected % log_freq == 0\n",
        "        or steps_collected == n) and count > 0:\n",
        "      steps_per_sec = ((steps_collected - timed_at_step)\n",
        "                       / (time.time() - time_st))\n",
        "      timed_at_step = steps_collected\n",
        "      time_st = time.time()\n",
        "      logging.info('(%d/%d) steps collected at %.4g steps/s.', steps_collected,\n",
        "                   n, steps_per_sec)\n",
        "\n",
        "\n",
        "\n",
        "def collect_data(\n",
        "    log_dir,\n",
        "    data_config,\n",
        "    n_samples=int(1e3),\n",
        "    env_name='HalfCheetah-v2',\n",
        "    log_freq=int(1e2),\n",
        "    n_eval_episodes=20,\n",
        "    ):\n",
        "  \"\"\"\n",
        "               **** Main function ****\n",
        "  Creates dataset of transitions based on desired config.\n",
        "  \"\"\"\n",
        "  seed=0\n",
        "  torch.manual_seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  dm_env = gym.spec(env_name).make()\n",
        "  env = alf_gym_wrapper.AlfGymWrapper(dm_env, discount=0.99)\n",
        "  env = TimeLimit(env, MUJOCO_ENVS_LENNGTH[env_name])\n",
        "  observation_spec = env.observation_spec()\n",
        "  action_spec = env.action_spec()\n",
        "  \n",
        "\n",
        "  # Initialize dataset.\n",
        "  sample_sizes = list([cfg[-1] for cfg in data_config])\n",
        "  \n",
        "  sample_sizes = get_sample_counts(n_samples, sample_sizes)\n",
        "  logging.info(\", \".join([\"%s\" % s for s in sample_sizes]))\n",
        "  data = Dataset(\n",
        "        observation_spec,\n",
        "        action_spec,\n",
        "        n_samples,\n",
        "        circular=False)\n",
        "  \n",
        "  # Collect data for each policy in data_config.\n",
        "  time_st = time.time()\n",
        "  test_results = collections.OrderedDict()\n",
        "  for (policy_name, policy_cfg, _), n_transitions in zip(\n",
        "      data_config, sample_sizes):\n",
        "    policy_cfg = parse_policy_cfg(policy_cfg)\n",
        "    policy = load_policy(policy_cfg, action_spec, observation_spec)\n",
        "    logging.info('Testing policy %s...', policy_name)\n",
        "    eval_mean, eval_std,_ = eval_policy_episodes(\n",
        "        env, policy, n_eval_episodes)\n",
        "    test_results[policy_name] = [eval_mean, eval_std]\n",
        "    logging.info('Return mean %.4g, std %.4g.', eval_mean, eval_std)\n",
        "    logging.info('Collecting data from policy %s...', policy_name)\n",
        "    collect_n_transitions(env, policy, data, n_transitions, log_freq)\n",
        "  # Save final dataset.\n",
        "  assert data.size == data.capacity\n",
        "  data_ckpt_name = os.path.join(log_dir, 'data_{}.pt'.format(env_name))\n",
        "  torch.save([data.capacity, data.state_dict()], data_ckpt_name)\n",
        "  whole_data_ckpt_name = os.path.join(log_dir, 'data_{}.pth'.format(env_name))\n",
        "  with open( whole_data_ckpt_name, 'wb') as filehandler: \n",
        "    pickle.dump(data, filehandler)\n",
        "\n",
        "  time_cost = time.time() - time_st\n",
        "  logging.info('Finished: %d transitions collected, '\n",
        "               'saved at %s, '\n",
        "               'time cost %.4gs.', n_samples, data_ckpt_name, time_cost)\n",
        "\n",
        "\n",
        "def main(args):\n",
        "  logging.set_verbosity(logging.INFO)\n",
        "  sub_dir = args.sub_offlinerl_dir\n",
        "  log_dir = os.path.join(\n",
        "      args.root_offlinerl_dir,\n",
        "      args.env_name,\n",
        "      args.data_name,\n",
        "      sub_dir,\n",
        "      )\n",
        "  maybe_makedirs(log_dir)\n",
        "  #print(args.config_dir)\n",
        "  #print(args.n_samples)\n",
        "  config_module = importlib.import_module(\n",
        "      '{}.{}'.format(args.config_dir, args.config_file))\n",
        "  collect_data(\n",
        "      log_dir=log_dir,\n",
        "      data_config=config_module.get_data_config(args.env_name,\n",
        "                                                args.policy_root_dir),\n",
        "      n_samples = args.n_samples,\n",
        "      env_name  = args.env_name,\n",
        "      n_eval_episodes=args.n_eval_episodes)\n",
        "\n",
        "def collect_gym_data(args):\n",
        "    args.sub_offlinerl_dir = '0'\n",
        "    args.env_name = 'Pendulum-v1'\n",
        "    args.data_name = 'example'\n",
        "    args.config_file = 'D2E_example'\n",
        "    data_dir = 'testdata'\n",
        "    args.test_srcdir = os.getcwd()\n",
        "    args.policy_root_dir = os.path.join(args.test_srcdir,\n",
        "                                               data_dir)\n",
        "    args.n_samples = 50000  # Short collection.\n",
        "    args.n_eval_episodes = 1\n",
        "    main(args)\n",
        "if __name__ == \"__main__\":\n",
        "  args = parser.parse_args(sys.argv[1:])\n",
        "  collect_gym_data(args)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mXZey9iovFOT"
      },
      "outputs": [],
      "source": [
        "class AgentConfig(object):\n",
        "  \"\"\"Class for handling agent parameters.\"\"\"\n",
        "\n",
        "  def __init__(self, agent_flags):\n",
        "    self._agent_flags = agent_flags\n",
        "    self._agent_args = self._get_agent_args()\n",
        "\n",
        "  def _get_agent_args(self):\n",
        "    \"\"\"Gets agent parameters associated with config.\"\"\"\n",
        "    agent_flags = self._agent_flags\n",
        "    agent_args = Flags(\n",
        "        observation_spec=agent_flags.observation_spec,\n",
        "        action_spec=agent_flags.action_spec,\n",
        "        optimizers=agent_flags.optimizers,\n",
        "        batch_size=agent_flags.batch_size,\n",
        "        weight_decays=agent_flags.weight_decays,\n",
        "        update_rate=agent_flags.update_rate,\n",
        "        update_freq=agent_flags.update_freq,\n",
        "        discount=agent_flags.discount,\n",
        "        train_data=agent_flags.train_data,\n",
        "        )\n",
        "    agent_args.modules = self._get_modules()\n",
        "    return agent_args\n",
        "\n",
        "  def _get_modules(self):\n",
        "    raise NotImplementedError\n",
        "\n",
        "  @property\n",
        "  def agent_args(self):\n",
        "    return self._agent_args\n",
        "  \n",
        "class Config(AgentConfig):\n",
        "\n",
        "  def _get_modules(self):\n",
        "    return get_modules(\n",
        "        self._agent_flags.model_params,\n",
        "        self._agent_flags.observation_spec,\n",
        "        self._agent_flags.action_spec)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlyO0vJiI5pP"
      },
      "source": [
        "install deepmind control suite  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8ZX4jbziwBg9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "46369c24-918a-44f5-873b-1fc6a14219e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:absl:Data directory /content/gdrive/My Drive/offlinerl/data.\n",
            "INFO:absl:Loading data from /content/gdrive/My Drive/offlinerl/data/Pendulum-v1/example/0/ ...\n",
            "INFO:absl:n_train 100.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3,)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-63b613c50984>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    258\u001b[0m   \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munknown\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_known_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m   \u001b[0mgin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_config_files_and_bindings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgin_bindings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfinalize_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_includes_and_imports\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m   \u001b[0mTrain_offline_brac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m   \u001b[0mgin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m   \u001b[0mgin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_REGISTRY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-63b613c50984>\u001b[0m in \u001b[0;36mTrain_offline_brac\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_train_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m  \u001b[0;31m# Short training.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Just test that it runs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m   \u001b[0;31m#args = parser.parse_args(sys.argv[1:])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-63b613c50984>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    234\u001b[0m       \u001b[0mn_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m       \u001b[0mtotal_train_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_train_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m       \u001b[0mn_eval_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_eval_episodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m       )\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gin/config.py\u001b[0m in \u001b[0;36mgin_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0mscope_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" in scope '{}'\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscope_str\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mscope_str\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m       \u001b[0merr_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merr_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn_or_cls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m       \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment_exception_message_and_reraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mgin_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gin/utils.py\u001b[0m in \u001b[0;36maugment_exception_message_and_reraise\u001b[0;34m(exception, message)\u001b[0m\n\u001b[1;32m     39\u001b[0m   \u001b[0mproxy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExceptionProxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0mExceptionProxy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mproxy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gin/config.py\u001b[0m in \u001b[0;36mgin_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1582\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1583\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m       \u001b[0merr_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-63b613c50984>\u001b[0m in \u001b[0;36mtrain_eval_offline\u001b[0;34m(log_dir, data_file, env_name, n_train, shuffle_steps, seed, use_seed_for_data, total_train_steps, summary_freq, print_freq, save_freq, eval_freq, n_eval_episodes, model_params, optimizers, batch_size, weight_decays, update_freq, update_rate, discount)\u001b[0m\n\u001b[1;32m    103\u001b[0m       train_data=train_data)\n\u001b[1;32m    104\u001b[0m   \u001b[0magent_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_flags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m   \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD2EAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#ATTENTION: Debugg ====> should it be D2EAgent here??\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m   \u001b[0magent_ckpt_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'agent'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gin/config.py\u001b[0m in \u001b[0;36mgin_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0mscope_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" in scope '{}'\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscope_str\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mscope_str\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m       \u001b[0merr_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merr_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn_or_cls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m       \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment_exception_message_and_reraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mgin_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gin/utils.py\u001b[0m in \u001b[0;36maugment_exception_message_and_reraise\u001b[0;34m(exception, message)\u001b[0m\n\u001b[1;32m     39\u001b[0m   \u001b[0mproxy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExceptionProxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0mExceptionProxy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mproxy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gin/config.py\u001b[0m in \u001b[0;36mgin_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1582\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1583\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m       \u001b[0merr_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-503562eaf42f>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, alpha, alpha_max, train_alpha, value_penalty, target_divergence, alpha_entropy, train_alpha_entropy, target_entropy, EIM_config, divergence_name, warm_start, c_iter, ensemble_q_lambda, **kwargs)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensemble_q_lambda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensemble_q_lambda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD2EAgent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    695\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_build_fns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_agent_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgentModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodules_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-503562eaf42f>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, observation_spec, action_spec, time_step_spec, modules, optimizers, batch_size, weight_decays, update_freq, update_rate, discount, train_data, resume, device)\u001b[0m\n\u001b[1;32m    454\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"checkpoint.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_build_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-503562eaf42f>\u001b[0m in \u001b[0;36m_build_agent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[0mtrain_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_train_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_global_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_optimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-503562eaf42f>\u001b[0m in \u001b[0;36m_init_vars\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m   1071\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_q_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_p_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1073\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_v_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1074\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_c_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_q_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_q_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-503562eaf42f>\u001b[0m in \u001b[0;36m_build_v_loss\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;31m#########################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m     \u001b[0;31m#input_data next_state (s2) and state (s1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 887\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_EIM_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConditionalMixtureEIM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_EIM_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    888\u001b[0m     \u001b[0mEIM_gate_res\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_EIM_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_components\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'recorder'\n  In call to configurable 'D2EAgent' (<class '__main__.D2EAgent'>)\n  In call to configurable 'train_eval_offline' (<function train_eval_offline at 0x7f0845545d40>)"
          ]
        }
      ],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import datetime\n",
        "import re\n",
        "import gin\n",
        "import dill\n",
        "\n",
        "# When using Gin interactively, reregistering a function is not an error.\n",
        "gin.enter_interactive_mode()\n",
        "gin.clear_config()\n",
        "def get_datetime():\n",
        "  now = datetime.datetime.now().isoformat()\n",
        "  now = re.sub(r'\\D', '', now)[:-6]\n",
        "  return now\n",
        "\n",
        "\n",
        "@gin.configurable(\"train_eval_offline\")\n",
        "def train_eval_offline(\n",
        "    # Basic args.\n",
        "    log_dir,\n",
        "    data_file,\n",
        "    env_name='HalfCheetah-v2',\n",
        "    n_train=int(1e6),\n",
        "    shuffle_steps=0,\n",
        "    seed=0,\n",
        "    use_seed_for_data=False,\n",
        "    # Train and eval args.\n",
        "    total_train_steps=int(1e6),\n",
        "    summary_freq=100,\n",
        "    print_freq=1000,\n",
        "    save_freq=int(2e4),\n",
        "    eval_freq=5000,\n",
        "    n_eval_episodes=20,\n",
        "    # Agent args.\n",
        "    model_params=(((200, 200),), 2),\n",
        "    optimizers=(( 0.0001, 0.5, 0.99),),\n",
        "    batch_size=256,\n",
        "    weight_decays=(0.0,),\n",
        "    update_freq=1,\n",
        "    update_rate=0.005,\n",
        "    discount=0.99,\n",
        "    ):\n",
        "  ###Training a policy with a fixed dataset.###\n",
        "  # Create tf_env to get specs.\n",
        "  dm_env = gym.spec(env_name).make()\n",
        "  env = alf_gym_wrapper.AlfGymWrapper(dm_env,discount=discount)\n",
        "  env = TimeLimit(env, MUJOCO_ENVS_LENNGTH[env_name])\n",
        "  observation_spec = env.observation_spec()\n",
        "  action_spec = env.action_spec()\n",
        "  \n",
        "  # Prepare data.\n",
        "  logging.info('Loading data from %s ...', data_file)\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  \n",
        "  \n",
        "  data_ckpt_name = os.path.join(data_file, 'data_{}.pt'.format(env_name))\n",
        "  whole_data_ckpt_name = os.path.join(data_file, 'data_{}.pth'.format(env_name))\n",
        "  \n",
        "  data_size, state = torch.load(data_ckpt_name, map_location=device)\n",
        "  \n",
        "  #full_data = Dataset(observation_spec, action_spec, data_size)\n",
        "  scores = {} # scores is an empty dict already\n",
        "\n",
        "  if os.path.getsize(whole_data_ckpt_name) > 0:       \n",
        "    with open(whole_data_ckpt_name, \"rb\") as f:\n",
        "        unpickler = pickle.Unpickler(f)\n",
        "        # if file is not empty scores will be equal\n",
        "        # to the value unpickled\n",
        "        scores = unpickler.load()\n",
        "  \n",
        "  full_data=scores\n",
        "  for k, v in full_data._config.items():\n",
        "        if k =='observation_spec':\n",
        "          full_data._config['observation_spec']=observation_spec\n",
        "        elif k=='action_spec':\n",
        "          full_data._config['action_spec']=action_spec\n",
        "   \n",
        "  # Split data.\n",
        "  n_train = min(n_train, full_data.size)\n",
        "  logging.info('n_train %d.', n_train)\n",
        "  if use_seed_for_data:\n",
        "    rand = np.random.RandomState(seed)\n",
        "  else:\n",
        "    rand = np.random.RandomState(0)\n",
        "  shuffled_indices = shuffle_indices_with_steps(\n",
        "      n=full_data.size, steps=shuffle_steps, rand=rand)\n",
        "  train_indices = shuffled_indices[:n_train]\n",
        "  train_data = full_data.create_view(train_indices)\n",
        "\n",
        "  # Create agent.\n",
        "  agent_flags = Flags(\n",
        "      observation_spec=observation_spec,\n",
        "      action_spec=action_spec,\n",
        "      model_params=model_params,\n",
        "      optimizers=optimizers,\n",
        "      batch_size=batch_size,\n",
        "      weight_decays=weight_decays,\n",
        "      update_freq=update_freq,\n",
        "      update_rate=update_rate,\n",
        "      discount=discount,\n",
        "      env_name=env_name,\n",
        "      train_data=train_data)\n",
        "  agent_args = Config(agent_flags).agent_args\n",
        "  agent = D2EAgent(**vars(agent_args)) #ATTENTION: Debugg ====> should it be D2EAgent here??\n",
        "  agent_ckpt_name = os.path.join(log_dir, 'agent')\n",
        "\n",
        "  # Restore agent from checkpoint if there exists one.\n",
        "  if os.path.exists('{}.index'.format(agent_ckpt_name)):\n",
        "    logging.info('Checkpoint found at %s.', agent_ckpt_name)\n",
        "    torch.load(agent, agent_ckpt_name)\n",
        "\n",
        "  # Train agent.\n",
        "  train_summary_dir = os.path.join(log_dir, 'train')\n",
        "  eval_summary_dir = os.path.join(log_dir, 'eval')\n",
        "  train_summary_writer = SummaryWriter(\n",
        "      logdir=train_summary_dir)\n",
        "  eval_summary_writers = collections.OrderedDict()\n",
        "  for policy_key in agent.test_policies.keys():\n",
        "    eval_summary_writer = SummaryWriter(\n",
        "        logdir=os.path.join(eval_summary_dir, policy_key))\n",
        "    eval_summary_writers[policy_key] = eval_summary_writer\n",
        "  eval_results = []\n",
        "\n",
        "  time_st_total = time.time()\n",
        "  time_st = time.time()\n",
        "  step = agent.global_step\n",
        "  timed_at_step = step\n",
        "  while step < total_train_steps:\n",
        "    agent.train_step()\n",
        "    step = agent.global_step\n",
        "    if step % summary_freq == 0 or step == total_train_steps:\n",
        "      agent.write_train_summary(train_summary_writer)\n",
        "    if step % print_freq == 0 or step == total_train_steps:\n",
        "      agent.print_train_info()\n",
        "    if step % eval_freq == 0 or step == total_train_steps:\n",
        "      time_ed = time.time()\n",
        "      time_cost = time_ed - time_st\n",
        "      logging.info(\n",
        "          'Training at %.4g steps/s.', (step - timed_at_step) / time_cost)\n",
        "      eval_result, eval_infos = eval_policies(\n",
        "          env, agent.test_policies, n_eval_episodes)\n",
        "      eval_results.append([step] + eval_result)\n",
        "      logging.info('Testing at step %d:', step)\n",
        "      for policy_key, policy_info in eval_infos.items():\n",
        "        logging.info(utils.get_summary_str(\n",
        "            step=None, info=policy_info, prefix=policy_key+': '))\n",
        "        utils.write_summary(eval_summary_writers[policy_key], policy_info, step)\n",
        "      time_st = time.time()\n",
        "      timed_at_step = step\n",
        "    if step % save_freq == 0:\n",
        "       agent.checkpoint_path=agent_ckpt_name\n",
        "       agent._build_checkpointer()\n",
        "       #for k,v in agent.__dict__.items():\n",
        "       #   if k in ['_agent_module','_q_fns','_p_fn','_c_fn']:\n",
        "       #      print(v)\n",
        "       #torch.save(agent, agent_ckpt_name,pickle_protocol=pickle.HIGHEST_PROTOCOL)\n",
        "       logging.info('Agent saved at %s.', agent_ckpt_name)\n",
        "\n",
        "  agent._build_checkpointer()\n",
        "  time_cost = time.time() - time_st_total\n",
        "  logging.info('Training finished, time cost %.4gs.', time_cost)\n",
        "  return torch.tensor(eval_results)\n",
        "if not os.path.exists('/content/gdrive/My Drive/offlinerl/learn'):\n",
        "  os.makedirs('/content/gdrive/My Drive/offlinerl/learn')\n",
        "else:\n",
        "  pass\n",
        "\n",
        "##############################\n",
        "#/content/gdrive/My Drive/offlinerl/data\n",
        "\n",
        "###train_offline.py#/content/gdrive/My Drive/offlinerl/data/Pendulum-v1/example/0/data_Pendulum-v1.pt\n",
        "parser = argparse.ArgumentParser(description='BRAC')\n",
        "parser.add_argument('--data_root_offlinerl_dir', type=dir_path, default='/content/gdrive/My Drive/offlinerl/data/',\n",
        "                     help='Root directory for data.')\n",
        "parser.add_argument('--data_sub_offlinerl_dir',type=str, default=None, help= '')\n",
        "parser.add_argument('--test_srcdir', type=str, default='/content/gdrive/My Drive', help='directory for saving test data.')\n",
        "parser.add_argument('--data_name', type=str, default='eps1',help= 'data name.')\n",
        "parser.add_argument('--data_file_name', type=str, default='',help= 'data checkpoint file name.')\n",
        "\n",
        "# Flags for offline training.\n",
        "parser.add_argument('--root_dir',type=dir_path, default= '/content/gdrive/My Drive/offlinerl/learn',\n",
        "                    help='Root directory for writing logs/summaries/checkpoints.')\n",
        "parser.add_argument('--sub_dir', type=str, default='0', help='')\n",
        "\n",
        "parser.add_argument('--agent_name',  type=str, default='BRAC', help='agent name.')\n",
        "parser.add_argument('--env_name', type=str, default='HalfCheetah-v2',help = 'env name.')\n",
        "parser.add_argument('--seed', type=int, default=0, help='random seed, mainly for training samples.')\n",
        "parser.add_argument('--total_train_steps', type=int, default=int(5e5), help='')\n",
        "parser.add_argument('--n_eval_episodes',type=int, default= 20,help= '')\n",
        "parser.add_argument('--n_train', type=int, default=int(1e6),help= '')\n",
        "parser.add_argument(\"--gin_file\", type=str, default=[], nargs='*', help = 'Paths to the gin-config files.')\n",
        "\n",
        "parser.add_argument('--gin_bindings', type=str, default=[], nargs='*', help = 'Gin binding parameters.')\n",
        "args = parser.parse_args()\n",
        "\n",
        "\n",
        "def main(args):\n",
        "  logging.set_verbosity(logging.INFO)\n",
        "  \n",
        "  # Setup data file path.\n",
        "  data_dir = os.path.join(\n",
        "      args.data_root_offlinerl_dir,\n",
        "      args.env_name,\n",
        "      args.data_name,\n",
        "      args.data_sub_offlinerl_dir,\n",
        "      )\n",
        "  data_file = os.path.join(\n",
        "      data_dir, args.data_file_name)\n",
        "  logging.info('Data directory %s.', args.data_root_offlinerl_dir)\n",
        "  # Setup log dir.\n",
        "  if args.sub_dir == 'auto':\n",
        "    sub_dir = get_datetime()\n",
        "  else:\n",
        "    sub_dir = args.sub_dir\n",
        "  log_dir = os.path.join(\n",
        "      args.data_root_offlinerl_dir,\n",
        "      args.env_name,\n",
        "      args.data_name,\n",
        "      'n'+str(args.n_train),\n",
        "      args.agent_name,\n",
        "      sub_dir,\n",
        "      str(args.seed),\n",
        "      )\n",
        "  \n",
        "  if not os.path.exists(log_dir):\n",
        "     os.makedirs(log_dir)\n",
        "  else:\n",
        "    pass\n",
        "  train_eval_offline(\n",
        "      log_dir=log_dir,\n",
        "      data_file=data_file,\n",
        "      env_name=args.env_name,\n",
        "      n_train=args.n_train,\n",
        "      total_train_steps=args.total_train_steps,\n",
        "      n_eval_episodes=args.n_eval_episodes,\n",
        "      )\n",
        "\n",
        "def  Train_offline_brac(args):\n",
        "    \n",
        "    data_dir = 'offlinerl/data'\n",
        "    #args.test_srcdir = os.getcwd()\n",
        "    args.data_root_offlinerl_dir = os.path.join(args.test_srcdir, data_dir)\n",
        "    args.data_sub_offlinerl_dir = '0'\n",
        "    args.env_name = 'Pendulum-v1'\n",
        "    args.data_name = 'example'\n",
        "    args.agent_name = 'BRAC'\n",
        "    args.gin_bindings = [\n",
        "        'train_eval_offline.model_params=((200, 200),)',\n",
        "        'train_eval_offline.optimizers=((\"adam\", 5e-4),)']\n",
        "    args.n_train = 100\n",
        "    args.n_eval_episodes = 20\n",
        "    args.total_train_steps = 100  # Short training.\n",
        "     \n",
        "    main(args)  # Just test that it runs.\n",
        "if __name__ == \"__main__\":\n",
        "  #args = parser.parse_args(sys.argv[1:])\n",
        "  args, unknown = parser.parse_known_args()\n",
        "  gin.parse_config_files_and_bindings([], args.gin_bindings,finalize_config=False, skip_unknown=True, print_includes_and_imports=True)\n",
        "  Train_offline_brac(args)\n",
        "  gin.clear_config()\n",
        "  gin.config._REGISTRY.clear()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_9MDTjZZTe5"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\n",
        "# For security purposes, please check the contents of collect_env.py before running it.\n",
        "!python collect_env.py"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "authorship_tag": "ABX9TyMIkkAH/X9XGq9zs8oElGtU",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}